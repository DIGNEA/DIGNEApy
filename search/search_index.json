{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DIGNEApy Diverse Instance Generator with Novelty Search and Evolutionary Algorithms Repository containing the Python version of DIGNEA, a Diverse Instance Generator with Novelty Search and Evolutionary Algorithms. This framework is an extensible tool for generating diverse and discriminatory instances for any desired domain. The instances obtained generated will be biased to the performance of a target in a specified portfolio of algorithms. Documentation: https://dignea.github.io Contributing: https://github.com/DIGNEA/DIGNEApy/blob/main/CONTRIBUTING.md Bug reports: https://github.com/DIGNEA/DIGNEApy/issues A brief tutorial on how to create domains and solvers can be found here Installation: python3 -m pip install digneapy Publications DIGNEA and DIGNEApy has been used in the following publications: Alejandro Marrero, Eduardo Segredo, and Coromoto Leon. 2021. A parallel genetic algorithm to speed up the resolution of the algorithm selection problem. Proceedings of the Genetic and Evolutionary Computation Conference Companion. Association for Computing Machinery, New York, NY, USA, 1978\u20131981. DOI:https://doi.org/10.1145/3449726.3463160 Marrero, A., Segredo, E., Le\u00f3n, C., Hart, E. 2022. A Novelty-Search Approach to Filling an Instance-Space with Diverse and Discriminatory Instances for the Knapsack Problem. In: Rudolph, G., Kononova, A.V., Aguirre, H., Kerschke, P., Ochoa, G., Tu\u0161ar, T. (eds) Parallel Problem Solving from Nature \u2013 PPSN XVII. PPSN 2022. Lecture Notes in Computer Science, vol 13398. Springer, Cham. https://doi.org/10.1007/978-3-031-14714-2_16 Alejandro Marrero, Eduardo Segredo, Emma Hart, Jakob Bossek, and Aneta Neumann. 2023. Generating diverse and discriminatory knapsack instances by searching for novelty in variable dimensions of feature-space. In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO '23). Association for Computing Machinery, New York, NY, USA, 312\u2013320. https://doi.org/10.1145/3583131.3590504 Marrero, A., Segredo, E., Le\u00f3n, C., & Hart, E. 2024. Learning Descriptors for Novelty-Search Based Instance Generation via Meta-evolution. In Genetic and Evolutionary Computation Conference (GECCO \u201924), July 14\u201318, 2024, Melbourne, VIC, Australia. https://doi.org/10.1145/3638529.3654028 Alejandro Marrero, Eduardo Segredo, Coromoto Le\u00f3n, Emma Hart; Synthesising Diverse and Discriminatory Sets of Instances using Novelty Search in Combinatorial Domains. Evolutionary Computation 2024; doi: https://doi.org/10.1162/evco_a_00350 Marrero, A. 2024. Evolutionary Computation Methods for Instance Generation in Optimisation Domains. PhD thesis. Universidad de La Laguna. https://riull.ull.es/xmlui/handle/915/37726 How to Cite DIGNEA If you use DIGNEA in your research work, please cite the following: DIGNEA: A tool to generate diverse and discriminatory instance suites for optimisation domains Alejandro Marrero, Eduardo Segredo, Coromoto Le\u00f3n, Emma Hart SoftwareX , Volume 22, 2023, Page 101355 DOI: 10.1016/j.softx.2023.101355 URL: https://www.sciencedirect.com/science/article/pii/S2352711023000511 Keywords : Instance generation, Novelty search, Evolutionary algorithm, Optimisation, Knapsack problem Abstract : To advance research in the development of optimisation algorithms, it is crucial to have access to large test-beds of diverse and discriminatory instances from a domain that can highlight strengths and weaknesses of different algorithms. The DIGNEA tool enables diverse instance suites to be generated for any domain, that are also discriminatory with respect to a set of solvers of the user's choice. Written in C++, and delivered as a repository and as a Docker image, its modular and template-based design enables it to be easily adapted to multiple domains and types of solvers with minimal effort. This paper exemplifies how to generate instances for the Knapsack Problem domain.","title":"Home"},{"location":"#digneapy","text":"Diverse Instance Generator with Novelty Search and Evolutionary Algorithms Repository containing the Python version of DIGNEA, a Diverse Instance Generator with Novelty Search and Evolutionary Algorithms. This framework is an extensible tool for generating diverse and discriminatory instances for any desired domain. The instances obtained generated will be biased to the performance of a target in a specified portfolio of algorithms. Documentation: https://dignea.github.io Contributing: https://github.com/DIGNEA/DIGNEApy/blob/main/CONTRIBUTING.md Bug reports: https://github.com/DIGNEA/DIGNEApy/issues A brief tutorial on how to create domains and solvers can be found here Installation: python3 -m pip install digneapy","title":"DIGNEApy"},{"location":"#publications","text":"DIGNEA and DIGNEApy has been used in the following publications: Alejandro Marrero, Eduardo Segredo, and Coromoto Leon. 2021. A parallel genetic algorithm to speed up the resolution of the algorithm selection problem. Proceedings of the Genetic and Evolutionary Computation Conference Companion. Association for Computing Machinery, New York, NY, USA, 1978\u20131981. DOI:https://doi.org/10.1145/3449726.3463160 Marrero, A., Segredo, E., Le\u00f3n, C., Hart, E. 2022. A Novelty-Search Approach to Filling an Instance-Space with Diverse and Discriminatory Instances for the Knapsack Problem. In: Rudolph, G., Kononova, A.V., Aguirre, H., Kerschke, P., Ochoa, G., Tu\u0161ar, T. (eds) Parallel Problem Solving from Nature \u2013 PPSN XVII. PPSN 2022. Lecture Notes in Computer Science, vol 13398. Springer, Cham. https://doi.org/10.1007/978-3-031-14714-2_16 Alejandro Marrero, Eduardo Segredo, Emma Hart, Jakob Bossek, and Aneta Neumann. 2023. Generating diverse and discriminatory knapsack instances by searching for novelty in variable dimensions of feature-space. In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO '23). Association for Computing Machinery, New York, NY, USA, 312\u2013320. https://doi.org/10.1145/3583131.3590504 Marrero, A., Segredo, E., Le\u00f3n, C., & Hart, E. 2024. Learning Descriptors for Novelty-Search Based Instance Generation via Meta-evolution. In Genetic and Evolutionary Computation Conference (GECCO \u201924), July 14\u201318, 2024, Melbourne, VIC, Australia. https://doi.org/10.1145/3638529.3654028 Alejandro Marrero, Eduardo Segredo, Coromoto Le\u00f3n, Emma Hart; Synthesising Diverse and Discriminatory Sets of Instances using Novelty Search in Combinatorial Domains. Evolutionary Computation 2024; doi: https://doi.org/10.1162/evco_a_00350 Marrero, A. 2024. Evolutionary Computation Methods for Instance Generation in Optimisation Domains. PhD thesis. Universidad de La Laguna. https://riull.ull.es/xmlui/handle/915/37726","title":"Publications"},{"location":"#how-to-cite-dignea","text":"If you use DIGNEA in your research work, please cite the following: DIGNEA: A tool to generate diverse and discriminatory instance suites for optimisation domains Alejandro Marrero, Eduardo Segredo, Coromoto Le\u00f3n, Emma Hart SoftwareX , Volume 22, 2023, Page 101355 DOI: 10.1016/j.softx.2023.101355 URL: https://www.sciencedirect.com/science/article/pii/S2352711023000511 Keywords : Instance generation, Novelty search, Evolutionary algorithm, Optimisation, Knapsack problem Abstract : To advance research in the development of optimisation algorithms, it is crucial to have access to large test-beds of diverse and discriminatory instances from a domain that can highlight strengths and weaknesses of different algorithms. The DIGNEA tool enables diverse instance suites to be generated for any domain, that are also discriminatory with respect to a set of solvers of the user's choice. Written in C++, and delivered as a repository and as a Docker image, its modular and template-based design enables it to be easily adapted to multiple domains and types of solvers with minimal effort. This paper exemplifies how to generate instances for the Knapsack Problem domain.","title":"How to Cite DIGNEA"},{"location":"CONTRIBUTING/","text":"Contributing Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways, follow this step-by-step guide to start contributing to Dignea. Report bugs Report bugs at https://github.com/amarrerod/digneapy/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug. Fix Bugs Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it. Implement Features Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it. However, you can also propose new features via PR. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :) Write Documentation Dignea could always use more documentation and examples, whether as part of the official digneapy docs, in docstrings, or even on the web in blog posts, articles, and such. Get Started! Ready to contribute? Here's how to set up digneapy for local development. Fork the digneapy repo on GitHub. Clone your fork locally $ git clone git@github.com:your_name_here/digneapy.git Install your local copy into a virtualenv. Assuming you have uv installed, this is how you set up your fork for local development:: $ cd digneapy $ uv venv $ source .venv/bin/activate $ uv pip -r requirements.txt $ ud pip -r requirements_dev.txt Create a branch for local development:: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass ruff and the tests, including testing other Python versions with tox:: $ ruff check digneapy tests $ uv run pytest --doctest-modules tests Commit your changes and push your branch to GitHub:: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website. Pull Request Guidelines Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring. The pull request should work for Python 3.12 and 3.13. Check Github Actions and make sure that the tests pass for all supported Python versions. Tips To run a subset of tests: $ uv run pytest --doctest-modules tests/new_directory/test_new_feature.py","title":"Contributing"},{"location":"CONTRIBUTING/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways, follow this step-by-step guide to start contributing to Dignea.","title":"Contributing"},{"location":"CONTRIBUTING/#report-bugs","text":"Report bugs at https://github.com/amarrerod/digneapy/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug.","title":"Report bugs"},{"location":"CONTRIBUTING/#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.","title":"Fix Bugs"},{"location":"CONTRIBUTING/#implement-features","text":"Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it. However, you can also propose new features via PR. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :)","title":"Implement Features"},{"location":"CONTRIBUTING/#write-documentation","text":"Dignea could always use more documentation and examples, whether as part of the official digneapy docs, in docstrings, or even on the web in blog posts, articles, and such.","title":"Write Documentation"},{"location":"CONTRIBUTING/#get-started","text":"Ready to contribute? Here's how to set up digneapy for local development. Fork the digneapy repo on GitHub. Clone your fork locally $ git clone git@github.com:your_name_here/digneapy.git Install your local copy into a virtualenv. Assuming you have uv installed, this is how you set up your fork for local development:: $ cd digneapy $ uv venv $ source .venv/bin/activate $ uv pip -r requirements.txt $ ud pip -r requirements_dev.txt Create a branch for local development:: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass ruff and the tests, including testing other Python versions with tox:: $ ruff check digneapy tests $ uv run pytest --doctest-modules tests Commit your changes and push your branch to GitHub:: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website.","title":"Get Started!"},{"location":"CONTRIBUTING/#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring. The pull request should work for Python 3.12 and 3.13. Check Github Actions and make sure that the tests pass for all supported Python versions.","title":"Pull Request Guidelines"},{"location":"CONTRIBUTING/#tips","text":"To run a subset of tests: $ uv run pytest --doctest-modules tests/new_directory/test_new_feature.py","title":"Tips"},{"location":"authors/","text":"Credits Development Lead Alejandro Marrero amarrerd@ull.edu.es Contributors None yet. Why not be the first?","title":"Credits"},{"location":"authors/#credits","text":"","title":"Credits"},{"location":"authors/#development-lead","text":"Alejandro Marrero amarrerd@ull.edu.es","title":"Development Lead"},{"location":"authors/#contributors","text":"None yet. Why not be the first?","title":"Contributors"},{"location":"documentation/","text":"","title":"Documentation"},{"location":"example/","text":"Reference Class Archive Stores a collection of diverse Instances Source code in digneapy/archives/_base_archive.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 class Archive : \"\"\"Class Archive Stores a collection of diverse Instances \"\"\" def __init__ ( self , threshold : float , instances : Optional [ Sequence [ Instance ]] = None , dtype = np . float64 , ): \"\"\"Creates an instance of a Archive (unstructured) for QD algorithms Args: threshold (float): Minimum value of sparseness to include an Instance into the archive. instances (Iterable[Instance], optional): Instances to initialise the archive. Defaults to None. \"\"\" self . _storage = { \"instances\" : [], \"descriptors\" : []} if instances : self . _storage [ \"instances\" ] . extend ( instances ) self . _storage [ \"descriptors\" ] . extend ( np . asarray ([ instance . descriptor for instance in instances ]) ) self . _threshold = threshold self . _dtype = dtype @property def instances ( self ) -> Sequence [ Instance ]: return self . _storage [ \"instances\" ] @property def descriptors ( self ) -> np . ndarray : return np . asarray ( self . _storage [ \"descriptors\" ]) @property def threshold ( self ): return self . _threshold @threshold . setter def threshold ( self , t : float ): try : t_f = float ( t ) except Exception : msg = f \"The threshold value { t } is not a float in 'threshold' setter of class { self . __class__ . __name__ } \" raise TypeError ( msg ) self . _threshold = t_f def __iter__ ( self ): return iter ( self . _storage [ \"instances\" ]) def __str__ ( self ): return f \"Archive(threshold= { self . _threshold } ,data=(| { len ( self ) } |))\" def __repr__ ( self ): return f \"Archive(threshold= { self . _threshold } ,data=(| { len ( self ) } |))\" def __array__ ( self , dtype = None , copy = None ) -> np . ndarray : \"\"\"Creates a ndarray with the descriptors >>> import numpy as np >>> descriptors = [list(range(d, d + 5)) for d in range(10)] >>> archive = Archive(descriptors) >>> np_archive = np.array(archive) >>> assert len(np_archive) == len(archive) >>> assert type(np_archive) == type(np.zeros(1)) \"\"\" return np . asarray ( self . _storage [ \"instances\" ], dtype = dtype , copy = copy ) def __eq__ ( self , other : Self ): \"\"\"Compares whether to Archives are equal >>> import copy >>> variables = [list(range(d, d + 5)) for d in range(10)] >>> instances = [Instance(variables=v, s=1.0) for v in variables] >>> archive = Archive(threshold=0.0, instances=instances) >>> empty_archive = Archive(threshold=0.0) >>> a1 = copy.copy(archive) >>> assert a1 == archive >>> assert empty_archive != archive \"\"\" return len ( self ) == len ( other ) and all ( np . array_equal ( a , b ) for a , b in zip ( self . _storage [ \"descriptors\" ], other . _storage [ \"descriptors\" ]) ) def __hash__ ( self ): from functools import reduce hashes = ( hash ( i ) for i in self . instances ) return reduce ( lambda a , b : a ^ b , hashes , 0 ) def __bool__ ( self ): \"\"\"Returns True if len(self) > 1 >>> descriptors = [list(range(d, d + 5)) for d in range(10)] >>> archive = Archive(threshold=0.0, instances=descriptors) >>> empty_archive = Archive(threshold=0.0) >>> assert archive >>> assert not empty_archive \"\"\" return len ( self ) != 0 def __len__ ( self ): return len ( self . instances ) def __getitem__ ( self , key ): if isinstance ( key , slice ): cls = type ( self ) # To facilitate subclassing return cls ( self . _threshold , self . instances [ key ]) index = operator . index ( key ) return self . _storage [ \"instances\" ][ index ] def extend ( self , instances : Sequence [ Instance ], novelty_scores : Optional [ np . ndarray ] = None , descriptors : Optional [ np . ndarray ] = None , ): \"\"\"Extends the current archive with all the individuals inside iterable that have a sparseness value greater than the archive threshold. Args: instances (Sequence[Instance]): Sequence of instances to be include in the archive. \"\"\" scores = ( novelty_scores if novelty_scores is not None else np . asarray ([ instance . s for instance in instances ]) ) descriptors = ( descriptors if descriptors is not None else np . asarray ([ instance . descriptor for instance in instances ]) ) to_insert = np . where ( scores >= self . threshold )[ 0 ] self . _storage [ \"instances\" ] . extend (( instances [ i ] for i in to_insert )) self . _storage [ \"descriptors\" ] . extend ( descriptors [ to_insert ]) def __format__ ( self , fmt_spec = \"\" ): variables = self outer_fmt = \"( {} )\" components = ( format ( c , fmt_spec ) for c in variables ) return outer_fmt . format ( \", \" . join ( components )) def asdict ( self ) -> dict : return { \"threshold\" : self . _threshold , \"instances\" : { i : instance . asdict () for i , instance in enumerate ( self . _storage [ \"instances\" ]) }, } def to_json ( self ) -> str : \"\"\"Converts the archive into a JSON object Returns: str: JSON str of the archive content \"\"\" return json . dumps ( self . asdict (), indent = 4 ) __array__ ( dtype = None , copy = None ) Creates a ndarray with the descriptors import numpy as np descriptors = [list(range(d, d + 5)) for d in range(10)] archive = Archive(descriptors) np_archive = np.array(archive) assert len(np_archive) == len(archive) assert type(np_archive) == type(np.zeros(1)) Source code in digneapy/archives/_base_archive.py 81 82 83 84 85 86 87 88 89 90 91 def __array__ ( self , dtype = None , copy = None ) -> np . ndarray : \"\"\"Creates a ndarray with the descriptors >>> import numpy as np >>> descriptors = [list(range(d, d + 5)) for d in range(10)] >>> archive = Archive(descriptors) >>> np_archive = np.array(archive) >>> assert len(np_archive) == len(archive) >>> assert type(np_archive) == type(np.zeros(1)) \"\"\" return np . asarray ( self . _storage [ \"instances\" ], dtype = dtype , copy = copy ) __bool__ () Returns True if len(self) > 1 descriptors = [list(range(d, d + 5)) for d in range(10)] archive = Archive(threshold=0.0, instances=descriptors) empty_archive = Archive(threshold=0.0) assert archive assert not empty_archive Source code in digneapy/archives/_base_archive.py 117 118 119 120 121 122 123 124 125 126 127 def __bool__ ( self ): \"\"\"Returns True if len(self) > 1 >>> descriptors = [list(range(d, d + 5)) for d in range(10)] >>> archive = Archive(threshold=0.0, instances=descriptors) >>> empty_archive = Archive(threshold=0.0) >>> assert archive >>> assert not empty_archive \"\"\" return len ( self ) != 0 __eq__ ( other ) Compares whether to Archives are equal import copy variables = [list(range(d, d + 5)) for d in range(10)] instances = [Instance(variables=v, s=1.0) for v in variables] archive = Archive(threshold=0.0, instances=instances) empty_archive = Archive(threshold=0.0) a1 = copy.copy(archive) assert a1 == archive assert empty_archive != archive Source code in digneapy/archives/_base_archive.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def __eq__ ( self , other : Self ): \"\"\"Compares whether to Archives are equal >>> import copy >>> variables = [list(range(d, d + 5)) for d in range(10)] >>> instances = [Instance(variables=v, s=1.0) for v in variables] >>> archive = Archive(threshold=0.0, instances=instances) >>> empty_archive = Archive(threshold=0.0) >>> a1 = copy.copy(archive) >>> assert a1 == archive >>> assert empty_archive != archive \"\"\" return len ( self ) == len ( other ) and all ( np . array_equal ( a , b ) for a , b in zip ( self . _storage [ \"descriptors\" ], other . _storage [ \"descriptors\" ]) ) __init__ ( threshold , instances = None , dtype = np . float64 ) Creates an instance of a Archive (unstructured) for QD algorithms Parameters: threshold ( float ) \u2013 Minimum value of sparseness to include an Instance into the archive. instances ( Iterable [ Instance ] , default: None ) \u2013 Instances to initialise the archive. Defaults to None. Source code in digneapy/archives/_base_archive.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , threshold : float , instances : Optional [ Sequence [ Instance ]] = None , dtype = np . float64 , ): \"\"\"Creates an instance of a Archive (unstructured) for QD algorithms Args: threshold (float): Minimum value of sparseness to include an Instance into the archive. instances (Iterable[Instance], optional): Instances to initialise the archive. Defaults to None. \"\"\" self . _storage = { \"instances\" : [], \"descriptors\" : []} if instances : self . _storage [ \"instances\" ] . extend ( instances ) self . _storage [ \"descriptors\" ] . extend ( np . asarray ([ instance . descriptor for instance in instances ]) ) self . _threshold = threshold self . _dtype = dtype extend ( instances , novelty_scores = None , descriptors = None ) Extends the current archive with all the individuals inside iterable that have a sparseness value greater than the archive threshold. Parameters: instances ( Sequence [ Instance ] ) \u2013 Sequence of instances to be include in the archive. Source code in digneapy/archives/_base_archive.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def extend ( self , instances : Sequence [ Instance ], novelty_scores : Optional [ np . ndarray ] = None , descriptors : Optional [ np . ndarray ] = None , ): \"\"\"Extends the current archive with all the individuals inside iterable that have a sparseness value greater than the archive threshold. Args: instances (Sequence[Instance]): Sequence of instances to be include in the archive. \"\"\" scores = ( novelty_scores if novelty_scores is not None else np . asarray ([ instance . s for instance in instances ]) ) descriptors = ( descriptors if descriptors is not None else np . asarray ([ instance . descriptor for instance in instances ]) ) to_insert = np . where ( scores >= self . threshold )[ 0 ] self . _storage [ \"instances\" ] . extend (( instances [ i ] for i in to_insert )) self . _storage [ \"descriptors\" ] . extend ( descriptors [ to_insert ]) to_json () Converts the archive into a JSON object Returns: str ( str ) \u2013 JSON str of the archive content Source code in digneapy/archives/_base_archive.py 181 182 183 184 185 186 187 188 def to_json ( self ) -> str : \"\"\"Converts the archive into a JSON object Returns: str: JSON str of the archive content \"\"\" return json . dumps ( self . asdict (), indent = 4 )","title":"Reference"},{"location":"example/#reference","text":"Class Archive Stores a collection of diverse Instances Source code in digneapy/archives/_base_archive.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 class Archive : \"\"\"Class Archive Stores a collection of diverse Instances \"\"\" def __init__ ( self , threshold : float , instances : Optional [ Sequence [ Instance ]] = None , dtype = np . float64 , ): \"\"\"Creates an instance of a Archive (unstructured) for QD algorithms Args: threshold (float): Minimum value of sparseness to include an Instance into the archive. instances (Iterable[Instance], optional): Instances to initialise the archive. Defaults to None. \"\"\" self . _storage = { \"instances\" : [], \"descriptors\" : []} if instances : self . _storage [ \"instances\" ] . extend ( instances ) self . _storage [ \"descriptors\" ] . extend ( np . asarray ([ instance . descriptor for instance in instances ]) ) self . _threshold = threshold self . _dtype = dtype @property def instances ( self ) -> Sequence [ Instance ]: return self . _storage [ \"instances\" ] @property def descriptors ( self ) -> np . ndarray : return np . asarray ( self . _storage [ \"descriptors\" ]) @property def threshold ( self ): return self . _threshold @threshold . setter def threshold ( self , t : float ): try : t_f = float ( t ) except Exception : msg = f \"The threshold value { t } is not a float in 'threshold' setter of class { self . __class__ . __name__ } \" raise TypeError ( msg ) self . _threshold = t_f def __iter__ ( self ): return iter ( self . _storage [ \"instances\" ]) def __str__ ( self ): return f \"Archive(threshold= { self . _threshold } ,data=(| { len ( self ) } |))\" def __repr__ ( self ): return f \"Archive(threshold= { self . _threshold } ,data=(| { len ( self ) } |))\" def __array__ ( self , dtype = None , copy = None ) -> np . ndarray : \"\"\"Creates a ndarray with the descriptors >>> import numpy as np >>> descriptors = [list(range(d, d + 5)) for d in range(10)] >>> archive = Archive(descriptors) >>> np_archive = np.array(archive) >>> assert len(np_archive) == len(archive) >>> assert type(np_archive) == type(np.zeros(1)) \"\"\" return np . asarray ( self . _storage [ \"instances\" ], dtype = dtype , copy = copy ) def __eq__ ( self , other : Self ): \"\"\"Compares whether to Archives are equal >>> import copy >>> variables = [list(range(d, d + 5)) for d in range(10)] >>> instances = [Instance(variables=v, s=1.0) for v in variables] >>> archive = Archive(threshold=0.0, instances=instances) >>> empty_archive = Archive(threshold=0.0) >>> a1 = copy.copy(archive) >>> assert a1 == archive >>> assert empty_archive != archive \"\"\" return len ( self ) == len ( other ) and all ( np . array_equal ( a , b ) for a , b in zip ( self . _storage [ \"descriptors\" ], other . _storage [ \"descriptors\" ]) ) def __hash__ ( self ): from functools import reduce hashes = ( hash ( i ) for i in self . instances ) return reduce ( lambda a , b : a ^ b , hashes , 0 ) def __bool__ ( self ): \"\"\"Returns True if len(self) > 1 >>> descriptors = [list(range(d, d + 5)) for d in range(10)] >>> archive = Archive(threshold=0.0, instances=descriptors) >>> empty_archive = Archive(threshold=0.0) >>> assert archive >>> assert not empty_archive \"\"\" return len ( self ) != 0 def __len__ ( self ): return len ( self . instances ) def __getitem__ ( self , key ): if isinstance ( key , slice ): cls = type ( self ) # To facilitate subclassing return cls ( self . _threshold , self . instances [ key ]) index = operator . index ( key ) return self . _storage [ \"instances\" ][ index ] def extend ( self , instances : Sequence [ Instance ], novelty_scores : Optional [ np . ndarray ] = None , descriptors : Optional [ np . ndarray ] = None , ): \"\"\"Extends the current archive with all the individuals inside iterable that have a sparseness value greater than the archive threshold. Args: instances (Sequence[Instance]): Sequence of instances to be include in the archive. \"\"\" scores = ( novelty_scores if novelty_scores is not None else np . asarray ([ instance . s for instance in instances ]) ) descriptors = ( descriptors if descriptors is not None else np . asarray ([ instance . descriptor for instance in instances ]) ) to_insert = np . where ( scores >= self . threshold )[ 0 ] self . _storage [ \"instances\" ] . extend (( instances [ i ] for i in to_insert )) self . _storage [ \"descriptors\" ] . extend ( descriptors [ to_insert ]) def __format__ ( self , fmt_spec = \"\" ): variables = self outer_fmt = \"( {} )\" components = ( format ( c , fmt_spec ) for c in variables ) return outer_fmt . format ( \", \" . join ( components )) def asdict ( self ) -> dict : return { \"threshold\" : self . _threshold , \"instances\" : { i : instance . asdict () for i , instance in enumerate ( self . _storage [ \"instances\" ]) }, } def to_json ( self ) -> str : \"\"\"Converts the archive into a JSON object Returns: str: JSON str of the archive content \"\"\" return json . dumps ( self . asdict (), indent = 4 )","title":"Reference"},{"location":"example/#digneapy.archives.Archive.__array__","text":"Creates a ndarray with the descriptors import numpy as np descriptors = [list(range(d, d + 5)) for d in range(10)] archive = Archive(descriptors) np_archive = np.array(archive) assert len(np_archive) == len(archive) assert type(np_archive) == type(np.zeros(1)) Source code in digneapy/archives/_base_archive.py 81 82 83 84 85 86 87 88 89 90 91 def __array__ ( self , dtype = None , copy = None ) -> np . ndarray : \"\"\"Creates a ndarray with the descriptors >>> import numpy as np >>> descriptors = [list(range(d, d + 5)) for d in range(10)] >>> archive = Archive(descriptors) >>> np_archive = np.array(archive) >>> assert len(np_archive) == len(archive) >>> assert type(np_archive) == type(np.zeros(1)) \"\"\" return np . asarray ( self . _storage [ \"instances\" ], dtype = dtype , copy = copy )","title":"__array__"},{"location":"example/#digneapy.archives.Archive.__bool__","text":"Returns True if len(self) > 1 descriptors = [list(range(d, d + 5)) for d in range(10)] archive = Archive(threshold=0.0, instances=descriptors) empty_archive = Archive(threshold=0.0) assert archive assert not empty_archive Source code in digneapy/archives/_base_archive.py 117 118 119 120 121 122 123 124 125 126 127 def __bool__ ( self ): \"\"\"Returns True if len(self) > 1 >>> descriptors = [list(range(d, d + 5)) for d in range(10)] >>> archive = Archive(threshold=0.0, instances=descriptors) >>> empty_archive = Archive(threshold=0.0) >>> assert archive >>> assert not empty_archive \"\"\" return len ( self ) != 0","title":"__bool__"},{"location":"example/#digneapy.archives.Archive.__eq__","text":"Compares whether to Archives are equal import copy variables = [list(range(d, d + 5)) for d in range(10)] instances = [Instance(variables=v, s=1.0) for v in variables] archive = Archive(threshold=0.0, instances=instances) empty_archive = Archive(threshold=0.0) a1 = copy.copy(archive) assert a1 == archive assert empty_archive != archive Source code in digneapy/archives/_base_archive.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def __eq__ ( self , other : Self ): \"\"\"Compares whether to Archives are equal >>> import copy >>> variables = [list(range(d, d + 5)) for d in range(10)] >>> instances = [Instance(variables=v, s=1.0) for v in variables] >>> archive = Archive(threshold=0.0, instances=instances) >>> empty_archive = Archive(threshold=0.0) >>> a1 = copy.copy(archive) >>> assert a1 == archive >>> assert empty_archive != archive \"\"\" return len ( self ) == len ( other ) and all ( np . array_equal ( a , b ) for a , b in zip ( self . _storage [ \"descriptors\" ], other . _storage [ \"descriptors\" ]) )","title":"__eq__"},{"location":"example/#digneapy.archives.Archive.__init__","text":"Creates an instance of a Archive (unstructured) for QD algorithms Parameters: threshold ( float ) \u2013 Minimum value of sparseness to include an Instance into the archive. instances ( Iterable [ Instance ] , default: None ) \u2013 Instances to initialise the archive. Defaults to None. Source code in digneapy/archives/_base_archive.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , threshold : float , instances : Optional [ Sequence [ Instance ]] = None , dtype = np . float64 , ): \"\"\"Creates an instance of a Archive (unstructured) for QD algorithms Args: threshold (float): Minimum value of sparseness to include an Instance into the archive. instances (Iterable[Instance], optional): Instances to initialise the archive. Defaults to None. \"\"\" self . _storage = { \"instances\" : [], \"descriptors\" : []} if instances : self . _storage [ \"instances\" ] . extend ( instances ) self . _storage [ \"descriptors\" ] . extend ( np . asarray ([ instance . descriptor for instance in instances ]) ) self . _threshold = threshold self . _dtype = dtype","title":"__init__"},{"location":"example/#digneapy.archives.Archive.extend","text":"Extends the current archive with all the individuals inside iterable that have a sparseness value greater than the archive threshold. Parameters: instances ( Sequence [ Instance ] ) \u2013 Sequence of instances to be include in the archive. Source code in digneapy/archives/_base_archive.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def extend ( self , instances : Sequence [ Instance ], novelty_scores : Optional [ np . ndarray ] = None , descriptors : Optional [ np . ndarray ] = None , ): \"\"\"Extends the current archive with all the individuals inside iterable that have a sparseness value greater than the archive threshold. Args: instances (Sequence[Instance]): Sequence of instances to be include in the archive. \"\"\" scores = ( novelty_scores if novelty_scores is not None else np . asarray ([ instance . s for instance in instances ]) ) descriptors = ( descriptors if descriptors is not None else np . asarray ([ instance . descriptor for instance in instances ]) ) to_insert = np . where ( scores >= self . threshold )[ 0 ] self . _storage [ \"instances\" ] . extend (( instances [ i ] for i in to_insert )) self . _storage [ \"descriptors\" ] . extend ( descriptors [ to_insert ])","title":"extend"},{"location":"example/#digneapy.archives.Archive.to_json","text":"Converts the archive into a JSON object Returns: str ( str ) \u2013 JSON str of the archive content Source code in digneapy/archives/_base_archive.py 181 182 183 184 185 186 187 188 def to_json ( self ) -> str : \"\"\"Converts the archive into a JSON object Returns: str: JSON str of the archive content \"\"\" return json . dumps ( self . asdict (), indent = 4 )","title":"to_json"},{"location":"installation/","text":"Installation Stable release To install digneapy, run this command in your terminal: $ pip install digneapy This is the preferred method to install digneapy, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process. pip Python installation guide","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#stable-release","text":"To install digneapy, run this command in your terminal: $ pip install digneapy This is the preferred method to install digneapy, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process. pip Python installation guide","title":"Stable release"},{"location":"license/","text":"GNU GENERAL PUBLIC LICENSE Version 3, 29 June 2007 Python Boilerplate contains all the boilerplate you need to create a Python package. Copyright (C) 2023 Alejandro Marrero This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see <http://www.gnu.org/licenses/>. Also add information on how to contact you by electronic and paper mail. You should also get your employer (if you work as a programmer) or school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. For more information on this, and how to apply and follow the GNU GPL, see http://www.gnu.org/licenses/ . The GNU General Public License does not permit incorporating your program into proprietary programs. If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library. If this is what you want to do, use the GNU Lesser General Public License instead of this License. But first, please read http://www.gnu.org/philosophy/why-not-lgpl.html .","title":"License"},{"location":"reference/SUMMARY/","text":"core constants domain instance metrics novelty_search problem solution solver descriptors scores types archives base_archive cvt_archive grid_archive domains bpp kp tsp generators operators crossover mutation replacement selection solvers bpp evolutionary pisinger tsp utils transformers base autoencoders neural pca tuner utils timers save_data serializer sorting visualize evo_generator_evolution_plot map_elites_evolution_plot","title":"Overview"},{"location":"reference/generators/","text":"@File : generators.py @Time : 2023/10/30 14:20:21 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2023, Alejandro Marrero @Desc : None DEAGenerator Bases: EAGenerator Object to generate instances based on a Evolutionary Algorithn with a Dominated Novelty Search approach Source code in digneapy/generators.py 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 class DEAGenerator ( EAGenerator ): \"\"\" Object to generate instances based on a Evolutionary Algorithn with a Dominated Novelty Search approach \"\"\" def __init__ ( self , domain : Domain , portfolio : Iterable [ SupportsSolve [ P ]], pop_size : int = 128 , offspring_size : int = 128 , generations : int = 1000 , k : int = 15 , descriptor_strategy : str = \"features\" , transformer : Optional [ SupportsTransform ] = None , repetitions : int = 1 , cxrate : float = 0.5 , mutrate : float = 0.8 , crossover : Crossover = uniform_crossover , mutation : Mutation = uniform_one_mutation , selection : Selection = binary_tournament_selection , performance_function : PerformanceFn = max_gap_target , ): \"\"\"Creates a Evolutionary Instance Generator based on Novelty Search Args: domain (Domain): Domain for which the instances are generated for. portfolio (Iterable[SupportSolve]): Iterable item of callable objects that can evaluate a instance. pop_size (int, optional): Number of instances in the population to evolve. Defaults to 128. offspring_size (int, optional): Number of instances in the offspring population. Defaults to 128. generations (int, optional): Number of total generations to perform. Defaults to 1000. k (int, optional): Number of neighbours to calculate the sparseness. Defaults to 15. descriptor_strategy (str, optional): Descriptor used to calculate the diversity. The options available are defined in the dictionary digneapy.qd.descriptor_strategies. Defaults to \"features\". transformer (callable, optional): Define a strategy to transform the high-dimensional descriptors to low-dimensional.Defaults to None. repetitions (int, optional): Number times a solver in the portfolio must be run over the same instance. Defaults to 1. cxrate (float, optional): Crossover rate. Defaults to 0.5. mutrate (float, optional): Mutation rate. Defaults to 0.8. crossover (Crossover, optional): Crossover operator. Defaults to uniform_crossover. mutation (Mutation, optional): Mutation operator. Defaults to uniform_one_mutation. selection (Selection, optional): Selection operator. Defaults to binary_tournament_selection. performance_function (PerformanceFn, optional): Performance function to calculate the performance score. Defaults to max_gap_target. \"\"\" super () . __init__ ( domain = domain , portfolio = portfolio , pop_size = pop_size , novelty_approach = None , generations = generations , descriptor_strategy = descriptor_strategy , transformer = transformer , repetitions = repetitions , cxrate = cxrate , mutrate = mutrate , crossover = crossover , mutation = mutation , selection = selection , performance_function = performance_function , ) self . k = k self . offspring_size = offspring_size def __str__ ( self ): port_names = [ s . __name__ for s in self . portfolio ] domain_name = self . domain . name if self . domain is not None else \"None\" return f \"DEAGenerator(pop_size= { self . pop_size } ,gen= { self . generations } ,domain= { domain_name } ,portfolio= { port_names !r} )\" def __repr__ ( self ) -> str : port_names = [ s . __name__ for s in self . portfolio ] domain_name = self . domain . name if self . domain is not None else \"None\" return f \"DEAGenerator<pop_size= { self . pop_size } ,gen= { self . generations } ,domain= { domain_name } ,portfolio= { port_names !r} >\" def __call__ ( self , verbose : bool = False ) -> GenResult : if self . domain is None : raise ValueError ( \"You must specify a domain to run the generator.\" ) if len ( self . portfolio ) == 0 : raise ValueError ( \"The portfolio is empty. To run the generator you must provide a valid portfolio of solvers\" ) self . population = self . domain . generate_instances ( n = self . pop_size ) perf_biases , portfolio_scores = self . _evaluate_population ( self . population ) descriptors , features = self . _update_descriptors ( self . population , portfolio_scores = portfolio_scores ) if features is not None : combined_features = np . empty ( shape = ( self . pop_size * 2 , features . shape [ 1 ]), dtype = np . float32 ) for pgen in range ( self . generations ): offspring = self . _generate_offspring ( self . pop_size ) off_perf_biases , off_portfolio_scores = self . _evaluate_population ( offspring ) off_descriptors , off_features = self . _update_descriptors ( offspring , portfolio_scores = off_portfolio_scores ) combined_descriptors = np . concatenate ( ( descriptors , off_descriptors ), axis = 0 ) combined_performances = np . concatenate ( ( perf_biases , off_perf_biases ), axis = 0 ) combined_port_scores = np . concatenate ( ( portfolio_scores , portfolio_scores ), axis = 0 ) genotypes = np . concatenate ( ( np . asarray ( self . population , copy = True ), offspring ), axis = 0 ) if features is not None : combined_features = np . concatenate (( features , off_features ), axis = 0 ) ( sorted_descriptors , sorted_performances , sorted_competition_fitness , sorted_indexing , ) = dominated_novelty_search ( descriptors = combined_descriptors , performances = combined_performances , k = self . k , force_feasible_only = True , ) # Keep the top N for the next generation sorted_indexing = sorted_indexing [: self . pop_size ] fitness = sorted_competition_fitness [: self . pop_size ] descriptors = sorted_descriptors [: self . pop_size ] perf_biases = sorted_performances [: self . pop_size ] # Track from the combined arrays based on the indexing portfolio_scores = combined_port_scores [ sorted_indexing ] genotypes = genotypes [ sorted_indexing ] if features is not None : features = combined_features [ sorted_indexing ] # Both population and offspring are used in the replacement # Record the stats and update the performed gens self . population = [ Instance ( variables = genotypes [ i ], fitness = fitness [ i ], descriptor = descriptors [ i ], portfolio_scores = portfolio_scores [ i ], p = perf_biases [ i ], features = features [ i ] if features is not None else (), ) for i in range ( self . pop_size ) ] self . _logbook . update ( generation = pgen , population = self . population , feedback = verbose ) if verbose : # Clear the terminal blank = \" \" * 80 print ( f \" \\r { blank } \\r \" , end = \"\" ) return GenResult ( target = self . portfolio [ 0 ] . __name__ , instances = self . population , history = self . _logbook , ) __init__ ( domain , portfolio , pop_size = 128 , offspring_size = 128 , generations = 1000 , k = 15 , descriptor_strategy = 'features' , transformer = None , repetitions = 1 , cxrate = 0.5 , mutrate = 0.8 , crossover = uniform_crossover , mutation = uniform_one_mutation , selection = binary_tournament_selection , performance_function = max_gap_target ) Creates a Evolutionary Instance Generator based on Novelty Search Parameters: domain ( Domain ) \u2013 Domain for which the instances are generated for. portfolio ( Iterable [ SupportSolve ] ) \u2013 Iterable item of callable objects that can evaluate a instance. pop_size ( int , default: 128 ) \u2013 Number of instances in the population to evolve. Defaults to 128. offspring_size ( int , default: 128 ) \u2013 Number of instances in the offspring population. Defaults to 128. generations ( int , default: 1000 ) \u2013 Number of total generations to perform. Defaults to 1000. k ( int , default: 15 ) \u2013 Number of neighbours to calculate the sparseness. Defaults to 15. descriptor_strategy ( str , default: 'features' ) \u2013 Descriptor used to calculate the diversity. The options available are defined in the dictionary digneapy.qd.descriptor_strategies. Defaults to \"features\". transformer ( callable , default: None ) \u2013 Define a strategy to transform the high-dimensional descriptors to low-dimensional.Defaults to None. repetitions ( int , default: 1 ) \u2013 Number times a solver in the portfolio must be run over the same instance. Defaults to 1. cxrate ( float , default: 0.5 ) \u2013 Crossover rate. Defaults to 0.5. mutrate ( float , default: 0.8 ) \u2013 Mutation rate. Defaults to 0.8. crossover ( Crossover , default: uniform_crossover ) \u2013 Crossover operator. Defaults to uniform_crossover. mutation ( Mutation , default: uniform_one_mutation ) \u2013 Mutation operator. Defaults to uniform_one_mutation. selection ( Selection , default: binary_tournament_selection ) \u2013 Selection operator. Defaults to binary_tournament_selection. performance_function ( PerformanceFn , default: max_gap_target ) \u2013 Performance function to calculate the performance score. Defaults to max_gap_target. Source code in digneapy/generators.py 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 def __init__ ( self , domain : Domain , portfolio : Iterable [ SupportsSolve [ P ]], pop_size : int = 128 , offspring_size : int = 128 , generations : int = 1000 , k : int = 15 , descriptor_strategy : str = \"features\" , transformer : Optional [ SupportsTransform ] = None , repetitions : int = 1 , cxrate : float = 0.5 , mutrate : float = 0.8 , crossover : Crossover = uniform_crossover , mutation : Mutation = uniform_one_mutation , selection : Selection = binary_tournament_selection , performance_function : PerformanceFn = max_gap_target , ): \"\"\"Creates a Evolutionary Instance Generator based on Novelty Search Args: domain (Domain): Domain for which the instances are generated for. portfolio (Iterable[SupportSolve]): Iterable item of callable objects that can evaluate a instance. pop_size (int, optional): Number of instances in the population to evolve. Defaults to 128. offspring_size (int, optional): Number of instances in the offspring population. Defaults to 128. generations (int, optional): Number of total generations to perform. Defaults to 1000. k (int, optional): Number of neighbours to calculate the sparseness. Defaults to 15. descriptor_strategy (str, optional): Descriptor used to calculate the diversity. The options available are defined in the dictionary digneapy.qd.descriptor_strategies. Defaults to \"features\". transformer (callable, optional): Define a strategy to transform the high-dimensional descriptors to low-dimensional.Defaults to None. repetitions (int, optional): Number times a solver in the portfolio must be run over the same instance. Defaults to 1. cxrate (float, optional): Crossover rate. Defaults to 0.5. mutrate (float, optional): Mutation rate. Defaults to 0.8. crossover (Crossover, optional): Crossover operator. Defaults to uniform_crossover. mutation (Mutation, optional): Mutation operator. Defaults to uniform_one_mutation. selection (Selection, optional): Selection operator. Defaults to binary_tournament_selection. performance_function (PerformanceFn, optional): Performance function to calculate the performance score. Defaults to max_gap_target. \"\"\" super () . __init__ ( domain = domain , portfolio = portfolio , pop_size = pop_size , novelty_approach = None , generations = generations , descriptor_strategy = descriptor_strategy , transformer = transformer , repetitions = repetitions , cxrate = cxrate , mutrate = mutrate , crossover = crossover , mutation = mutation , selection = selection , performance_function = performance_function , ) self . k = k self . offspring_size = offspring_size EAGenerator Bases: Generator , RNG Object to generate instances based on a Evolutionary Algorithn with set of diverse solutions Source code in digneapy/generators.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 class EAGenerator ( Generator , RNG ): \"\"\"Object to generate instances based on a Evolutionary Algorithn with set of diverse solutions\"\"\" def __init__ ( self , domain : Domain , portfolio : Iterable [ SupportsSolve [ P ]], novelty_approach : NS , pop_size : int = 100 , generations : int = 1000 , solution_set : Optional [ Archive ] = None , descriptor_strategy : str = \"features\" , transformer : Optional [ SupportsTransform ] = None , repetitions : int = 1 , cxrate : float = 0.5 , mutrate : float = 0.8 , crossover : Crossover = uniform_crossover , mutation : Mutation = uniform_one_mutation , selection : Selection = binary_tournament_selection , replacement : Replacement = generational_replacement , performance_function : PerformanceFn = max_gap_target , phi : float = 0.85 , seed : int = 42 , ): \"\"\"Creates a Evolutionary Instance Generator based on Novelty Search The generator uses a set of solvers to evaluate the instances and a novelty search algorithm to guide the evolution of the instances. Args: domain (Domain): Domain for which the instances are generated for. portfolio (Iterable[SupportSolve]): Iterable item of callable objects that can evaluate a instance. pop_size (int, optional): Number of instances in the population to evolve. Defaults to 100. generations (int, optional): Number of generations to perform. Defaults to 1000. solution_set (Optional[Archive], optional): Solution set to store the instances. Defaults to None. descriptor_strategy (str, optional): Descriptor used to calculate the diversity. The options available are defined in the dictionary digneapy.qd.descriptor_strategies. Defaults to \"features\". transformer (callable, optional): Define a strategy to transform the high-dimensional descriptors to low-dimensional.Defaults to None. repetitions (int, optional): Number times a solver in the portfolio must be run over the same instance. Defaults to 1. cxrate (float, optional): Crossover rate. Defaults to 0.5. mutrate (float, optional): Mutation rate. Defaults to 0.8. crossover (Crossover, optional): Crossover operator. Defaults to uniform_crossover. mutation (Mutation, optional): Mutation operator. Defaults to uniform_one_mutation. selection (Selection, optional): Selection operator. Defaults to binary_tournament_selection. replacement (Replacement, optional): Replacement operator. Defaults to generational_replacement. performance_function (PerformanceFn, optional): Performance function to calculate the performance score. Defaults to max_gap_target. phi (float, optional): Phi balance value for the weighted fitness function. Defaults to 0.85. seed (int, optional): Seed for the RNG protocol. Defaults to 42. Raises: ValueError: Raises error if phi is not a floating point value or it is not in the range [0.0-1.0] KeyError: Raises error if the descriptor strategy is not available in the DESCRIPTORS dictionary \"\"\" try : self . _desc_key = descriptor_strategy self . _descriptor_strategy = DESCRIPTORS [ self . _desc_key ] except KeyError : self . _desc_key = \"instance\" self . _descriptor_strategy = DESCRIPTORS [ self . _desc_key ] print ( f \"Descriptor: { descriptor_strategy } not available. Using the full instance as default descriptor\" ) try : phi = float ( phi ) except ValueError : raise ValueError ( \"Phi must be a float number in the range [0.0-1.0].\" ) if phi < 0.0 or phi > 1.0 : msg = f \"Phi must be a float number in the range [0.0-1.0]. Got: { phi } .\" raise ValueError ( msg ) self . phi = phi self . _novelty_search = novelty_approach self . _solution_set = None # By default there's not solution set if solution_set is not None : self . _ns_solution_set = NS ( archive = solution_set , k = 1 ) self . _transformer = transformer self . pop_size = pop_size self . offspring_size = pop_size self . generations = generations self . domain = domain self . portfolio = tuple ( portfolio ) if portfolio else () self . population = [] self . repetitions = repetitions self . cxrate = cxrate self . mutrate = mutrate self . crossover = crossover self . mutation = mutation self . selection = selection self . replacement = replacement self . performance_function = performance_function self . _logbook = Logbook () self . initialize_rng ( seed = seed ) @property def log ( self ) -> Logbook : return self . _logbook def __str__ ( self ): port_names = [ s . __name__ for s in self . portfolio ] domain_name = self . domain . name if self . domain is not None else \"None\" return f \"EAGenerator(pop_size= { self . pop_size } ,gen= { self . generations } ,domain= { domain_name } ,portfolio= { port_names !r} , { self . _novelty_search . __str__ () } )\" def __repr__ ( self ) -> str : port_names = [ s . __name__ for s in self . portfolio ] domain_name = self . domain . name if self . domain is not None else \"None\" return f \"EAGenerator<pop_size= { self . pop_size } ,gen= { self . generations } ,domain= { domain_name } ,portfolio= { port_names !r} , { self . _novelty_search . __repr__ () } >\" def __call__ ( self , verbose : bool = False ) -> GenResult : if self . domain is None : raise ValueError ( \"You must specify a domain to run the generator.\" ) if len ( self . portfolio ) == 0 : raise ValueError ( \"The portfolio is empty. To run the generator you must provide a valid portfolio of solvers\" ) self . population = self . domain . generate_instances ( n = self . pop_size ) perf_biases , portfolio_scores = self . _evaluate_population ( self . population ) descriptors , features = self . _update_descriptors ( self . population , portfolio_scores = portfolio_scores ) for pgen in range ( self . generations ): offspring = self . _generate_offspring ( self . pop_size ) perf_biases , portfolio_scores = self . _evaluate_population ( offspring ) descriptors , features = self . _update_descriptors ( offspring , portfolio_scores = portfolio_scores ) novelty_scores = self . _novelty_search ( instances_descriptors = descriptors ) offspring_fitness = self . __compute_fitness ( perf_biases , novelty_scores ) # Update to include this # 1. Novelty Scores --> novelty_scores # 2. Performance bias --> perf_biases # 3. Fitness --> oiffspring_fitness # 4. Descriptor --> descriptors offspring = [ Instance ( variables = offspring [ i ], fitness = offspring_fitness [ i ], descriptor = descriptors [ i ], portfolio_scores = portfolio_scores [ i ], p = perf_biases [ i ], s = novelty_scores [ i ], features = features [ i ] if features is not None else None , ) for i in range ( len ( offspring )) ] # Only the feasible instances are considered to be included # in the archive and the solution set. feasible_indeces = np . where ( perf_biases > 0 )[ 0 ] self . _novelty_search . archive . extend ( instances = [ offspring [ i ] for i in feasible_indeces ], descriptors = descriptors [ feasible_indeces ], novelty_scores = novelty_scores [ feasible_indeces ], ) if self . _ns_solution_set : # TODO: Here I should only compute the feasible individuals novelty_solution_set = self . _ns_solution_set ( instances_descriptors = descriptors ) self . _ns_solution_set . archive . extend ( instances = [ offspring [ i ] for i in feasible_indeces ], descriptors = descriptors [ feasible_indeces ], novelty_scores = novelty_solution_set [ feasible_indeces ], ) # However the whole offspring population is used in the replacement operator self . population = self . replacement ( self . population , offspring ) # Record the stats and update the performed gens self . _logbook . update ( generation = pgen , population = self . population , feedback = verbose ) if verbose : # Clear the terminal blank = \" \" * 80 print ( f \" \\r { blank } \\r \" , end = \"\" ) _instances = ( self . _ns_solution_set . archive if self . _ns_solution_set is not None else self . _novelty_search . archive ) return GenResult ( target = self . portfolio [ 0 ] . __name__ , instances = _instances , history = self . _logbook , ) def _generate_offspring ( self , offspring_size : int ) -> np . ndarray : \"\"\"Generates a offspring population of size |offspring_size| from the current population Args: offspring_size (int): offspring size. Defaults to pop_size. Returns: Sequence[Instance] Returns a sequence with the instances definitions, the offspring population. \"\"\" offspring = [ None ] * offspring_size # np.empty(offspring_size, dtype=Instance) for i in range ( offspring_size ): p_1 = self . selection ( self . population ) p_2 = self . selection ( self . population ) child = self . __reproduce ( p_1 , p_2 ) offspring [ i ] = child return np . array ( offspring ) def _update_descriptors ( self , population : np . ndarray , portfolio_scores : Optional [ np . ndarray ] = None , ) -> Tuple [ np . ndarray , Optional [ np . ndarray ]]: \"\"\"Updates the descriptors of the population of instances Args: population (Sequence[Instance]): Population of instances to update the descriptors. \"\"\" descriptors = np . empty ( len ( population )) features = None if self . _desc_key == \"features\" : descriptors = self . domain . extract_features ( population ) features = descriptors . copy () elif self . _desc_key == \"performance\" : descriptors = np . mean ( portfolio_scores , axis = 2 ) else : descriptors = self . _descriptor_strategy ( population ) if self . _transformer is not None : # Transform the descriptors if necessary descriptors = self . _transformer ( descriptors ) return ( descriptors , features ) def _evaluate_population ( self , population : Sequence [ Instance ] ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Evaluates the population of instances using the portfolio of solvers. Args: population (Sequence[Instance]): Sequence of instances to evaluate \"\"\" solvers_scores = np . zeros ( shape = ( len ( population ), len ( self . portfolio ), self . repetitions ) ) problems_to_solve = self . domain . generate_problems_from_instances ( population ) for j , problem in enumerate ( problems_to_solve ): for i , solver in enumerate ( self . portfolio ): # There is no need to change anything in the evaluation code when using Pisinger solvers # because the algs. only return one solution per run (len(solutions) == 1) # The same happens with the simple KP heuristics. However, when using Pisinger solvers # the lower the running time the better they're considered to work an instance scores = np . zeros ( self . repetitions ) for rep in range ( self . repetitions ): scores [ rep ] = max ( solver ( problem ), key = attrgetter ( \"fitness\" ) ) . fitness solvers_scores [ j , i , :] = scores mean_solvers_scores = np . mean ( solvers_scores , axis = 2 ) performance_biases = self . performance_function ( mean_solvers_scores ) return performance_biases , solvers_scores def __compute_fitness ( self , performance_biases : np . ndarray , novelty_scores : np . ndarray ) -> np . ndarray : \"\"\"Calculates the fitness of each instance in the population Args: performance_biases (np.ndarray): Performance biases or scores of each instance novelty_scores (np.ndarray): Novelty scores of each instance Returns: fitness of each instance (np.ndarray) \"\"\" phi_r = 1.0 - self . phi fitness = np . zeros ( len ( performance_biases )) fitness = ( fitness * self . phi ) + ( novelty_scores * phi_r ) return fitness def __reproduce ( self , parent_1 : Instance , parent_2 : Instance ) -> Instance : \"\"\"Generates a new offspring instance from two parent instances Args: parent_1 (Instance): First Parent parent_1 (Instance): Second Parent Returns: Instance: New offspring \"\"\" offspring = parent_1 . clone () if self . _rng . random () < self . cxrate : offspring = self . crossover ( offspring , parent_2 ) return self . mutation ( offspring , self . domain . bounds ) else : return self . mutation ( offspring , self . domain . bounds ) __compute_fitness ( performance_biases , novelty_scores ) Calculates the fitness of each instance in the population Parameters: performance_biases ( ndarray ) \u2013 Performance biases or scores of each instance novelty_scores ( ndarray ) \u2013 Novelty scores of each instance Returns: ndarray \u2013 fitness of each instance (np.ndarray) Source code in digneapy/generators.py 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 def __compute_fitness ( self , performance_biases : np . ndarray , novelty_scores : np . ndarray ) -> np . ndarray : \"\"\"Calculates the fitness of each instance in the population Args: performance_biases (np.ndarray): Performance biases or scores of each instance novelty_scores (np.ndarray): Novelty scores of each instance Returns: fitness of each instance (np.ndarray) \"\"\" phi_r = 1.0 - self . phi fitness = np . zeros ( len ( performance_biases )) fitness = ( fitness * self . phi ) + ( novelty_scores * phi_r ) return fitness __init__ ( domain , portfolio , novelty_approach , pop_size = 100 , generations = 1000 , solution_set = None , descriptor_strategy = 'features' , transformer = None , repetitions = 1 , cxrate = 0.5 , mutrate = 0.8 , crossover = uniform_crossover , mutation = uniform_one_mutation , selection = binary_tournament_selection , replacement = generational_replacement , performance_function = max_gap_target , phi = 0.85 , seed = 42 ) Creates a Evolutionary Instance Generator based on Novelty Search The generator uses a set of solvers to evaluate the instances and a novelty search algorithm to guide the evolution of the instances. Parameters: domain ( Domain ) \u2013 Domain for which the instances are generated for. portfolio ( Iterable [ SupportSolve ] ) \u2013 Iterable item of callable objects that can evaluate a instance. pop_size ( int , default: 100 ) \u2013 Number of instances in the population to evolve. Defaults to 100. generations ( int , default: 1000 ) \u2013 Number of generations to perform. Defaults to 1000. solution_set ( Optional [ Archive ] , default: None ) \u2013 Solution set to store the instances. Defaults to None. descriptor_strategy ( str , default: 'features' ) \u2013 Descriptor used to calculate the diversity. The options available are defined in the dictionary digneapy.qd.descriptor_strategies. Defaults to \"features\". transformer ( callable , default: None ) \u2013 Define a strategy to transform the high-dimensional descriptors to low-dimensional.Defaults to None. repetitions ( int , default: 1 ) \u2013 Number times a solver in the portfolio must be run over the same instance. Defaults to 1. cxrate ( float , default: 0.5 ) \u2013 Crossover rate. Defaults to 0.5. mutrate ( float , default: 0.8 ) \u2013 Mutation rate. Defaults to 0.8. crossover ( Crossover , default: uniform_crossover ) \u2013 Crossover operator. Defaults to uniform_crossover. mutation ( Mutation , default: uniform_one_mutation ) \u2013 Mutation operator. Defaults to uniform_one_mutation. selection ( Selection , default: binary_tournament_selection ) \u2013 Selection operator. Defaults to binary_tournament_selection. replacement ( Replacement , default: generational_replacement ) \u2013 Replacement operator. Defaults to generational_replacement. performance_function ( PerformanceFn , default: max_gap_target ) \u2013 Performance function to calculate the performance score. Defaults to max_gap_target. phi ( float , default: 0.85 ) \u2013 Phi balance value for the weighted fitness function. Defaults to 0.85. seed ( int , default: 42 ) \u2013 Seed for the RNG protocol. Defaults to 42. Raises: ValueError \u2013 Raises error if phi is not a floating point value or it is not in the range [0.0-1.0] KeyError \u2013 Raises error if the descriptor strategy is not available in the DESCRIPTORS dictionary Source code in digneapy/generators.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def __init__ ( self , domain : Domain , portfolio : Iterable [ SupportsSolve [ P ]], novelty_approach : NS , pop_size : int = 100 , generations : int = 1000 , solution_set : Optional [ Archive ] = None , descriptor_strategy : str = \"features\" , transformer : Optional [ SupportsTransform ] = None , repetitions : int = 1 , cxrate : float = 0.5 , mutrate : float = 0.8 , crossover : Crossover = uniform_crossover , mutation : Mutation = uniform_one_mutation , selection : Selection = binary_tournament_selection , replacement : Replacement = generational_replacement , performance_function : PerformanceFn = max_gap_target , phi : float = 0.85 , seed : int = 42 , ): \"\"\"Creates a Evolutionary Instance Generator based on Novelty Search The generator uses a set of solvers to evaluate the instances and a novelty search algorithm to guide the evolution of the instances. Args: domain (Domain): Domain for which the instances are generated for. portfolio (Iterable[SupportSolve]): Iterable item of callable objects that can evaluate a instance. pop_size (int, optional): Number of instances in the population to evolve. Defaults to 100. generations (int, optional): Number of generations to perform. Defaults to 1000. solution_set (Optional[Archive], optional): Solution set to store the instances. Defaults to None. descriptor_strategy (str, optional): Descriptor used to calculate the diversity. The options available are defined in the dictionary digneapy.qd.descriptor_strategies. Defaults to \"features\". transformer (callable, optional): Define a strategy to transform the high-dimensional descriptors to low-dimensional.Defaults to None. repetitions (int, optional): Number times a solver in the portfolio must be run over the same instance. Defaults to 1. cxrate (float, optional): Crossover rate. Defaults to 0.5. mutrate (float, optional): Mutation rate. Defaults to 0.8. crossover (Crossover, optional): Crossover operator. Defaults to uniform_crossover. mutation (Mutation, optional): Mutation operator. Defaults to uniform_one_mutation. selection (Selection, optional): Selection operator. Defaults to binary_tournament_selection. replacement (Replacement, optional): Replacement operator. Defaults to generational_replacement. performance_function (PerformanceFn, optional): Performance function to calculate the performance score. Defaults to max_gap_target. phi (float, optional): Phi balance value for the weighted fitness function. Defaults to 0.85. seed (int, optional): Seed for the RNG protocol. Defaults to 42. Raises: ValueError: Raises error if phi is not a floating point value or it is not in the range [0.0-1.0] KeyError: Raises error if the descriptor strategy is not available in the DESCRIPTORS dictionary \"\"\" try : self . _desc_key = descriptor_strategy self . _descriptor_strategy = DESCRIPTORS [ self . _desc_key ] except KeyError : self . _desc_key = \"instance\" self . _descriptor_strategy = DESCRIPTORS [ self . _desc_key ] print ( f \"Descriptor: { descriptor_strategy } not available. Using the full instance as default descriptor\" ) try : phi = float ( phi ) except ValueError : raise ValueError ( \"Phi must be a float number in the range [0.0-1.0].\" ) if phi < 0.0 or phi > 1.0 : msg = f \"Phi must be a float number in the range [0.0-1.0]. Got: { phi } .\" raise ValueError ( msg ) self . phi = phi self . _novelty_search = novelty_approach self . _solution_set = None # By default there's not solution set if solution_set is not None : self . _ns_solution_set = NS ( archive = solution_set , k = 1 ) self . _transformer = transformer self . pop_size = pop_size self . offspring_size = pop_size self . generations = generations self . domain = domain self . portfolio = tuple ( portfolio ) if portfolio else () self . population = [] self . repetitions = repetitions self . cxrate = cxrate self . mutrate = mutrate self . crossover = crossover self . mutation = mutation self . selection = selection self . replacement = replacement self . performance_function = performance_function self . _logbook = Logbook () self . initialize_rng ( seed = seed ) __reproduce ( parent_1 , parent_2 ) Generates a new offspring instance from two parent instances Parameters: parent_1 ( Instance ) \u2013 First Parent parent_1 ( Instance ) \u2013 Second Parent Returns: Instance ( Instance ) \u2013 New offspring Source code in digneapy/generators.py 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 def __reproduce ( self , parent_1 : Instance , parent_2 : Instance ) -> Instance : \"\"\"Generates a new offspring instance from two parent instances Args: parent_1 (Instance): First Parent parent_1 (Instance): Second Parent Returns: Instance: New offspring \"\"\" offspring = parent_1 . clone () if self . _rng . random () < self . cxrate : offspring = self . crossover ( offspring , parent_2 ) return self . mutation ( offspring , self . domain . bounds ) else : return self . mutation ( offspring , self . domain . bounds ) GenResult dataclass Class to store the results of the generator Attributes: target (str): Name of the target solver used to evaluate the instances. instances (Sequence[Instance]): List of generated instances. history (Logbook): Logbook with the history of the generator. metrics (Optional[pd.Series], optional): Metrics of the instances. Defaults to None. Source code in digneapy/generators.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 @dataclass class GenResult : \"\"\"Class to store the results of the generator Attributes: target (str): Name of the target solver used to evaluate the instances. instances (Sequence[Instance]): List of generated instances. history (Logbook): Logbook with the history of the generator. metrics (Optional[pd.Series], optional): Metrics of the instances. Defaults to None. \"\"\" target : str instances : np . ndarray history : Logbook metrics : Optional [ pd . Series ] = None def __post_init__ ( self ): if len ( self . instances ) != 0 : self . metrics = Statistics ()( self . instances , as_series = True ) Generator Bases: Protocol Protocol to type check all generators of instances types in digneapy Source code in digneapy/generators.py 70 71 72 73 74 75 76 77 78 79 class Generator ( Protocol ): \"\"\"Protocol to type check all generators of instances types in digneapy\"\"\" def __call__ ( self , * args , ** kwargs ) -> GenResult : ... def _update_descriptors ( self , population : np . ndarray , portfolio_scores : Optional [ np . ndarray ] = None , ) -> Tuple [ np . ndarray , Optional [ np . ndarray ]]: ... MapElitesGenerator Bases: Generator , RNG Object to generate instances based on MAP-Elites algorithm. Source code in digneapy/generators.py 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 class MapElitesGenerator ( Generator , RNG ): \"\"\"Object to generate instances based on MAP-Elites algorithm.\"\"\" def __init__ ( self , domain : Domain , portfolio : Iterable [ SupportsSolve [ P ]], initial_pop_size : int , generations : int , archive : GridArchive | CVTArchive , mutation : Mutation , repetitions : int , descriptor : str , performance_function : PerformanceFn = max_gap_target , seed : int = 42 , ): \"\"\"Creates a MAP-Elites instance generator. The generator uses a set of solvers to evaluate the instances and MAP-Elites to guide the evolution of the instances. Args: domain (Domain): Domain for which the instances are generated for. portfolio (Iterable[SupportSolve]): Iterable item of callable objects that can evaluate a instance. initial_pop_size (int): Number of instances in the population to evolve. Defaults to 100. generations (int): Number of generations to perform. Defaults to 1000. archive (GridArchive | CVTArchive): Archive to store the instances. It can be a GridArchive or a CVTArchive. mutation (Mutation): Mutation operator repetitions (int): Number times a solver in the portfolio must be run over the same instance. Defaults to 1. descriptor (str): Descriptor used to calculate the diversity. The options available are defined in the dictionary digneapy.qd.descriptor_strategies. performance_function (PerformanceFn, optional): Performance function to calculate the performance score. Defaults to max_gap_target. seed (int, optional): Seed for the RNG protocol. Defaults to 42. Raises: ValueError: If the archive is not a GridArchive or CVTArchive \"\"\" if not isinstance ( archive , ( GridArchive , CVTArchive )): raise ValueError ( f \"MapElitesGenerator expects an archive of class GridArchive or CVTArchive and got { archive . __class__ . __name__ } \" ) self . _domain = domain self . _portfolio = list ( portfolio ) self . _init_pop_size = initial_pop_size self . _generations = generations self . _archive = archive self . _mutation = mutation self . _repetitions = repetitions self . _performance_fn = performance_function if descriptor not in DESCRIPTORS : msg = f \"descriptor { descriptor } not available in { self . __class__ . __name__ } .__init__. Set to features by default\" print ( msg ) descriptor = \"features\" self . _descriptor = descriptor match descriptor : case \"features\" : self . _descriptor_strategy = self . _domain . extract_features case _ : self . _descriptor_strategy = DESCRIPTORS [ descriptor ] self . _logbook = Logbook () self . initialize_rng ( seed = seed ) @property def archive ( self ): return self . _archive @property def log ( self ) -> Logbook : return self . _logbook def __str__ ( self ) -> str : port_names = [ s . __name__ for s in self . _portfolio ] domain_name = self . _domain . name if self . _domain is not None else \"None\" return f \"MapElites(descriptor= { self . _descriptor } ,pop_size= { self . _init_pop_size } ,gen= { self . _generations } ,domain= { domain_name } ,portfolio= { port_names !r} )\" def __repr__ ( self ) -> str : port_names = [ s . __name__ for s in self . _portfolio ] domain_name = self . _domain . name if self . _domain is not None else \"None\" return f \"MapElites<descriptor= { self . _descriptor } ,pop_size= { self . _init_pop_size } ,gen= { self . _generations } ,domain= { domain_name } ,portfolio= { port_names !r} >\" def _update_descriptors ( self , population : np . ndarray | Sequence [ Instance ], portfolio_scores : Optional [ np . ndarray ] = None , ) -> Tuple [ np . ndarray , Optional [ np . ndarray ]]: \"\"\"Updates the descriptors of the population of instances Args: population (Sequence[Instance]): Population of instances to update the descriptors. \"\"\" descriptors = np . empty ( len ( population )) features = None if self . _descriptor == \"features\" : descriptors = self . _domain . extract_features ( population ) features = descriptors . copy () elif self . _descriptor == \"performance\" : descriptors = np . mean ( portfolio_scores , axis = 2 ) else : descriptors = self . _descriptor_strategy ( population ) return ( descriptors , features ) def _evaluate_population ( self , population : Sequence [ Instance ] ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Evaluates the population of instances using the portfolio of solvers. Args: population (Sequence[Instance]): Sequence of instances to evaluate \"\"\" solvers_scores = np . zeros ( shape = ( len ( population ), len ( self . _portfolio ), self . _repetitions ) ) problems_to_solve = self . _domain . generate_problems_from_instances ( population ) for j , problem in enumerate ( problems_to_solve ): for i , solver in enumerate ( self . _portfolio ): # There is no need to change anything in the evaluation code when using Pisinger solvers # because the algs. only return one solution per run (len(solutions) == 1) # The same happens with the simple KP heuristics. However, when using Pisinger solvers # the lower the running time the better they're considered to work an instance scores = np . zeros ( self . _repetitions ) for rep in range ( self . _repetitions ): scores [ rep ] = max ( solver ( problem ), key = attrgetter ( \"fitness\" ) ) . fitness solvers_scores [ j , i , :] = scores mean_solvers_scores = np . mean ( solvers_scores , axis = 2 ) performance_biases = self . _performance_fn ( mean_solvers_scores ) return performance_biases , solvers_scores def __call__ ( self , verbose : bool = False ) -> GenResult : instances = self . _domain . generate_instances ( n = self . _init_pop_size ) perf_biases , portfolio_scores = self . _evaluate_population ( instances ) descriptors , features = self . _update_descriptors ( instances , portfolio_scores ) # Here we do not care for p >= 0. We are starting the archive # Must be removed later on self . _archive . extend ( instances = instances , descriptors = descriptors ) self . _logbook . update ( generation = 0 , population = instances , feedback = verbose ) for generation in range ( self . _generations ): indices = self . _rng . choice ( list ( self . _archive . filled_cells ), size = self . _init_pop_size ) parents = np . asarray ( self . _archive [ indices ], copy = True ) offspring = batch_uniform_one_mutation ( parents , self . _domain . _lbs , ub = self . _domain . _ubs ) perf_biases , portfolio_scores = self . _evaluate_population ( offspring ) # feasible_indices = np.where(perf_biases >= 0)[0] descriptors , features = self . _update_descriptors ( population = offspring , portfolio_scores = portfolio_scores ) offspring_population = [ Instance ( variables = offspring [ i ], fitness = perf_biases [ i ], descriptor = descriptors [ i ], portfolio_scores = portfolio_scores [ i ], p = perf_biases [ i ], features = features [ i ] if features is not None else None , ) for i in range ( self . _init_pop_size ) ] self . _archive . extend ( instances = offspring_population , descriptors = descriptors ) # Record the stats and update the performed gens self . _logbook . update ( generation = generation + 1 , population = self . _archive , feedback = verbose ) if verbose : # Clear the terminal blank = \" \" * 80 print ( f \" \\r { blank } \\r \" , end = \"\" ) self . _archive . purge_unfeasible () return GenResult ( target = self . _portfolio [ 0 ] . __name__ , instances = self . _archive , history = self . _logbook , ) __init__ ( domain , portfolio , initial_pop_size , generations , archive , mutation , repetitions , descriptor , performance_function = max_gap_target , seed = 42 ) Creates a MAP-Elites instance generator. The generator uses a set of solvers to evaluate the instances and MAP-Elites to guide the evolution of the instances. Parameters: domain ( Domain ) \u2013 Domain for which the instances are generated for. portfolio ( Iterable [ SupportSolve ] ) \u2013 Iterable item of callable objects that can evaluate a instance. initial_pop_size ( int ) \u2013 Number of instances in the population to evolve. Defaults to 100. generations ( int ) \u2013 Number of generations to perform. Defaults to 1000. archive ( GridArchive | CVTArchive ) \u2013 Archive to store the instances. It can be a GridArchive or a CVTArchive. mutation ( Mutation ) \u2013 Mutation operator repetitions ( int ) \u2013 Number times a solver in the portfolio must be run over the same instance. Defaults to 1. descriptor ( str ) \u2013 Descriptor used to calculate the diversity. The options available are defined in the dictionary digneapy.qd.descriptor_strategies. performance_function ( PerformanceFn , default: max_gap_target ) \u2013 Performance function to calculate the performance score. Defaults to max_gap_target. seed ( int , default: 42 ) \u2013 Seed for the RNG protocol. Defaults to 42. Raises: ValueError \u2013 If the archive is not a GridArchive or CVTArchive Source code in digneapy/generators.py 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 def __init__ ( self , domain : Domain , portfolio : Iterable [ SupportsSolve [ P ]], initial_pop_size : int , generations : int , archive : GridArchive | CVTArchive , mutation : Mutation , repetitions : int , descriptor : str , performance_function : PerformanceFn = max_gap_target , seed : int = 42 , ): \"\"\"Creates a MAP-Elites instance generator. The generator uses a set of solvers to evaluate the instances and MAP-Elites to guide the evolution of the instances. Args: domain (Domain): Domain for which the instances are generated for. portfolio (Iterable[SupportSolve]): Iterable item of callable objects that can evaluate a instance. initial_pop_size (int): Number of instances in the population to evolve. Defaults to 100. generations (int): Number of generations to perform. Defaults to 1000. archive (GridArchive | CVTArchive): Archive to store the instances. It can be a GridArchive or a CVTArchive. mutation (Mutation): Mutation operator repetitions (int): Number times a solver in the portfolio must be run over the same instance. Defaults to 1. descriptor (str): Descriptor used to calculate the diversity. The options available are defined in the dictionary digneapy.qd.descriptor_strategies. performance_function (PerformanceFn, optional): Performance function to calculate the performance score. Defaults to max_gap_target. seed (int, optional): Seed for the RNG protocol. Defaults to 42. Raises: ValueError: If the archive is not a GridArchive or CVTArchive \"\"\" if not isinstance ( archive , ( GridArchive , CVTArchive )): raise ValueError ( f \"MapElitesGenerator expects an archive of class GridArchive or CVTArchive and got { archive . __class__ . __name__ } \" ) self . _domain = domain self . _portfolio = list ( portfolio ) self . _init_pop_size = initial_pop_size self . _generations = generations self . _archive = archive self . _mutation = mutation self . _repetitions = repetitions self . _performance_fn = performance_function if descriptor not in DESCRIPTORS : msg = f \"descriptor { descriptor } not available in { self . __class__ . __name__ } .__init__. Set to features by default\" print ( msg ) descriptor = \"features\" self . _descriptor = descriptor match descriptor : case \"features\" : self . _descriptor_strategy = self . _domain . extract_features case _ : self . _descriptor_strategy = DESCRIPTORS [ descriptor ] self . _logbook = Logbook () self . initialize_rng ( seed = seed )","title":"Generators"},{"location":"reference/generators/#generators.DEAGenerator","text":"Bases: EAGenerator Object to generate instances based on a Evolutionary Algorithn with a Dominated Novelty Search approach Source code in digneapy/generators.py 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 class DEAGenerator ( EAGenerator ): \"\"\" Object to generate instances based on a Evolutionary Algorithn with a Dominated Novelty Search approach \"\"\" def __init__ ( self , domain : Domain , portfolio : Iterable [ SupportsSolve [ P ]], pop_size : int = 128 , offspring_size : int = 128 , generations : int = 1000 , k : int = 15 , descriptor_strategy : str = \"features\" , transformer : Optional [ SupportsTransform ] = None , repetitions : int = 1 , cxrate : float = 0.5 , mutrate : float = 0.8 , crossover : Crossover = uniform_crossover , mutation : Mutation = uniform_one_mutation , selection : Selection = binary_tournament_selection , performance_function : PerformanceFn = max_gap_target , ): \"\"\"Creates a Evolutionary Instance Generator based on Novelty Search Args: domain (Domain): Domain for which the instances are generated for. portfolio (Iterable[SupportSolve]): Iterable item of callable objects that can evaluate a instance. pop_size (int, optional): Number of instances in the population to evolve. Defaults to 128. offspring_size (int, optional): Number of instances in the offspring population. Defaults to 128. generations (int, optional): Number of total generations to perform. Defaults to 1000. k (int, optional): Number of neighbours to calculate the sparseness. Defaults to 15. descriptor_strategy (str, optional): Descriptor used to calculate the diversity. The options available are defined in the dictionary digneapy.qd.descriptor_strategies. Defaults to \"features\". transformer (callable, optional): Define a strategy to transform the high-dimensional descriptors to low-dimensional.Defaults to None. repetitions (int, optional): Number times a solver in the portfolio must be run over the same instance. Defaults to 1. cxrate (float, optional): Crossover rate. Defaults to 0.5. mutrate (float, optional): Mutation rate. Defaults to 0.8. crossover (Crossover, optional): Crossover operator. Defaults to uniform_crossover. mutation (Mutation, optional): Mutation operator. Defaults to uniform_one_mutation. selection (Selection, optional): Selection operator. Defaults to binary_tournament_selection. performance_function (PerformanceFn, optional): Performance function to calculate the performance score. Defaults to max_gap_target. \"\"\" super () . __init__ ( domain = domain , portfolio = portfolio , pop_size = pop_size , novelty_approach = None , generations = generations , descriptor_strategy = descriptor_strategy , transformer = transformer , repetitions = repetitions , cxrate = cxrate , mutrate = mutrate , crossover = crossover , mutation = mutation , selection = selection , performance_function = performance_function , ) self . k = k self . offspring_size = offspring_size def __str__ ( self ): port_names = [ s . __name__ for s in self . portfolio ] domain_name = self . domain . name if self . domain is not None else \"None\" return f \"DEAGenerator(pop_size= { self . pop_size } ,gen= { self . generations } ,domain= { domain_name } ,portfolio= { port_names !r} )\" def __repr__ ( self ) -> str : port_names = [ s . __name__ for s in self . portfolio ] domain_name = self . domain . name if self . domain is not None else \"None\" return f \"DEAGenerator<pop_size= { self . pop_size } ,gen= { self . generations } ,domain= { domain_name } ,portfolio= { port_names !r} >\" def __call__ ( self , verbose : bool = False ) -> GenResult : if self . domain is None : raise ValueError ( \"You must specify a domain to run the generator.\" ) if len ( self . portfolio ) == 0 : raise ValueError ( \"The portfolio is empty. To run the generator you must provide a valid portfolio of solvers\" ) self . population = self . domain . generate_instances ( n = self . pop_size ) perf_biases , portfolio_scores = self . _evaluate_population ( self . population ) descriptors , features = self . _update_descriptors ( self . population , portfolio_scores = portfolio_scores ) if features is not None : combined_features = np . empty ( shape = ( self . pop_size * 2 , features . shape [ 1 ]), dtype = np . float32 ) for pgen in range ( self . generations ): offspring = self . _generate_offspring ( self . pop_size ) off_perf_biases , off_portfolio_scores = self . _evaluate_population ( offspring ) off_descriptors , off_features = self . _update_descriptors ( offspring , portfolio_scores = off_portfolio_scores ) combined_descriptors = np . concatenate ( ( descriptors , off_descriptors ), axis = 0 ) combined_performances = np . concatenate ( ( perf_biases , off_perf_biases ), axis = 0 ) combined_port_scores = np . concatenate ( ( portfolio_scores , portfolio_scores ), axis = 0 ) genotypes = np . concatenate ( ( np . asarray ( self . population , copy = True ), offspring ), axis = 0 ) if features is not None : combined_features = np . concatenate (( features , off_features ), axis = 0 ) ( sorted_descriptors , sorted_performances , sorted_competition_fitness , sorted_indexing , ) = dominated_novelty_search ( descriptors = combined_descriptors , performances = combined_performances , k = self . k , force_feasible_only = True , ) # Keep the top N for the next generation sorted_indexing = sorted_indexing [: self . pop_size ] fitness = sorted_competition_fitness [: self . pop_size ] descriptors = sorted_descriptors [: self . pop_size ] perf_biases = sorted_performances [: self . pop_size ] # Track from the combined arrays based on the indexing portfolio_scores = combined_port_scores [ sorted_indexing ] genotypes = genotypes [ sorted_indexing ] if features is not None : features = combined_features [ sorted_indexing ] # Both population and offspring are used in the replacement # Record the stats and update the performed gens self . population = [ Instance ( variables = genotypes [ i ], fitness = fitness [ i ], descriptor = descriptors [ i ], portfolio_scores = portfolio_scores [ i ], p = perf_biases [ i ], features = features [ i ] if features is not None else (), ) for i in range ( self . pop_size ) ] self . _logbook . update ( generation = pgen , population = self . population , feedback = verbose ) if verbose : # Clear the terminal blank = \" \" * 80 print ( f \" \\r { blank } \\r \" , end = \"\" ) return GenResult ( target = self . portfolio [ 0 ] . __name__ , instances = self . population , history = self . _logbook , )","title":"DEAGenerator"},{"location":"reference/generators/#generators.DEAGenerator.__init__","text":"Creates a Evolutionary Instance Generator based on Novelty Search Parameters: domain ( Domain ) \u2013 Domain for which the instances are generated for. portfolio ( Iterable [ SupportSolve ] ) \u2013 Iterable item of callable objects that can evaluate a instance. pop_size ( int , default: 128 ) \u2013 Number of instances in the population to evolve. Defaults to 128. offspring_size ( int , default: 128 ) \u2013 Number of instances in the offspring population. Defaults to 128. generations ( int , default: 1000 ) \u2013 Number of total generations to perform. Defaults to 1000. k ( int , default: 15 ) \u2013 Number of neighbours to calculate the sparseness. Defaults to 15. descriptor_strategy ( str , default: 'features' ) \u2013 Descriptor used to calculate the diversity. The options available are defined in the dictionary digneapy.qd.descriptor_strategies. Defaults to \"features\". transformer ( callable , default: None ) \u2013 Define a strategy to transform the high-dimensional descriptors to low-dimensional.Defaults to None. repetitions ( int , default: 1 ) \u2013 Number times a solver in the portfolio must be run over the same instance. Defaults to 1. cxrate ( float , default: 0.5 ) \u2013 Crossover rate. Defaults to 0.5. mutrate ( float , default: 0.8 ) \u2013 Mutation rate. Defaults to 0.8. crossover ( Crossover , default: uniform_crossover ) \u2013 Crossover operator. Defaults to uniform_crossover. mutation ( Mutation , default: uniform_one_mutation ) \u2013 Mutation operator. Defaults to uniform_one_mutation. selection ( Selection , default: binary_tournament_selection ) \u2013 Selection operator. Defaults to binary_tournament_selection. performance_function ( PerformanceFn , default: max_gap_target ) \u2013 Performance function to calculate the performance score. Defaults to max_gap_target. Source code in digneapy/generators.py 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 def __init__ ( self , domain : Domain , portfolio : Iterable [ SupportsSolve [ P ]], pop_size : int = 128 , offspring_size : int = 128 , generations : int = 1000 , k : int = 15 , descriptor_strategy : str = \"features\" , transformer : Optional [ SupportsTransform ] = None , repetitions : int = 1 , cxrate : float = 0.5 , mutrate : float = 0.8 , crossover : Crossover = uniform_crossover , mutation : Mutation = uniform_one_mutation , selection : Selection = binary_tournament_selection , performance_function : PerformanceFn = max_gap_target , ): \"\"\"Creates a Evolutionary Instance Generator based on Novelty Search Args: domain (Domain): Domain for which the instances are generated for. portfolio (Iterable[SupportSolve]): Iterable item of callable objects that can evaluate a instance. pop_size (int, optional): Number of instances in the population to evolve. Defaults to 128. offspring_size (int, optional): Number of instances in the offspring population. Defaults to 128. generations (int, optional): Number of total generations to perform. Defaults to 1000. k (int, optional): Number of neighbours to calculate the sparseness. Defaults to 15. descriptor_strategy (str, optional): Descriptor used to calculate the diversity. The options available are defined in the dictionary digneapy.qd.descriptor_strategies. Defaults to \"features\". transformer (callable, optional): Define a strategy to transform the high-dimensional descriptors to low-dimensional.Defaults to None. repetitions (int, optional): Number times a solver in the portfolio must be run over the same instance. Defaults to 1. cxrate (float, optional): Crossover rate. Defaults to 0.5. mutrate (float, optional): Mutation rate. Defaults to 0.8. crossover (Crossover, optional): Crossover operator. Defaults to uniform_crossover. mutation (Mutation, optional): Mutation operator. Defaults to uniform_one_mutation. selection (Selection, optional): Selection operator. Defaults to binary_tournament_selection. performance_function (PerformanceFn, optional): Performance function to calculate the performance score. Defaults to max_gap_target. \"\"\" super () . __init__ ( domain = domain , portfolio = portfolio , pop_size = pop_size , novelty_approach = None , generations = generations , descriptor_strategy = descriptor_strategy , transformer = transformer , repetitions = repetitions , cxrate = cxrate , mutrate = mutrate , crossover = crossover , mutation = mutation , selection = selection , performance_function = performance_function , ) self . k = k self . offspring_size = offspring_size","title":"__init__"},{"location":"reference/generators/#generators.EAGenerator","text":"Bases: Generator , RNG Object to generate instances based on a Evolutionary Algorithn with set of diverse solutions Source code in digneapy/generators.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 class EAGenerator ( Generator , RNG ): \"\"\"Object to generate instances based on a Evolutionary Algorithn with set of diverse solutions\"\"\" def __init__ ( self , domain : Domain , portfolio : Iterable [ SupportsSolve [ P ]], novelty_approach : NS , pop_size : int = 100 , generations : int = 1000 , solution_set : Optional [ Archive ] = None , descriptor_strategy : str = \"features\" , transformer : Optional [ SupportsTransform ] = None , repetitions : int = 1 , cxrate : float = 0.5 , mutrate : float = 0.8 , crossover : Crossover = uniform_crossover , mutation : Mutation = uniform_one_mutation , selection : Selection = binary_tournament_selection , replacement : Replacement = generational_replacement , performance_function : PerformanceFn = max_gap_target , phi : float = 0.85 , seed : int = 42 , ): \"\"\"Creates a Evolutionary Instance Generator based on Novelty Search The generator uses a set of solvers to evaluate the instances and a novelty search algorithm to guide the evolution of the instances. Args: domain (Domain): Domain for which the instances are generated for. portfolio (Iterable[SupportSolve]): Iterable item of callable objects that can evaluate a instance. pop_size (int, optional): Number of instances in the population to evolve. Defaults to 100. generations (int, optional): Number of generations to perform. Defaults to 1000. solution_set (Optional[Archive], optional): Solution set to store the instances. Defaults to None. descriptor_strategy (str, optional): Descriptor used to calculate the diversity. The options available are defined in the dictionary digneapy.qd.descriptor_strategies. Defaults to \"features\". transformer (callable, optional): Define a strategy to transform the high-dimensional descriptors to low-dimensional.Defaults to None. repetitions (int, optional): Number times a solver in the portfolio must be run over the same instance. Defaults to 1. cxrate (float, optional): Crossover rate. Defaults to 0.5. mutrate (float, optional): Mutation rate. Defaults to 0.8. crossover (Crossover, optional): Crossover operator. Defaults to uniform_crossover. mutation (Mutation, optional): Mutation operator. Defaults to uniform_one_mutation. selection (Selection, optional): Selection operator. Defaults to binary_tournament_selection. replacement (Replacement, optional): Replacement operator. Defaults to generational_replacement. performance_function (PerformanceFn, optional): Performance function to calculate the performance score. Defaults to max_gap_target. phi (float, optional): Phi balance value for the weighted fitness function. Defaults to 0.85. seed (int, optional): Seed for the RNG protocol. Defaults to 42. Raises: ValueError: Raises error if phi is not a floating point value or it is not in the range [0.0-1.0] KeyError: Raises error if the descriptor strategy is not available in the DESCRIPTORS dictionary \"\"\" try : self . _desc_key = descriptor_strategy self . _descriptor_strategy = DESCRIPTORS [ self . _desc_key ] except KeyError : self . _desc_key = \"instance\" self . _descriptor_strategy = DESCRIPTORS [ self . _desc_key ] print ( f \"Descriptor: { descriptor_strategy } not available. Using the full instance as default descriptor\" ) try : phi = float ( phi ) except ValueError : raise ValueError ( \"Phi must be a float number in the range [0.0-1.0].\" ) if phi < 0.0 or phi > 1.0 : msg = f \"Phi must be a float number in the range [0.0-1.0]. Got: { phi } .\" raise ValueError ( msg ) self . phi = phi self . _novelty_search = novelty_approach self . _solution_set = None # By default there's not solution set if solution_set is not None : self . _ns_solution_set = NS ( archive = solution_set , k = 1 ) self . _transformer = transformer self . pop_size = pop_size self . offspring_size = pop_size self . generations = generations self . domain = domain self . portfolio = tuple ( portfolio ) if portfolio else () self . population = [] self . repetitions = repetitions self . cxrate = cxrate self . mutrate = mutrate self . crossover = crossover self . mutation = mutation self . selection = selection self . replacement = replacement self . performance_function = performance_function self . _logbook = Logbook () self . initialize_rng ( seed = seed ) @property def log ( self ) -> Logbook : return self . _logbook def __str__ ( self ): port_names = [ s . __name__ for s in self . portfolio ] domain_name = self . domain . name if self . domain is not None else \"None\" return f \"EAGenerator(pop_size= { self . pop_size } ,gen= { self . generations } ,domain= { domain_name } ,portfolio= { port_names !r} , { self . _novelty_search . __str__ () } )\" def __repr__ ( self ) -> str : port_names = [ s . __name__ for s in self . portfolio ] domain_name = self . domain . name if self . domain is not None else \"None\" return f \"EAGenerator<pop_size= { self . pop_size } ,gen= { self . generations } ,domain= { domain_name } ,portfolio= { port_names !r} , { self . _novelty_search . __repr__ () } >\" def __call__ ( self , verbose : bool = False ) -> GenResult : if self . domain is None : raise ValueError ( \"You must specify a domain to run the generator.\" ) if len ( self . portfolio ) == 0 : raise ValueError ( \"The portfolio is empty. To run the generator you must provide a valid portfolio of solvers\" ) self . population = self . domain . generate_instances ( n = self . pop_size ) perf_biases , portfolio_scores = self . _evaluate_population ( self . population ) descriptors , features = self . _update_descriptors ( self . population , portfolio_scores = portfolio_scores ) for pgen in range ( self . generations ): offspring = self . _generate_offspring ( self . pop_size ) perf_biases , portfolio_scores = self . _evaluate_population ( offspring ) descriptors , features = self . _update_descriptors ( offspring , portfolio_scores = portfolio_scores ) novelty_scores = self . _novelty_search ( instances_descriptors = descriptors ) offspring_fitness = self . __compute_fitness ( perf_biases , novelty_scores ) # Update to include this # 1. Novelty Scores --> novelty_scores # 2. Performance bias --> perf_biases # 3. Fitness --> oiffspring_fitness # 4. Descriptor --> descriptors offspring = [ Instance ( variables = offspring [ i ], fitness = offspring_fitness [ i ], descriptor = descriptors [ i ], portfolio_scores = portfolio_scores [ i ], p = perf_biases [ i ], s = novelty_scores [ i ], features = features [ i ] if features is not None else None , ) for i in range ( len ( offspring )) ] # Only the feasible instances are considered to be included # in the archive and the solution set. feasible_indeces = np . where ( perf_biases > 0 )[ 0 ] self . _novelty_search . archive . extend ( instances = [ offspring [ i ] for i in feasible_indeces ], descriptors = descriptors [ feasible_indeces ], novelty_scores = novelty_scores [ feasible_indeces ], ) if self . _ns_solution_set : # TODO: Here I should only compute the feasible individuals novelty_solution_set = self . _ns_solution_set ( instances_descriptors = descriptors ) self . _ns_solution_set . archive . extend ( instances = [ offspring [ i ] for i in feasible_indeces ], descriptors = descriptors [ feasible_indeces ], novelty_scores = novelty_solution_set [ feasible_indeces ], ) # However the whole offspring population is used in the replacement operator self . population = self . replacement ( self . population , offspring ) # Record the stats and update the performed gens self . _logbook . update ( generation = pgen , population = self . population , feedback = verbose ) if verbose : # Clear the terminal blank = \" \" * 80 print ( f \" \\r { blank } \\r \" , end = \"\" ) _instances = ( self . _ns_solution_set . archive if self . _ns_solution_set is not None else self . _novelty_search . archive ) return GenResult ( target = self . portfolio [ 0 ] . __name__ , instances = _instances , history = self . _logbook , ) def _generate_offspring ( self , offspring_size : int ) -> np . ndarray : \"\"\"Generates a offspring population of size |offspring_size| from the current population Args: offspring_size (int): offspring size. Defaults to pop_size. Returns: Sequence[Instance] Returns a sequence with the instances definitions, the offspring population. \"\"\" offspring = [ None ] * offspring_size # np.empty(offspring_size, dtype=Instance) for i in range ( offspring_size ): p_1 = self . selection ( self . population ) p_2 = self . selection ( self . population ) child = self . __reproduce ( p_1 , p_2 ) offspring [ i ] = child return np . array ( offspring ) def _update_descriptors ( self , population : np . ndarray , portfolio_scores : Optional [ np . ndarray ] = None , ) -> Tuple [ np . ndarray , Optional [ np . ndarray ]]: \"\"\"Updates the descriptors of the population of instances Args: population (Sequence[Instance]): Population of instances to update the descriptors. \"\"\" descriptors = np . empty ( len ( population )) features = None if self . _desc_key == \"features\" : descriptors = self . domain . extract_features ( population ) features = descriptors . copy () elif self . _desc_key == \"performance\" : descriptors = np . mean ( portfolio_scores , axis = 2 ) else : descriptors = self . _descriptor_strategy ( population ) if self . _transformer is not None : # Transform the descriptors if necessary descriptors = self . _transformer ( descriptors ) return ( descriptors , features ) def _evaluate_population ( self , population : Sequence [ Instance ] ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Evaluates the population of instances using the portfolio of solvers. Args: population (Sequence[Instance]): Sequence of instances to evaluate \"\"\" solvers_scores = np . zeros ( shape = ( len ( population ), len ( self . portfolio ), self . repetitions ) ) problems_to_solve = self . domain . generate_problems_from_instances ( population ) for j , problem in enumerate ( problems_to_solve ): for i , solver in enumerate ( self . portfolio ): # There is no need to change anything in the evaluation code when using Pisinger solvers # because the algs. only return one solution per run (len(solutions) == 1) # The same happens with the simple KP heuristics. However, when using Pisinger solvers # the lower the running time the better they're considered to work an instance scores = np . zeros ( self . repetitions ) for rep in range ( self . repetitions ): scores [ rep ] = max ( solver ( problem ), key = attrgetter ( \"fitness\" ) ) . fitness solvers_scores [ j , i , :] = scores mean_solvers_scores = np . mean ( solvers_scores , axis = 2 ) performance_biases = self . performance_function ( mean_solvers_scores ) return performance_biases , solvers_scores def __compute_fitness ( self , performance_biases : np . ndarray , novelty_scores : np . ndarray ) -> np . ndarray : \"\"\"Calculates the fitness of each instance in the population Args: performance_biases (np.ndarray): Performance biases or scores of each instance novelty_scores (np.ndarray): Novelty scores of each instance Returns: fitness of each instance (np.ndarray) \"\"\" phi_r = 1.0 - self . phi fitness = np . zeros ( len ( performance_biases )) fitness = ( fitness * self . phi ) + ( novelty_scores * phi_r ) return fitness def __reproduce ( self , parent_1 : Instance , parent_2 : Instance ) -> Instance : \"\"\"Generates a new offspring instance from two parent instances Args: parent_1 (Instance): First Parent parent_1 (Instance): Second Parent Returns: Instance: New offspring \"\"\" offspring = parent_1 . clone () if self . _rng . random () < self . cxrate : offspring = self . crossover ( offspring , parent_2 ) return self . mutation ( offspring , self . domain . bounds ) else : return self . mutation ( offspring , self . domain . bounds )","title":"EAGenerator"},{"location":"reference/generators/#generators.EAGenerator.__compute_fitness","text":"Calculates the fitness of each instance in the population Parameters: performance_biases ( ndarray ) \u2013 Performance biases or scores of each instance novelty_scores ( ndarray ) \u2013 Novelty scores of each instance Returns: ndarray \u2013 fitness of each instance (np.ndarray) Source code in digneapy/generators.py 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 def __compute_fitness ( self , performance_biases : np . ndarray , novelty_scores : np . ndarray ) -> np . ndarray : \"\"\"Calculates the fitness of each instance in the population Args: performance_biases (np.ndarray): Performance biases or scores of each instance novelty_scores (np.ndarray): Novelty scores of each instance Returns: fitness of each instance (np.ndarray) \"\"\" phi_r = 1.0 - self . phi fitness = np . zeros ( len ( performance_biases )) fitness = ( fitness * self . phi ) + ( novelty_scores * phi_r ) return fitness","title":"__compute_fitness"},{"location":"reference/generators/#generators.EAGenerator.__init__","text":"Creates a Evolutionary Instance Generator based on Novelty Search The generator uses a set of solvers to evaluate the instances and a novelty search algorithm to guide the evolution of the instances. Parameters: domain ( Domain ) \u2013 Domain for which the instances are generated for. portfolio ( Iterable [ SupportSolve ] ) \u2013 Iterable item of callable objects that can evaluate a instance. pop_size ( int , default: 100 ) \u2013 Number of instances in the population to evolve. Defaults to 100. generations ( int , default: 1000 ) \u2013 Number of generations to perform. Defaults to 1000. solution_set ( Optional [ Archive ] , default: None ) \u2013 Solution set to store the instances. Defaults to None. descriptor_strategy ( str , default: 'features' ) \u2013 Descriptor used to calculate the diversity. The options available are defined in the dictionary digneapy.qd.descriptor_strategies. Defaults to \"features\". transformer ( callable , default: None ) \u2013 Define a strategy to transform the high-dimensional descriptors to low-dimensional.Defaults to None. repetitions ( int , default: 1 ) \u2013 Number times a solver in the portfolio must be run over the same instance. Defaults to 1. cxrate ( float , default: 0.5 ) \u2013 Crossover rate. Defaults to 0.5. mutrate ( float , default: 0.8 ) \u2013 Mutation rate. Defaults to 0.8. crossover ( Crossover , default: uniform_crossover ) \u2013 Crossover operator. Defaults to uniform_crossover. mutation ( Mutation , default: uniform_one_mutation ) \u2013 Mutation operator. Defaults to uniform_one_mutation. selection ( Selection , default: binary_tournament_selection ) \u2013 Selection operator. Defaults to binary_tournament_selection. replacement ( Replacement , default: generational_replacement ) \u2013 Replacement operator. Defaults to generational_replacement. performance_function ( PerformanceFn , default: max_gap_target ) \u2013 Performance function to calculate the performance score. Defaults to max_gap_target. phi ( float , default: 0.85 ) \u2013 Phi balance value for the weighted fitness function. Defaults to 0.85. seed ( int , default: 42 ) \u2013 Seed for the RNG protocol. Defaults to 42. Raises: ValueError \u2013 Raises error if phi is not a floating point value or it is not in the range [0.0-1.0] KeyError \u2013 Raises error if the descriptor strategy is not available in the DESCRIPTORS dictionary Source code in digneapy/generators.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def __init__ ( self , domain : Domain , portfolio : Iterable [ SupportsSolve [ P ]], novelty_approach : NS , pop_size : int = 100 , generations : int = 1000 , solution_set : Optional [ Archive ] = None , descriptor_strategy : str = \"features\" , transformer : Optional [ SupportsTransform ] = None , repetitions : int = 1 , cxrate : float = 0.5 , mutrate : float = 0.8 , crossover : Crossover = uniform_crossover , mutation : Mutation = uniform_one_mutation , selection : Selection = binary_tournament_selection , replacement : Replacement = generational_replacement , performance_function : PerformanceFn = max_gap_target , phi : float = 0.85 , seed : int = 42 , ): \"\"\"Creates a Evolutionary Instance Generator based on Novelty Search The generator uses a set of solvers to evaluate the instances and a novelty search algorithm to guide the evolution of the instances. Args: domain (Domain): Domain for which the instances are generated for. portfolio (Iterable[SupportSolve]): Iterable item of callable objects that can evaluate a instance. pop_size (int, optional): Number of instances in the population to evolve. Defaults to 100. generations (int, optional): Number of generations to perform. Defaults to 1000. solution_set (Optional[Archive], optional): Solution set to store the instances. Defaults to None. descriptor_strategy (str, optional): Descriptor used to calculate the diversity. The options available are defined in the dictionary digneapy.qd.descriptor_strategies. Defaults to \"features\". transformer (callable, optional): Define a strategy to transform the high-dimensional descriptors to low-dimensional.Defaults to None. repetitions (int, optional): Number times a solver in the portfolio must be run over the same instance. Defaults to 1. cxrate (float, optional): Crossover rate. Defaults to 0.5. mutrate (float, optional): Mutation rate. Defaults to 0.8. crossover (Crossover, optional): Crossover operator. Defaults to uniform_crossover. mutation (Mutation, optional): Mutation operator. Defaults to uniform_one_mutation. selection (Selection, optional): Selection operator. Defaults to binary_tournament_selection. replacement (Replacement, optional): Replacement operator. Defaults to generational_replacement. performance_function (PerformanceFn, optional): Performance function to calculate the performance score. Defaults to max_gap_target. phi (float, optional): Phi balance value for the weighted fitness function. Defaults to 0.85. seed (int, optional): Seed for the RNG protocol. Defaults to 42. Raises: ValueError: Raises error if phi is not a floating point value or it is not in the range [0.0-1.0] KeyError: Raises error if the descriptor strategy is not available in the DESCRIPTORS dictionary \"\"\" try : self . _desc_key = descriptor_strategy self . _descriptor_strategy = DESCRIPTORS [ self . _desc_key ] except KeyError : self . _desc_key = \"instance\" self . _descriptor_strategy = DESCRIPTORS [ self . _desc_key ] print ( f \"Descriptor: { descriptor_strategy } not available. Using the full instance as default descriptor\" ) try : phi = float ( phi ) except ValueError : raise ValueError ( \"Phi must be a float number in the range [0.0-1.0].\" ) if phi < 0.0 or phi > 1.0 : msg = f \"Phi must be a float number in the range [0.0-1.0]. Got: { phi } .\" raise ValueError ( msg ) self . phi = phi self . _novelty_search = novelty_approach self . _solution_set = None # By default there's not solution set if solution_set is not None : self . _ns_solution_set = NS ( archive = solution_set , k = 1 ) self . _transformer = transformer self . pop_size = pop_size self . offspring_size = pop_size self . generations = generations self . domain = domain self . portfolio = tuple ( portfolio ) if portfolio else () self . population = [] self . repetitions = repetitions self . cxrate = cxrate self . mutrate = mutrate self . crossover = crossover self . mutation = mutation self . selection = selection self . replacement = replacement self . performance_function = performance_function self . _logbook = Logbook () self . initialize_rng ( seed = seed )","title":"__init__"},{"location":"reference/generators/#generators.EAGenerator.__reproduce","text":"Generates a new offspring instance from two parent instances Parameters: parent_1 ( Instance ) \u2013 First Parent parent_1 ( Instance ) \u2013 Second Parent Returns: Instance ( Instance ) \u2013 New offspring Source code in digneapy/generators.py 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 def __reproduce ( self , parent_1 : Instance , parent_2 : Instance ) -> Instance : \"\"\"Generates a new offspring instance from two parent instances Args: parent_1 (Instance): First Parent parent_1 (Instance): Second Parent Returns: Instance: New offspring \"\"\" offspring = parent_1 . clone () if self . _rng . random () < self . cxrate : offspring = self . crossover ( offspring , parent_2 ) return self . mutation ( offspring , self . domain . bounds ) else : return self . mutation ( offspring , self . domain . bounds )","title":"__reproduce"},{"location":"reference/generators/#generators.GenResult","text":"Class to store the results of the generator Attributes: target (str): Name of the target solver used to evaluate the instances. instances (Sequence[Instance]): List of generated instances. history (Logbook): Logbook with the history of the generator. metrics (Optional[pd.Series], optional): Metrics of the instances. Defaults to None. Source code in digneapy/generators.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 @dataclass class GenResult : \"\"\"Class to store the results of the generator Attributes: target (str): Name of the target solver used to evaluate the instances. instances (Sequence[Instance]): List of generated instances. history (Logbook): Logbook with the history of the generator. metrics (Optional[pd.Series], optional): Metrics of the instances. Defaults to None. \"\"\" target : str instances : np . ndarray history : Logbook metrics : Optional [ pd . Series ] = None def __post_init__ ( self ): if len ( self . instances ) != 0 : self . metrics = Statistics ()( self . instances , as_series = True )","title":"GenResult"},{"location":"reference/generators/#generators.Generator","text":"Bases: Protocol Protocol to type check all generators of instances types in digneapy Source code in digneapy/generators.py 70 71 72 73 74 75 76 77 78 79 class Generator ( Protocol ): \"\"\"Protocol to type check all generators of instances types in digneapy\"\"\" def __call__ ( self , * args , ** kwargs ) -> GenResult : ... def _update_descriptors ( self , population : np . ndarray , portfolio_scores : Optional [ np . ndarray ] = None , ) -> Tuple [ np . ndarray , Optional [ np . ndarray ]]: ...","title":"Generator"},{"location":"reference/generators/#generators.MapElitesGenerator","text":"Bases: Generator , RNG Object to generate instances based on MAP-Elites algorithm. Source code in digneapy/generators.py 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 class MapElitesGenerator ( Generator , RNG ): \"\"\"Object to generate instances based on MAP-Elites algorithm.\"\"\" def __init__ ( self , domain : Domain , portfolio : Iterable [ SupportsSolve [ P ]], initial_pop_size : int , generations : int , archive : GridArchive | CVTArchive , mutation : Mutation , repetitions : int , descriptor : str , performance_function : PerformanceFn = max_gap_target , seed : int = 42 , ): \"\"\"Creates a MAP-Elites instance generator. The generator uses a set of solvers to evaluate the instances and MAP-Elites to guide the evolution of the instances. Args: domain (Domain): Domain for which the instances are generated for. portfolio (Iterable[SupportSolve]): Iterable item of callable objects that can evaluate a instance. initial_pop_size (int): Number of instances in the population to evolve. Defaults to 100. generations (int): Number of generations to perform. Defaults to 1000. archive (GridArchive | CVTArchive): Archive to store the instances. It can be a GridArchive or a CVTArchive. mutation (Mutation): Mutation operator repetitions (int): Number times a solver in the portfolio must be run over the same instance. Defaults to 1. descriptor (str): Descriptor used to calculate the diversity. The options available are defined in the dictionary digneapy.qd.descriptor_strategies. performance_function (PerformanceFn, optional): Performance function to calculate the performance score. Defaults to max_gap_target. seed (int, optional): Seed for the RNG protocol. Defaults to 42. Raises: ValueError: If the archive is not a GridArchive or CVTArchive \"\"\" if not isinstance ( archive , ( GridArchive , CVTArchive )): raise ValueError ( f \"MapElitesGenerator expects an archive of class GridArchive or CVTArchive and got { archive . __class__ . __name__ } \" ) self . _domain = domain self . _portfolio = list ( portfolio ) self . _init_pop_size = initial_pop_size self . _generations = generations self . _archive = archive self . _mutation = mutation self . _repetitions = repetitions self . _performance_fn = performance_function if descriptor not in DESCRIPTORS : msg = f \"descriptor { descriptor } not available in { self . __class__ . __name__ } .__init__. Set to features by default\" print ( msg ) descriptor = \"features\" self . _descriptor = descriptor match descriptor : case \"features\" : self . _descriptor_strategy = self . _domain . extract_features case _ : self . _descriptor_strategy = DESCRIPTORS [ descriptor ] self . _logbook = Logbook () self . initialize_rng ( seed = seed ) @property def archive ( self ): return self . _archive @property def log ( self ) -> Logbook : return self . _logbook def __str__ ( self ) -> str : port_names = [ s . __name__ for s in self . _portfolio ] domain_name = self . _domain . name if self . _domain is not None else \"None\" return f \"MapElites(descriptor= { self . _descriptor } ,pop_size= { self . _init_pop_size } ,gen= { self . _generations } ,domain= { domain_name } ,portfolio= { port_names !r} )\" def __repr__ ( self ) -> str : port_names = [ s . __name__ for s in self . _portfolio ] domain_name = self . _domain . name if self . _domain is not None else \"None\" return f \"MapElites<descriptor= { self . _descriptor } ,pop_size= { self . _init_pop_size } ,gen= { self . _generations } ,domain= { domain_name } ,portfolio= { port_names !r} >\" def _update_descriptors ( self , population : np . ndarray | Sequence [ Instance ], portfolio_scores : Optional [ np . ndarray ] = None , ) -> Tuple [ np . ndarray , Optional [ np . ndarray ]]: \"\"\"Updates the descriptors of the population of instances Args: population (Sequence[Instance]): Population of instances to update the descriptors. \"\"\" descriptors = np . empty ( len ( population )) features = None if self . _descriptor == \"features\" : descriptors = self . _domain . extract_features ( population ) features = descriptors . copy () elif self . _descriptor == \"performance\" : descriptors = np . mean ( portfolio_scores , axis = 2 ) else : descriptors = self . _descriptor_strategy ( population ) return ( descriptors , features ) def _evaluate_population ( self , population : Sequence [ Instance ] ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Evaluates the population of instances using the portfolio of solvers. Args: population (Sequence[Instance]): Sequence of instances to evaluate \"\"\" solvers_scores = np . zeros ( shape = ( len ( population ), len ( self . _portfolio ), self . _repetitions ) ) problems_to_solve = self . _domain . generate_problems_from_instances ( population ) for j , problem in enumerate ( problems_to_solve ): for i , solver in enumerate ( self . _portfolio ): # There is no need to change anything in the evaluation code when using Pisinger solvers # because the algs. only return one solution per run (len(solutions) == 1) # The same happens with the simple KP heuristics. However, when using Pisinger solvers # the lower the running time the better they're considered to work an instance scores = np . zeros ( self . _repetitions ) for rep in range ( self . _repetitions ): scores [ rep ] = max ( solver ( problem ), key = attrgetter ( \"fitness\" ) ) . fitness solvers_scores [ j , i , :] = scores mean_solvers_scores = np . mean ( solvers_scores , axis = 2 ) performance_biases = self . _performance_fn ( mean_solvers_scores ) return performance_biases , solvers_scores def __call__ ( self , verbose : bool = False ) -> GenResult : instances = self . _domain . generate_instances ( n = self . _init_pop_size ) perf_biases , portfolio_scores = self . _evaluate_population ( instances ) descriptors , features = self . _update_descriptors ( instances , portfolio_scores ) # Here we do not care for p >= 0. We are starting the archive # Must be removed later on self . _archive . extend ( instances = instances , descriptors = descriptors ) self . _logbook . update ( generation = 0 , population = instances , feedback = verbose ) for generation in range ( self . _generations ): indices = self . _rng . choice ( list ( self . _archive . filled_cells ), size = self . _init_pop_size ) parents = np . asarray ( self . _archive [ indices ], copy = True ) offspring = batch_uniform_one_mutation ( parents , self . _domain . _lbs , ub = self . _domain . _ubs ) perf_biases , portfolio_scores = self . _evaluate_population ( offspring ) # feasible_indices = np.where(perf_biases >= 0)[0] descriptors , features = self . _update_descriptors ( population = offspring , portfolio_scores = portfolio_scores ) offspring_population = [ Instance ( variables = offspring [ i ], fitness = perf_biases [ i ], descriptor = descriptors [ i ], portfolio_scores = portfolio_scores [ i ], p = perf_biases [ i ], features = features [ i ] if features is not None else None , ) for i in range ( self . _init_pop_size ) ] self . _archive . extend ( instances = offspring_population , descriptors = descriptors ) # Record the stats and update the performed gens self . _logbook . update ( generation = generation + 1 , population = self . _archive , feedback = verbose ) if verbose : # Clear the terminal blank = \" \" * 80 print ( f \" \\r { blank } \\r \" , end = \"\" ) self . _archive . purge_unfeasible () return GenResult ( target = self . _portfolio [ 0 ] . __name__ , instances = self . _archive , history = self . _logbook , )","title":"MapElitesGenerator"},{"location":"reference/generators/#generators.MapElitesGenerator.__init__","text":"Creates a MAP-Elites instance generator. The generator uses a set of solvers to evaluate the instances and MAP-Elites to guide the evolution of the instances. Parameters: domain ( Domain ) \u2013 Domain for which the instances are generated for. portfolio ( Iterable [ SupportSolve ] ) \u2013 Iterable item of callable objects that can evaluate a instance. initial_pop_size ( int ) \u2013 Number of instances in the population to evolve. Defaults to 100. generations ( int ) \u2013 Number of generations to perform. Defaults to 1000. archive ( GridArchive | CVTArchive ) \u2013 Archive to store the instances. It can be a GridArchive or a CVTArchive. mutation ( Mutation ) \u2013 Mutation operator repetitions ( int ) \u2013 Number times a solver in the portfolio must be run over the same instance. Defaults to 1. descriptor ( str ) \u2013 Descriptor used to calculate the diversity. The options available are defined in the dictionary digneapy.qd.descriptor_strategies. performance_function ( PerformanceFn , default: max_gap_target ) \u2013 Performance function to calculate the performance score. Defaults to max_gap_target. seed ( int , default: 42 ) \u2013 Seed for the RNG protocol. Defaults to 42. Raises: ValueError \u2013 If the archive is not a GridArchive or CVTArchive Source code in digneapy/generators.py 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 def __init__ ( self , domain : Domain , portfolio : Iterable [ SupportsSolve [ P ]], initial_pop_size : int , generations : int , archive : GridArchive | CVTArchive , mutation : Mutation , repetitions : int , descriptor : str , performance_function : PerformanceFn = max_gap_target , seed : int = 42 , ): \"\"\"Creates a MAP-Elites instance generator. The generator uses a set of solvers to evaluate the instances and MAP-Elites to guide the evolution of the instances. Args: domain (Domain): Domain for which the instances are generated for. portfolio (Iterable[SupportSolve]): Iterable item of callable objects that can evaluate a instance. initial_pop_size (int): Number of instances in the population to evolve. Defaults to 100. generations (int): Number of generations to perform. Defaults to 1000. archive (GridArchive | CVTArchive): Archive to store the instances. It can be a GridArchive or a CVTArchive. mutation (Mutation): Mutation operator repetitions (int): Number times a solver in the portfolio must be run over the same instance. Defaults to 1. descriptor (str): Descriptor used to calculate the diversity. The options available are defined in the dictionary digneapy.qd.descriptor_strategies. performance_function (PerformanceFn, optional): Performance function to calculate the performance score. Defaults to max_gap_target. seed (int, optional): Seed for the RNG protocol. Defaults to 42. Raises: ValueError: If the archive is not a GridArchive or CVTArchive \"\"\" if not isinstance ( archive , ( GridArchive , CVTArchive )): raise ValueError ( f \"MapElitesGenerator expects an archive of class GridArchive or CVTArchive and got { archive . __class__ . __name__ } \" ) self . _domain = domain self . _portfolio = list ( portfolio ) self . _init_pop_size = initial_pop_size self . _generations = generations self . _archive = archive self . _mutation = mutation self . _repetitions = repetitions self . _performance_fn = performance_function if descriptor not in DESCRIPTORS : msg = f \"descriptor { descriptor } not available in { self . __class__ . __name__ } .__init__. Set to features by default\" print ( msg ) descriptor = \"features\" self . _descriptor = descriptor match descriptor : case \"features\" : self . _descriptor_strategy = self . _domain . extract_features case _ : self . _descriptor_strategy = DESCRIPTORS [ descriptor ] self . _logbook = Logbook () self . initialize_rng ( seed = seed )","title":"__init__"},{"location":"reference/_core/","text":"@File : init .py @Time : 2024/06/07 14:06:26 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2024, Alejandro Marrero @Desc : None Domain Bases: ABC , RNG Domain is a class that defines the domain of the problem. The domain is defined by its dimension and the bounds of each variable. Parameters: RNG \u2013 Subclass that implements the RNG protocol Source code in digneapy/_core/_domain.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 class Domain ( ABC , RNG ): \"\"\"Domain is a class that defines the domain of the problem. The domain is defined by its dimension and the bounds of each variable. Args: RNG: Subclass that implements the RNG protocol \"\"\" def __init__ ( self , dimension : int , bounds : Sequence [ tuple ], dtype = np . float64 , name : str = \"Domain\" , feat_names : Optional [ Sequence [ str ]] = None , seed : Optional [ int ] = None , * args , ** kwargs , ): self . name = name self . __name__ = name self . _dimension = dimension self . _bounds = bounds self . _dtype = dtype self . feat_names = feat_names if feat_names else list () self . initialize_rng ( seed = seed ) if len ( self . _bounds ) != 0 : ranges = list ( zip ( * bounds )) self . _lbs = np . array ( ranges [ 0 ], dtype = dtype ) self . _ubs = np . array ( ranges [ 1 ], dtype = dtype ) @abstractmethod def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances for the domain. Args: n (int, optional): Number of instances to generate. Defaults to 1. Returns: List[Instance]: A list of Instance objects created from the raw numpy generation \"\"\" raise NotImplementedError ( \"generate_n_instances is not implemented in Domain class.\" ) @abstractmethod def generate_problems_from_instances ( self , instances : Sequence [ Instance ] | np . ndarray ) -> List [ Problem ]: msg = \"generate_problems_from_instances is not implemented in Domain class.\" raise NotImplementedError ( msg ) @abstractmethod def extract_features ( self , instances : Sequence [ Instance ] | np . ndarray ) -> np . ndarray : \"\"\"Extract the features of the instances based on the domain Args: instance (Instance): Instance to extract the features from Returns: Tuple: Values of each feature \"\"\" msg = \"extract_features is not implemented in Domain class.\" raise NotImplementedError ( msg ) @abstractmethod def extract_features_as_dict ( self , instances : Sequence [ Instance ] | np . ndarray ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Args: instance (Instance): Instance to extract the features from Returns: Mapping[str, float]: Dictionary with the names/values of each feature \"\"\" msg = \"extract_features_as_dict is not implemented in Domain class.\" raise NotImplementedError ( msg ) @property def bounds ( self ): return self . _bounds def get_bounds_at ( self , i : int ) -> tuple : if i < 0 or i > len ( self . _bounds ): raise ValueError ( f \"Index { i } out-of-range. The bounds are 0- { len ( self . _bounds ) } \" ) return ( self . _lbs [ i ], self . _ubs [ i ]) @property def dimension ( self ): return self . _dimension def __len__ ( self ): return self . _dimension extract_features ( instances ) abstractmethod Extract the features of the instances based on the domain Parameters: instance ( Instance ) \u2013 Instance to extract the features from Returns: Tuple ( ndarray ) \u2013 Values of each feature Source code in digneapy/_core/_domain.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 @abstractmethod def extract_features ( self , instances : Sequence [ Instance ] | np . ndarray ) -> np . ndarray : \"\"\"Extract the features of the instances based on the domain Args: instance (Instance): Instance to extract the features from Returns: Tuple: Values of each feature \"\"\" msg = \"extract_features is not implemented in Domain class.\" raise NotImplementedError ( msg ) extract_features_as_dict ( instances ) abstractmethod Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Parameters: instance ( Instance ) \u2013 Instance to extract the features from Returns: List [ Dict [ str , float32 ]] \u2013 Mapping[str, float]: Dictionary with the names/values of each feature Source code in digneapy/_core/_domain.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 @abstractmethod def extract_features_as_dict ( self , instances : Sequence [ Instance ] | np . ndarray ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Args: instance (Instance): Instance to extract the features from Returns: Mapping[str, float]: Dictionary with the names/values of each feature \"\"\" msg = \"extract_features_as_dict is not implemented in Domain class.\" raise NotImplementedError ( msg ) generate_instances ( n = 1 ) abstractmethod Generates N instances for the domain. Parameters: n ( int , default: 1 ) \u2013 Number of instances to generate. Defaults to 1. Returns: List [ Instance ] \u2013 List[Instance]: A list of Instance objects created from the raw numpy generation Source code in digneapy/_core/_domain.py 57 58 59 60 61 62 63 64 65 66 67 68 69 @abstractmethod def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances for the domain. Args: n (int, optional): Number of instances to generate. Defaults to 1. Returns: List[Instance]: A list of Instance objects created from the raw numpy generation \"\"\" raise NotImplementedError ( \"generate_n_instances is not implemented in Domain class.\" ) Instance Source code in digneapy/_core/_instance.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 class Instance : __slots__ = ( \"_vars\" , \"_fit\" , \"_p\" , \"_s\" , \"_features\" , \"_desc\" , \"_pscores\" , \"_otype\" , \"_dtype\" , ) def __init__ ( self , variables : Optional [ npt . ArrayLike ] = None , fitness : np . float64 = np . float64 ( 0.0 ), p : np . float64 = np . float64 ( 0.0 ), s : np . float64 = np . float64 ( 0.0 ), features : Optional [ tuple [ np . float32 ]] = None , descriptor : Optional [ tuple [ np . float32 ]] = None , portfolio_scores : Optional [ tuple [ np . float64 ]] = None , otype = np . float64 , dtype = np . uint32 , ): \"\"\"Creates an instance of a Instance (unstructured) for QD algorithms This class is used to represent a solution in a QD algorithm. It contains the variables, fitness, performance, novelty, features, descriptor and portfolio scores of the solution. The variables are stored as a numpy array, and the fitness, performance and novelty are stored as floats. The features, descriptor and portfolio scores are stored as numpy arrays. Args: variables (Optional[npt.ArrayLike], optional): Variables or genome of the instance. Defaults to None. fitness (float, optional): Fitness of the instance. Defaults to 0.0. p (float, optional): Performance score. Defaults to 0.0. s (float, optional): Novelty score. Defaults to 0.0. features (Optional[tuple[float]], optional): Tuple of features extracted from the domain. Defaults to None. descriptor (Optional[tuple[float]], optional): Tuple with the descriptor information of the instance. Defaults to None. portfolio_scores (Optional[tuple[float]], optional): Scores of the solvers in the portfolio. Defaults to None. Raises: ValueError: If fitness, p or s are not convertible to float. \"\"\" self . _otype = otype self . _dtype = dtype try : fitness = self . _otype ( fitness ) p = self . _otype ( p ) s = self . _otype ( s ) except ValueError : raise ValueError ( \"The fitness, p and s parameters must be convertible to float\" ) self . _vars = ( np . array ( variables , dtype = self . _dtype ) if variables is not None else np . empty ( 0 , dtype = self . _dtype ) ) self . _fit = fitness self . _p = p self . _s = s self . _features = ( np . array ( features , dtype = np . float32 ) if features is not None else np . empty ( 0 , dtype = np . float32 ) ) self . _pscores = ( np . array ( portfolio_scores , dtype = self . _otype ) if portfolio_scores is not None else np . empty ( 0 , dtype = self . _otype ) ) self . _desc = ( np . array ( descriptor , dtype = np . float32 ) if descriptor is not None else np . empty ( 0 , dtype = np . float32 ) ) @property def dtype ( self ): return self . _dtype @property def otype ( self ): return self . _otype def clone ( self ) -> Self : \"\"\"Create a clone of the current instance. More efficient than using copy.deepcopy. Returns: Self: Instance object \"\"\" return Instance ( variables = list ( self . _vars ), fitness = self . _fit , p = self . _p , s = self . _s , features = tuple ( self . _features ), portfolio_scores = tuple ( self . _pscores ), descriptor = tuple ( self . _desc ), ) def clone_with ( self , ** overrides ): \"\"\"Clones an Instance with overriden attributes Returns: Instance \"\"\" new_object = self . clone () for key , value in overrides . items (): setattr ( new_object , key , value ) return new_object @property def variables ( self ): return self . _vars @variables . setter def variables ( self , new_variables : npt . ArrayLike ): if len ( new_variables ) != len ( self . _vars ): raise ValueError ( \"Updating the variables of an Instance object with a different number of values.\" f \"Instance have { len ( self . _vars ) } \" f \"variables and the new_variables sequence have { len ( new_variables ) } \" ) self . _vars = np . asarray ( new_variables ) @property def p ( self ) -> np . float64 : return self . _p @p . setter def p ( self , performance : np . float64 ): try : performance = np . float64 ( performance ) except ValueError : # if performance != 0.0 and not float(performance): msg = f \"The performance value { performance } is not a float in 'p' setter of class { self . __class__ . __name__ } \" raise ValueError ( msg ) self . _p = performance @property def s ( self ) -> np . float64 : return self . _s @s . setter def s ( self , novelty : np . float64 ): try : novelty = np . float64 ( novelty ) except ValueError : # if novelty != 0.0 and not float(novelty): msg = f \"The novelty value { novelty } is not a float in 's' setter of class { self . __class__ . __name__ } \" raise ValueError ( msg ) self . _s = novelty @property def fitness ( self ) -> np . float64 : return self . _fit @fitness . setter def fitness ( self , f : np . float64 ): try : f = np . float64 ( f ) except ValueError : # if f != 0.0 and not float(f): msg = f \"The fitness value { f } is not a float in fitness setter of class { self . __class__ . __name__ } \" raise ValueError ( msg ) self . _fit = f @property def features ( self ) -> np . ndarray : return self . _features @features . setter def features ( self , features : npt . ArrayLike ): self . _features = np . asarray ( features ) @property def descriptor ( self ) -> np . ndarray : return self . _desc @descriptor . setter def descriptor ( self , desc : npt . ArrayLike ): self . _desc = np . array ( desc ) @property def portfolio_scores ( self ): return self . _pscores @portfolio_scores . setter def portfolio_scores ( self , p : npt . ArrayLike ): self . _pscores = np . asarray ( p ) def __repr__ ( self ): return f \"Instance<f= { self . fitness } ,p= { self . p } ,s= { self . s } ,vars= { len ( self . _vars ) } ,features= { len ( self . features ) } ,descriptor= { len ( self . descriptor ) } ,performance= { len ( self . portfolio_scores ) } >\" def __str__ ( self ): import reprlib descriptor = reprlib . repr ( self . descriptor ) performance = reprlib . repr ( self . portfolio_scores ) performance = performance [ performance . find ( \"(\" ) : performance . rfind ( \")\" ) + 1 ] return f \"Instance(f= { self . fitness } ,p= { self . p } ,s= { self . s } ,features= { len ( self . features ) } ,descriptor= { descriptor } ,performance= { performance } )\" def __iter__ ( self ): return iter ( self . _vars ) def __len__ ( self ): return len ( self . _vars ) def __getitem__ ( self , key ): if isinstance ( key , slice ): cls = type ( self ) # To facilitate subclassing return cls ( self . _vars [ key ]) index = operator . index ( key ) return self . _vars [ index ] def __setitem__ ( self , key , value ): self . _vars [ key ] = value def __eq__ ( self , other ): if not isinstance ( other , Instance ): print ( f \"Other of type { other . __class__ . __name__ } can not be compared with with { self . __class__ . __name__ } \" ) return NotImplemented else : try : return all ( a == b for a , b in zip ( self , other , strict = True )) except ValueError : return False def __gt__ ( self , other ): if not isinstance ( other , Instance ): print ( f \"Other of type { other . __class__ . __name__ } can not be compared with with { self . __class__ . __name__ } \" ) return NotImplemented return self . fitness > other . fitness def __ge__ ( self , other ): if not isinstance ( other , Instance ): print ( f \"Other of type { other . __class__ . __name__ } can not be compared with with { self . __class__ . __name__ } \" ) return NotImplemented return self . fitness >= other . fitness def __hash__ ( self ): from functools import reduce hashes = ( hash ( x ) for x in self ) return reduce ( operator . or_ , hashes , 0 ) def __bool__ ( self ): return self . _vars . size != 0 def __format__ ( self , fmt_spec = \"\" ): if fmt_spec . endswith ( \"p\" ): # We are showing only the performances fmt_spec = fmt_spec [: - 1 ] components = self . portfolio_scores else : fmt_spec = fmt_spec [: - 1 ] components = self . descriptor components = ( format ( c , fmt_spec ) for c in components ) decriptor = \"descriptor=( {} )\" . format ( \",\" . join ( components )) msg = f \"Instance(f= { self . fitness } ,p= { format ( self . p , fmt_spec ) } , s= { format ( self . s , fmt_spec ) } , { decriptor } )\" return msg def asdict ( self , only_genotype : bool = False , variables_names : Optional [ Sequence [ str ]] = None , features_names : Optional [ Sequence [ str ]] = None , score_names : Optional [ Sequence [ str ]] = None , ) -> dict : \"\"\"Convert the instance to a dictionary. The keys are the names of the attributes and the values are the values of the attributes. Args: only_genotype (bool, Default True): Whether to return the Instance as a dictionary containing only the variables. variables_names (Optional[Sequence[str]], optional): Names of the variables in the dictionary, otherwise v_i. Defaults to None. features_names (Optional[Sequence[str]], optional): Name of the features in the dictionary, otherwise f_i. Defaults to None. score_names (Optional[Sequence[str]], optional): Name of the solvers, otherwise solver_i. Defaults to None. Returns: dict: Dictionary with the attributes of the instance as keys and the values of the attributes as values. \"\"\" _data = {} if variables_names : if len ( variables_names ) != len ( self . _vars ): print ( f \"Error in asdict(). len(variables_names) = { len ( variables_names ) } != len(variables) ( { len ( self . _vars ) } ). Fallback to v#\" ) _data [ \"variables\" ] = { f \"v { i } \" : v for i , v in enumerate ( self . _vars )} else : _data [ \"variables\" ] = { vk : v for vk , v in zip ( variables_names , self . _vars ) } else : _data [ \"variables\" ] = { f \"v { i } \" : v for i , v in enumerate ( self . _vars )} if only_genotype : return _data else : sckeys = ( [ f \"solver_ { i } \" for i in range ( len ( self . _pscores ))] if score_names is None else score_names ) _data = { \"fitness\" : self . _fit , \"s\" : self . _s , \"p\" : self . _p , \"portfolio_scores\" : { sk : v for sk , v in zip ( sckeys , self . _pscores )}, ** _data , } if len ( self . _desc ) not in ( len ( self . _vars ), len ( self . _features ), len ( self . _pscores ), ): # Transformed descriptor _data [ \"descriptor\" ] = { f \"d { i } \" : v for i , v in enumerate ( self . _desc )} if len ( self . features ) != 0 : f_keys = ( [ f \"f { i } \" for i in range ( len ( self . _features ))] if features_names is None or len ( features_names ) == 0 else features_names ) _data [ \"features\" ] = { fk : v for fk , v in zip ( f_keys , self . _features )} return _data def to_json ( self ) -> str : \"\"\"Convert the instance to a JSON string. The keys are the names of the attributes and the values are the values of the attributes. Returns: str: JSON string with the attributes of the instance as keys and the values of the attributes as values. \"\"\" import json return json . dumps ( self . asdict (), sort_keys = True , indent = 4 ) def to_series ( self , only_genotype : bool = False , variables_names : Optional [ Sequence [ str ]] = None , features_names : Optional [ Sequence [ str ]] = None , score_names : Optional [ Sequence [ str ]] = None , ) -> pd . Series : \"\"\"Creates a pandas Series from the instance. Args: only_genotype (bool, Default True): Whether to return the Instance as a pd.Series containing only the variables. variables_names (Optional[Sequence[str]], optional): Names of the variables in the dictionary, otherwise v_i. Defaults to None. features_names (Optional[Sequence[str]], optional): Name of the features in the dictionary, otherwise f_i. Defaults to None. score_names (Optional[Sequence[str]], optional): Name of the solvers, otherwise solver_i. Defaults to None. Returns: pd.Series: Pandas Series with the attributes of the instance as keys and the values of the attributes as values. \"\"\" _flatten_data = {} for key , value in self . asdict ( only_genotype = only_genotype , variables_names = variables_names , features_names = features_names , score_names = score_names , ) . items (): if isinstance ( value , dict ): # Flatten nested dicts for sub_key , sub_value in value . items (): _flatten_data [ f \" { sub_key } \" ] = sub_value else : _flatten_data [ key ] = value return pd . Series ( _flatten_data ) __init__ ( variables = None , fitness = np . float64 ( 0.0 ), p = np . float64 ( 0.0 ), s = np . float64 ( 0.0 ), features = None , descriptor = None , portfolio_scores = None , otype = np . float64 , dtype = np . uint32 ) Creates an instance of a Instance (unstructured) for QD algorithms This class is used to represent a solution in a QD algorithm. It contains the variables, fitness, performance, novelty, features, descriptor and portfolio scores of the solution. The variables are stored as a numpy array, and the fitness, performance and novelty are stored as floats. The features, descriptor and portfolio scores are stored as numpy arrays. Parameters: variables ( Optional [ ArrayLike ] , default: None ) \u2013 Variables or genome of the instance. Defaults to None. fitness ( float , default: float64 (0.0) ) \u2013 Fitness of the instance. Defaults to 0.0. p ( float , default: float64 (0.0) ) \u2013 Performance score. Defaults to 0.0. s ( float , default: float64 (0.0) ) \u2013 Novelty score. Defaults to 0.0. features ( Optional [ tuple [ float ]] , default: None ) \u2013 Tuple of features extracted from the domain. Defaults to None. descriptor ( Optional [ tuple [ float ]] , default: None ) \u2013 Tuple with the descriptor information of the instance. Defaults to None. portfolio_scores ( Optional [ tuple [ float ]] , default: None ) \u2013 Scores of the solvers in the portfolio. Defaults to None. Raises: ValueError \u2013 If fitness, p or s are not convertible to float. Source code in digneapy/_core/_instance.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def __init__ ( self , variables : Optional [ npt . ArrayLike ] = None , fitness : np . float64 = np . float64 ( 0.0 ), p : np . float64 = np . float64 ( 0.0 ), s : np . float64 = np . float64 ( 0.0 ), features : Optional [ tuple [ np . float32 ]] = None , descriptor : Optional [ tuple [ np . float32 ]] = None , portfolio_scores : Optional [ tuple [ np . float64 ]] = None , otype = np . float64 , dtype = np . uint32 , ): \"\"\"Creates an instance of a Instance (unstructured) for QD algorithms This class is used to represent a solution in a QD algorithm. It contains the variables, fitness, performance, novelty, features, descriptor and portfolio scores of the solution. The variables are stored as a numpy array, and the fitness, performance and novelty are stored as floats. The features, descriptor and portfolio scores are stored as numpy arrays. Args: variables (Optional[npt.ArrayLike], optional): Variables or genome of the instance. Defaults to None. fitness (float, optional): Fitness of the instance. Defaults to 0.0. p (float, optional): Performance score. Defaults to 0.0. s (float, optional): Novelty score. Defaults to 0.0. features (Optional[tuple[float]], optional): Tuple of features extracted from the domain. Defaults to None. descriptor (Optional[tuple[float]], optional): Tuple with the descriptor information of the instance. Defaults to None. portfolio_scores (Optional[tuple[float]], optional): Scores of the solvers in the portfolio. Defaults to None. Raises: ValueError: If fitness, p or s are not convertible to float. \"\"\" self . _otype = otype self . _dtype = dtype try : fitness = self . _otype ( fitness ) p = self . _otype ( p ) s = self . _otype ( s ) except ValueError : raise ValueError ( \"The fitness, p and s parameters must be convertible to float\" ) self . _vars = ( np . array ( variables , dtype = self . _dtype ) if variables is not None else np . empty ( 0 , dtype = self . _dtype ) ) self . _fit = fitness self . _p = p self . _s = s self . _features = ( np . array ( features , dtype = np . float32 ) if features is not None else np . empty ( 0 , dtype = np . float32 ) ) self . _pscores = ( np . array ( portfolio_scores , dtype = self . _otype ) if portfolio_scores is not None else np . empty ( 0 , dtype = self . _otype ) ) self . _desc = ( np . array ( descriptor , dtype = np . float32 ) if descriptor is not None else np . empty ( 0 , dtype = np . float32 ) ) asdict ( only_genotype = False , variables_names = None , features_names = None , score_names = None ) Convert the instance to a dictionary. The keys are the names of the attributes and the values are the values of the attributes. Parameters: only_genotype ( bool, Default True , default: False ) \u2013 Whether to return the Instance as a dictionary containing only the variables. variables_names ( Optional [ Sequence [ str ]] , default: None ) \u2013 Names of the variables in the dictionary, otherwise v_i. Defaults to None. features_names ( Optional [ Sequence [ str ]] , default: None ) \u2013 Name of the features in the dictionary, otherwise f_i. Defaults to None. score_names ( Optional [ Sequence [ str ]] , default: None ) \u2013 Name of the solvers, otherwise solver_i. Defaults to None. Returns: dict ( dict ) \u2013 Dictionary with the attributes of the instance as keys and the values of the attributes as values. Source code in digneapy/_core/_instance.py 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 def asdict ( self , only_genotype : bool = False , variables_names : Optional [ Sequence [ str ]] = None , features_names : Optional [ Sequence [ str ]] = None , score_names : Optional [ Sequence [ str ]] = None , ) -> dict : \"\"\"Convert the instance to a dictionary. The keys are the names of the attributes and the values are the values of the attributes. Args: only_genotype (bool, Default True): Whether to return the Instance as a dictionary containing only the variables. variables_names (Optional[Sequence[str]], optional): Names of the variables in the dictionary, otherwise v_i. Defaults to None. features_names (Optional[Sequence[str]], optional): Name of the features in the dictionary, otherwise f_i. Defaults to None. score_names (Optional[Sequence[str]], optional): Name of the solvers, otherwise solver_i. Defaults to None. Returns: dict: Dictionary with the attributes of the instance as keys and the values of the attributes as values. \"\"\" _data = {} if variables_names : if len ( variables_names ) != len ( self . _vars ): print ( f \"Error in asdict(). len(variables_names) = { len ( variables_names ) } != len(variables) ( { len ( self . _vars ) } ). Fallback to v#\" ) _data [ \"variables\" ] = { f \"v { i } \" : v for i , v in enumerate ( self . _vars )} else : _data [ \"variables\" ] = { vk : v for vk , v in zip ( variables_names , self . _vars ) } else : _data [ \"variables\" ] = { f \"v { i } \" : v for i , v in enumerate ( self . _vars )} if only_genotype : return _data else : sckeys = ( [ f \"solver_ { i } \" for i in range ( len ( self . _pscores ))] if score_names is None else score_names ) _data = { \"fitness\" : self . _fit , \"s\" : self . _s , \"p\" : self . _p , \"portfolio_scores\" : { sk : v for sk , v in zip ( sckeys , self . _pscores )}, ** _data , } if len ( self . _desc ) not in ( len ( self . _vars ), len ( self . _features ), len ( self . _pscores ), ): # Transformed descriptor _data [ \"descriptor\" ] = { f \"d { i } \" : v for i , v in enumerate ( self . _desc )} if len ( self . features ) != 0 : f_keys = ( [ f \"f { i } \" for i in range ( len ( self . _features ))] if features_names is None or len ( features_names ) == 0 else features_names ) _data [ \"features\" ] = { fk : v for fk , v in zip ( f_keys , self . _features )} return _data clone () Create a clone of the current instance. More efficient than using copy.deepcopy. Returns: Self ( Self ) \u2013 Instance object Source code in digneapy/_core/_instance.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def clone ( self ) -> Self : \"\"\"Create a clone of the current instance. More efficient than using copy.deepcopy. Returns: Self: Instance object \"\"\" return Instance ( variables = list ( self . _vars ), fitness = self . _fit , p = self . _p , s = self . _s , features = tuple ( self . _features ), portfolio_scores = tuple ( self . _pscores ), descriptor = tuple ( self . _desc ), ) clone_with ( ** overrides ) Clones an Instance with overriden attributes Returns: \u2013 Instance Source code in digneapy/_core/_instance.py 126 127 128 129 130 131 132 133 134 135 def clone_with ( self , ** overrides ): \"\"\"Clones an Instance with overriden attributes Returns: Instance \"\"\" new_object = self . clone () for key , value in overrides . items (): setattr ( new_object , key , value ) return new_object to_json () Convert the instance to a JSON string. The keys are the names of the attributes and the values are the values of the attributes. Returns: str ( str ) \u2013 JSON string with the attributes of the instance as keys and the values of the attributes as values. Source code in digneapy/_core/_instance.py 366 367 368 369 370 371 372 373 374 375 def to_json ( self ) -> str : \"\"\"Convert the instance to a JSON string. The keys are the names of the attributes and the values are the values of the attributes. Returns: str: JSON string with the attributes of the instance as keys and the values of the attributes as values. \"\"\" import json return json . dumps ( self . asdict (), sort_keys = True , indent = 4 ) to_series ( only_genotype = False , variables_names = None , features_names = None , score_names = None ) Creates a pandas Series from the instance. Parameters: only_genotype ( bool, Default True , default: False ) \u2013 Whether to return the Instance as a pd.Series containing only the variables. variables_names ( Optional [ Sequence [ str ]] , default: None ) \u2013 Names of the variables in the dictionary, otherwise v_i. Defaults to None. features_names ( Optional [ Sequence [ str ]] , default: None ) \u2013 Name of the features in the dictionary, otherwise f_i. Defaults to None. score_names ( Optional [ Sequence [ str ]] , default: None ) \u2013 Name of the solvers, otherwise solver_i. Defaults to None. Returns: Series \u2013 pd.Series: Pandas Series with the attributes of the instance as keys and the values of the attributes as values. Source code in digneapy/_core/_instance.py 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 def to_series ( self , only_genotype : bool = False , variables_names : Optional [ Sequence [ str ]] = None , features_names : Optional [ Sequence [ str ]] = None , score_names : Optional [ Sequence [ str ]] = None , ) -> pd . Series : \"\"\"Creates a pandas Series from the instance. Args: only_genotype (bool, Default True): Whether to return the Instance as a pd.Series containing only the variables. variables_names (Optional[Sequence[str]], optional): Names of the variables in the dictionary, otherwise v_i. Defaults to None. features_names (Optional[Sequence[str]], optional): Name of the features in the dictionary, otherwise f_i. Defaults to None. score_names (Optional[Sequence[str]], optional): Name of the solvers, otherwise solver_i. Defaults to None. Returns: pd.Series: Pandas Series with the attributes of the instance as keys and the values of the attributes as values. \"\"\" _flatten_data = {} for key , value in self . asdict ( only_genotype = only_genotype , variables_names = variables_names , features_names = features_names , score_names = score_names , ) . items (): if isinstance ( value , dict ): # Flatten nested dicts for sub_key , sub_value in value . items (): _flatten_data [ f \" { sub_key } \" ] = sub_value else : _flatten_data [ key ] = value return pd . Series ( _flatten_data ) NS Source code in digneapy/_core/_novelty_search.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 class NS : def __init__ ( self , archive : Optional [ Archive ] = None , k : int = 15 , ): \"\"\"Creates an instance of the Novelty Search Algorithm Args: archive (Archive): Archive to store the instances to guide the evolution. Defaults to Archive(threshold=0.001). k (int, optional): Number of neighbours to calculate the sparseness. Defaults to 15. \"\"\" if k < 0 : raise ValueError ( f \" { __name__ } k must be a positive integer and less than the number of instances.\" ) if archive is not None and not isinstance ( archive , Archive ): raise ValueError ( \"You must provide a valid Archive object\" ) self . _k = k self . _archive = archive if archive is not None else Archive ( threshold = 0.001 ) @property def archive ( self ): return self . _archive @property def k ( self ): return self . _k def __str__ ( self ): return f \"NS(k= { self . _k } ,A= { self . _archive } )\" def __repr__ ( self ) -> str : return f \"NS<k= { self . _k } ,A= { self . _archive } >\" def __call__ ( self , instances_descriptors : np . ndarray ) -> np . ndarray : \"\"\"Computes the Novelty Search of the instance descriptors with respect to the archive. It uses the Euclidean distance to compute the sparseness. Args: instance_descriptors (np.ndarray): Numpy array with the descriptors of the instances archive (Archive): Archive which stores the novelty instances found so far k (int, optional): Number of neighbors to consider in the computation of the sparseness. Defaults to 15. Raises: ValueError: If len(instance_descriptors) <= k Returns: np.ndarray: novelty scores (s) of the instances descriptors \"\"\" if len ( instances_descriptors ) == 0 : raise ValueError ( f \"NS was given an empty population to compute the sparseness. Shape is: { instances_descriptors . shape } \" ) num_instances = len ( instances_descriptors ) num_archive = len ( self . archive ) result = np . zeros ( num_instances , dtype = np . float64 ) if num_archive == 0 and num_instances <= self . _k : # Initially, the archive is empty and we may not have enough instances to evaluate print ( f \"NS has an empty archive at this moment and the given population is not large enough to compute the sparseness. { num_instances } < k ( { self . _k } ). Returning zeros.\" , file = sys . stderr , ) return result if num_instances + num_archive <= self . _k : msg = f \"Trying to calculate novelty search with k( { self . _k } ) >= { num_instances } (instances) + { num_archive } (archive).\" raise ValueError ( msg ) combined = ( instances_descriptors if num_archive == 0 else np . vstack ([ instances_descriptors , self . _archive . descriptors ]) ) for i in range ( num_instances ): mask = np . ones ( num_instances , bool ) mask [ i ] = False differences = combined [ i ] - combined [ np . nonzero ( mask )] distances = np . linalg . norm ( differences , axis = 1 ) _neighbors = np . partition ( distances , self . _k + 1 )[ 1 : self . _k + 1 ] result [ i ] = np . sum ( _neighbors ) / self . _k return result __call__ ( instances_descriptors ) Computes the Novelty Search of the instance descriptors with respect to the archive. It uses the Euclidean distance to compute the sparseness. Parameters: instance_descriptors ( ndarray ) \u2013 Numpy array with the descriptors of the instances archive ( Archive ) \u2013 Archive which stores the novelty instances found so far k ( int ) \u2013 Number of neighbors to consider in the computation of the sparseness. Defaults to 15. Raises: ValueError \u2013 If len(instance_descriptors) <= k Returns: ndarray \u2013 np.ndarray: novelty scores (s) of the instances descriptors Source code in digneapy/_core/_novelty_search.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def __call__ ( self , instances_descriptors : np . ndarray ) -> np . ndarray : \"\"\"Computes the Novelty Search of the instance descriptors with respect to the archive. It uses the Euclidean distance to compute the sparseness. Args: instance_descriptors (np.ndarray): Numpy array with the descriptors of the instances archive (Archive): Archive which stores the novelty instances found so far k (int, optional): Number of neighbors to consider in the computation of the sparseness. Defaults to 15. Raises: ValueError: If len(instance_descriptors) <= k Returns: np.ndarray: novelty scores (s) of the instances descriptors \"\"\" if len ( instances_descriptors ) == 0 : raise ValueError ( f \"NS was given an empty population to compute the sparseness. Shape is: { instances_descriptors . shape } \" ) num_instances = len ( instances_descriptors ) num_archive = len ( self . archive ) result = np . zeros ( num_instances , dtype = np . float64 ) if num_archive == 0 and num_instances <= self . _k : # Initially, the archive is empty and we may not have enough instances to evaluate print ( f \"NS has an empty archive at this moment and the given population is not large enough to compute the sparseness. { num_instances } < k ( { self . _k } ). Returning zeros.\" , file = sys . stderr , ) return result if num_instances + num_archive <= self . _k : msg = f \"Trying to calculate novelty search with k( { self . _k } ) >= { num_instances } (instances) + { num_archive } (archive).\" raise ValueError ( msg ) combined = ( instances_descriptors if num_archive == 0 else np . vstack ([ instances_descriptors , self . _archive . descriptors ]) ) for i in range ( num_instances ): mask = np . ones ( num_instances , bool ) mask [ i ] = False differences = combined [ i ] - combined [ np . nonzero ( mask )] distances = np . linalg . norm ( differences , axis = 1 ) _neighbors = np . partition ( distances , self . _k + 1 )[ 1 : self . _k + 1 ] result [ i ] = np . sum ( _neighbors ) / self . _k return result __init__ ( archive = None , k = 15 ) Creates an instance of the Novelty Search Algorithm Args: archive (Archive): Archive to store the instances to guide the evolution. Defaults to Archive(threshold=0.001). k (int, optional): Number of neighbours to calculate the sparseness. Defaults to 15. Source code in digneapy/_core/_novelty_search.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def __init__ ( self , archive : Optional [ Archive ] = None , k : int = 15 , ): \"\"\"Creates an instance of the Novelty Search Algorithm Args: archive (Archive): Archive to store the instances to guide the evolution. Defaults to Archive(threshold=0.001). k (int, optional): Number of neighbours to calculate the sparseness. Defaults to 15. \"\"\" if k < 0 : raise ValueError ( f \" { __name__ } k must be a positive integer and less than the number of instances.\" ) if archive is not None and not isinstance ( archive , Archive ): raise ValueError ( \"You must provide a valid Archive object\" ) self . _k = k self . _archive = archive if archive is not None else Archive ( threshold = 0.001 ) Problem Bases: ABC , RNG Source code in digneapy/_core/_problem.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 class Problem ( ABC , RNG ): def __init__ ( self , dimension : int , bounds : Sequence [ tuple ], name : str = \"DefaultProblem\" , dtype = np . float64 , seed : int = 42 , * args , ** kwargs , ): \"\"\"Creates a new problem instance. The problem is defined by its dimension and the bounds of each variable. Args: dimension (int): Number of variables in the problem bounds (Sequence[tuple]): Bounds of each variable in the problem name (str, optional): Name of the problem for printing and logging purposes. Defaults to \"DefaultProblem\". dtype (_type_, optional): Type of the variables. Defaults to np.float64. seed (int, optional): Seed for the RNG. Defaults to 42. \"\"\" self . _name = name self . __name__ = name self . _dimension = dimension self . _bounds = bounds self . _dtype = dtype self . initialize_rng ( seed = seed ) if len ( self . _bounds ) != 0 : ranges = list ( zip ( * bounds )) self . _lbs = np . array ( ranges [ 0 ], dtype = dtype ) self . _ubs = np . array ( ranges [ 1 ], dtype = dtype ) @property def dimension ( self ): return self . _dimension @property def bounds ( self ): return self . _bounds def get_bounds_at ( self , i : int ) -> tuple : if i < 0 or i > len ( self . _bounds ): raise ValueError ( f \"Index { i } out-of-range. The bounds are 0- { len ( self . _bounds ) } \" ) return ( self . _lbs [ i ], self . _ubs [ i ]) @abstractmethod def create_solution ( self ) -> Solution | np . ndarray : \"\"\"Creates a random solution to the problem. This method can be used to initialise the solutions for any algorithm \"\"\" msg = \"create_solution method not implemented in Problem\" raise NotImplementedError ( msg ) @abstractmethod def __array__ ( self , dtype : Any = None , copy : Optional [ bool ] = None ) -> npt . ArrayLike : msg = \"__array__ method not implemented in Problem\" raise NotImplementedError ( msg ) @abstractmethod def evaluate ( self , individual : Sequence | Solution | np . ndarray ) -> Tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Knapsack Args: individual (Sequence | Solution | np.ndarray): Individual to evaluate Raises: ValueError: Raises an error if the len(individual) != len(instance) / 2 Returns: Tuple[float]: fitness \"\"\" msg = \"evaluate method not implemented in Problem\" raise NotImplementedError ( msg ) @abstractmethod def __call__ ( self , individual : Sequence | Solution | np . ndarray ) -> Tuple [ float ]: msg = \"__call__ method not implemented in Problem\" raise NotImplementedError ( msg ) @abstractmethod def to_instance ( self ) -> Instance : \"\"\"Creates an instance from the information of the problem. This method is used in the generators to create instances to evolve \"\"\" msg = \"to_instance method not implemented in Problem\" raise NotImplementedError ( msg ) @abstractmethod def to_file ( self , filename : str ): msg = \"to_file method not implemented in Problem\" raise NotImplementedError ( msg ) @classmethod def from_file ( cls , filename : str ): msg = \"from_file method not implemented in Problem\" raise NotImplementedError ( msg ) __init__ ( dimension , bounds , name = 'DefaultProblem' , dtype = np . float64 , seed = 42 , * args , ** kwargs ) Creates a new problem instance. The problem is defined by its dimension and the bounds of each variable. Parameters: dimension ( int ) \u2013 Number of variables in the problem bounds ( Sequence [ tuple ] ) \u2013 Bounds of each variable in the problem name ( str , default: 'DefaultProblem' ) \u2013 Name of the problem for printing and logging purposes. Defaults to \"DefaultProblem\". dtype ( _type_ , default: float64 ) \u2013 Type of the variables. Defaults to np.float64. seed ( int , default: 42 ) \u2013 Seed for the RNG. Defaults to 42. Source code in digneapy/_core/_problem.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , dimension : int , bounds : Sequence [ tuple ], name : str = \"DefaultProblem\" , dtype = np . float64 , seed : int = 42 , * args , ** kwargs , ): \"\"\"Creates a new problem instance. The problem is defined by its dimension and the bounds of each variable. Args: dimension (int): Number of variables in the problem bounds (Sequence[tuple]): Bounds of each variable in the problem name (str, optional): Name of the problem for printing and logging purposes. Defaults to \"DefaultProblem\". dtype (_type_, optional): Type of the variables. Defaults to np.float64. seed (int, optional): Seed for the RNG. Defaults to 42. \"\"\" self . _name = name self . __name__ = name self . _dimension = dimension self . _bounds = bounds self . _dtype = dtype self . initialize_rng ( seed = seed ) if len ( self . _bounds ) != 0 : ranges = list ( zip ( * bounds )) self . _lbs = np . array ( ranges [ 0 ], dtype = dtype ) self . _ubs = np . array ( ranges [ 1 ], dtype = dtype ) create_solution () abstractmethod Creates a random solution to the problem. This method can be used to initialise the solutions for any algorithm Source code in digneapy/_core/_problem.py 72 73 74 75 76 77 78 79 @abstractmethod def create_solution ( self ) -> Solution | np . ndarray : \"\"\"Creates a random solution to the problem. This method can be used to initialise the solutions for any algorithm \"\"\" msg = \"create_solution method not implemented in Problem\" raise NotImplementedError ( msg ) evaluate ( individual ) abstractmethod Evaluates the candidate individual with the information of the Knapsack Parameters: individual ( Sequence | Solution | ndarray ) \u2013 Individual to evaluate Raises: ValueError \u2013 Raises an error if the len(individual) != len(instance) / 2 Returns: Tuple [ float ] \u2013 Tuple[float]: fitness Source code in digneapy/_core/_problem.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 @abstractmethod def evaluate ( self , individual : Sequence | Solution | np . ndarray ) -> Tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Knapsack Args: individual (Sequence | Solution | np.ndarray): Individual to evaluate Raises: ValueError: Raises an error if the len(individual) != len(instance) / 2 Returns: Tuple[float]: fitness \"\"\" msg = \"evaluate method not implemented in Problem\" raise NotImplementedError ( msg ) to_instance () abstractmethod Creates an instance from the information of the problem. This method is used in the generators to create instances to evolve Source code in digneapy/_core/_problem.py 109 110 111 112 113 114 115 @abstractmethod def to_instance ( self ) -> Instance : \"\"\"Creates an instance from the information of the problem. This method is used in the generators to create instances to evolve \"\"\" msg = \"to_instance method not implemented in Problem\" raise NotImplementedError ( msg ) RNG Bases: Protocol Protocol to type check all operators have _rng of instances types in digneapy Source code in digneapy/_core/types.py 18 19 20 21 22 23 24 25 26 class RNG ( Protocol ): \"\"\"Protocol to type check all operators have _rng of instances types in digneapy\"\"\" _rng : Generator _seed : int | None def initialize_rng ( self , seed : Optional [ int ] = None ): self . _seed = seed self . _rng = np . random . default_rng () Solution Class representing a solution in a genetic algorithm. It contains the variables, objectives, constraints, and fitness of the solution. Source code in digneapy/_core/_solution.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 class Solution : \"\"\" Class representing a solution in a genetic algorithm. It contains the variables, objectives, constraints, and fitness of the solution. \"\"\" def __init__ ( self , variables : Optional [ Iterable ] = [], objectives : Optional [ Iterable ] = [], constraints : Optional [ Iterable ] = [], fitness : np . float64 = np . float64 ( 0.0 ), dtype = np . uint32 , otype = np . float64 , ): \"\"\"Creates a new solution object. The variables is a numpy array of the solution's genes. The objectives and constraints are numpy arrays of the solution's objectives and constraints. The fitness is a float representing the solution's fitness value. Args: variables (Optional[Iterable], optional): Tuple or any other iterable with the variables/variables. Defaults to None. objectives (Optional[Iterable], optional): Tuple or any other iterable with the objectives values. Defaults to None. constraints (Optional[Iterable], optional): Tuple or any other iterable with the constraint values. Defaults to None. fitness (float, optional): Fitness of the solution. Defaults to 0.0. \"\"\" self . _otype = otype self . _dtype = dtype self . variables = np . asarray ( variables , dtype = self . dtype ) self . objectives = np . array ( objectives , dtype = self . otype ) self . constraints = np . array ( constraints , dtype = self . otype ) self . fitness = otype ( fitness ) @property def dtype ( self ): return self . _dtype @property def otype ( self ): return self . _otype def clone ( self ) -> Self : \"\"\"Returns a deep copy of the solution. It is more efficient than using the copy module. Returns: Self: Solution object \"\"\" return Solution ( variables = list ( self . variables ), objectives = list ( self . objectives ), constraints = list ( self . constraints ), fitness = self . fitness , otype = self . otype , ) def clone_with ( self , ** overrides ): \"\"\"Clones an Instance with overriden attributes Returns: Instance \"\"\" new_object = self . clone () for key , value in overrides . items (): setattr ( new_object , key , value ) return new_object def __str__ ( self ) -> str : return f \"Solution(dim= { len ( self . variables ) } ,f= { self . fitness } ,objs= { self . objectives } ,const= { self . constraints } )\" def __repr__ ( self ) -> str : return f \"Solution<dim= { len ( self . variables ) } ,f= { self . fitness } ,objs= { self . objectives } ,const= { self . constraints } >\" def __len__ ( self ) -> int : return len ( self . variables ) def __iter__ ( self ): return iter ( self . variables ) def __bool__ ( self ): return len ( self ) != 0 def __eq__ ( self , other ) -> bool : if isinstance ( other , Solution ): try : return all ( a == b for a , b in zip ( self , other , strict = True )) except ValueError : return False else : return NotImplemented def __gt__ ( self , other ): if not isinstance ( other , Solution ): msg = f \"Other of type { other . __class__ . __name__ } can not be compared with with { self . __class__ . __name__ } \" print ( msg ) return NotImplemented return self . fitness > other . fitness def __getitem__ ( self , key ): if isinstance ( key , slice ): cls = type ( self ) # To facilitate subclassing return cls ( self . variables [ key ]) index = operator . index ( key ) return self . variables [ index ] def __setitem__ ( self , key , value ): self . variables [ key ] = value __init__ ( variables = [], objectives = [], constraints = [], fitness = np . float64 ( 0.0 ), dtype = np . uint32 , otype = np . float64 ) Creates a new solution object. The variables is a numpy array of the solution's genes. The objectives and constraints are numpy arrays of the solution's objectives and constraints. The fitness is a float representing the solution's fitness value. Parameters: variables ( Optional [ Iterable ] , default: [] ) \u2013 Tuple or any other iterable with the variables/variables. Defaults to None. objectives ( Optional [ Iterable ] , default: [] ) \u2013 Tuple or any other iterable with the objectives values. Defaults to None. constraints ( Optional [ Iterable ] , default: [] ) \u2013 Tuple or any other iterable with the constraint values. Defaults to None. fitness ( float , default: float64 (0.0) ) \u2013 Fitness of the solution. Defaults to 0.0. Source code in digneapy/_core/_solution.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def __init__ ( self , variables : Optional [ Iterable ] = [], objectives : Optional [ Iterable ] = [], constraints : Optional [ Iterable ] = [], fitness : np . float64 = np . float64 ( 0.0 ), dtype = np . uint32 , otype = np . float64 , ): \"\"\"Creates a new solution object. The variables is a numpy array of the solution's genes. The objectives and constraints are numpy arrays of the solution's objectives and constraints. The fitness is a float representing the solution's fitness value. Args: variables (Optional[Iterable], optional): Tuple or any other iterable with the variables/variables. Defaults to None. objectives (Optional[Iterable], optional): Tuple or any other iterable with the objectives values. Defaults to None. constraints (Optional[Iterable], optional): Tuple or any other iterable with the constraint values. Defaults to None. fitness (float, optional): Fitness of the solution. Defaults to 0.0. \"\"\" self . _otype = otype self . _dtype = dtype self . variables = np . asarray ( variables , dtype = self . dtype ) self . objectives = np . array ( objectives , dtype = self . otype ) self . constraints = np . array ( constraints , dtype = self . otype ) self . fitness = otype ( fitness ) clone () Returns a deep copy of the solution. It is more efficient than using the copy module. Returns: Self ( Self ) \u2013 Solution object Source code in digneapy/_core/_solution.py 62 63 64 65 66 67 68 69 70 71 72 73 74 def clone ( self ) -> Self : \"\"\"Returns a deep copy of the solution. It is more efficient than using the copy module. Returns: Self: Solution object \"\"\" return Solution ( variables = list ( self . variables ), objectives = list ( self . objectives ), constraints = list ( self . constraints ), fitness = self . fitness , otype = self . otype , ) clone_with ( ** overrides ) Clones an Instance with overriden attributes Returns: \u2013 Instance Source code in digneapy/_core/_solution.py 76 77 78 79 80 81 82 83 84 85 def clone_with ( self , ** overrides ): \"\"\"Clones an Instance with overriden attributes Returns: Instance \"\"\" new_object = self . clone () for key , value in overrides . items (): setattr ( new_object , key , value ) return new_object Solver Bases: ABC , SupportsSolve [ P ] Solver is any callable type that receives a OptProblem as its argument and returns a tuple with the solution found Source code in digneapy/_core/_solver.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class Solver ( ABC , SupportsSolve [ P ]): \"\"\"Solver is any callable type that receives a OptProblem as its argument and returns a tuple with the solution found \"\"\" @abstractmethod def __call__ ( self , problem : P , * args , ** kwargs ) -> list [ Solution ]: \"\"\"Solves a optimisation problem Args: problem (OptProblem): Any optimisation problem or callablle that receives a Sequence and returns a Tuple[float] Raises: NotImplementedError: Must be implemented by subclasses Returns: List[Solution]: Returns a sequence of olutions \"\"\" msg = \"__call__ method not implemented in Solver\" raise NotImplementedError ( msg ) __call__ ( problem , * args , ** kwargs ) abstractmethod Solves a optimisation problem Parameters: problem ( OptProblem ) \u2013 Any optimisation problem or callablle that receives a Sequence and returns a Tuple[float] Raises: NotImplementedError \u2013 Must be implemented by subclasses Returns: list [ Solution ] \u2013 List[Solution]: Returns a sequence of olutions Source code in digneapy/_core/_solver.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 @abstractmethod def __call__ ( self , problem : P , * args , ** kwargs ) -> list [ Solution ]: \"\"\"Solves a optimisation problem Args: problem (OptProblem): Any optimisation problem or callablle that receives a Sequence and returns a Tuple[float] Raises: NotImplementedError: Must be implemented by subclasses Returns: List[Solution]: Returns a sequence of olutions \"\"\" msg = \"__call__ method not implemented in Solver\" raise NotImplementedError ( msg ) Statistics Source code in digneapy/_core/_metrics.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 class Statistics : def __init__ ( self ): self . _stats_s = tools . Statistics ( key = attrgetter ( \"s\" )) self . _stats_p = tools . Statistics ( key = attrgetter ( \"p\" )) self . _stats_f = tools . Statistics ( key = attrgetter ( \"fitness\" )) self . _stats = tools . MultiStatistics ( s = self . _stats_s , p = self . _stats_p , fitness = self . _stats_f ) self . _stats . register ( \"mean\" , np . mean ) self . _stats . register ( \"std\" , np . std ) self . _stats . register ( \"min\" , np . min ) self . _stats . register ( \"max\" , np . max ) self . _stats . register ( \"qd_score\" , np . sum ) def __call__ ( self , population : Sequence [ Instance ], as_series : bool = False ) -> dict | pd . Series : \"\"\"Calculates the statistics of the population. Args: population (Sequence[Instance]): List of instances to calculate the statistics. Returns: dict: Dictionary with the statistics of the population. \"\"\" if len ( population ) == 0 : raise ValueError ( \"Error: Trying to calculate the metrics with an empty population\" ) if not all ( isinstance ( ind , Instance ) for ind in population ): raise TypeError ( \"Error: Population must be a sequence of Instance objects\" ) record = self . _stats . compile ( population ) if as_series : _flatten_record = {} for key , value in record . items (): if isinstance ( value , dict ): # Flatten nested dicts for sub_key , sub_value in value . items (): _flatten_record [ f \" { key } _ { sub_key } \" ] = sub_value else : _flatten_record [ key ] = value return pd . Series ( _flatten_record ) else : return record __call__ ( population , as_series = False ) Calculates the statistics of the population. Args: population (Sequence[Instance]): List of instances to calculate the statistics. Returns: dict: Dictionary with the statistics of the population. Source code in digneapy/_core/_metrics.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def __call__ ( self , population : Sequence [ Instance ], as_series : bool = False ) -> dict | pd . Series : \"\"\"Calculates the statistics of the population. Args: population (Sequence[Instance]): List of instances to calculate the statistics. Returns: dict: Dictionary with the statistics of the population. \"\"\" if len ( population ) == 0 : raise ValueError ( \"Error: Trying to calculate the metrics with an empty population\" ) if not all ( isinstance ( ind , Instance ) for ind in population ): raise TypeError ( \"Error: Population must be a sequence of Instance objects\" ) record = self . _stats . compile ( population ) if as_series : _flatten_record = {} for key , value in record . items (): if isinstance ( value , dict ): # Flatten nested dicts for sub_key , sub_value in value . items (): _flatten_record [ f \" { key } _ { sub_key } \" ] = sub_value else : _flatten_record [ key ] = value return pd . Series ( _flatten_record ) else : return record SupportsSolve Bases: Protocol [ P ] Protocol to type check all the solver types in digneapy. A solver is any callable type that receives at least a problem (Problem) and returns a list of object of the Solution class. Source code in digneapy/_core/_solver.py 20 21 22 23 24 25 26 class SupportsSolve ( Protocol [ P ]): \"\"\"Protocol to type check all the solver types in digneapy. A solver is any callable type that receives at least a problem (Problem) and returns a list of object of the Solution class. \"\"\" def __call__ ( self , problem : P , * args , ** kwargs ) -> list [ Solution ]: ... dominated_novelty_search ( descriptors , performances , k = 15 , force_feasible_only = True ) Dominated Novelty Search (DNS) Bahlous-Boldi, R., Faldor, M., Grillotti, L., Janmohamed, H., Coiffard, L., Spector, L., & Cully, A. (2025). Dominated Novelty Search: Rethinking Local Competition in Quality-Diversity. 1. https://arxiv.org/abs/2502.00593v1 Quality-Diversity algorithm that implements local competition through dynamic fitness transformations, eliminating the need for predefined bounds or parameters. The competition fitness, also known as the dominated novelty score, is calculated as the average distance to the k nearest neighbors with higher fitness. The method returns a descending sorted list of instances by their competition fitness value. For each instance ``i'' in the sequence, we calculate all the other instances that dominate it. Then, we compute the distances between their descriptors using the norm of the difference for each dimension of the descriptors. Novel instances will get a competition fitness of np.inf (assuring they will survive). Less novel instances will be selected by their competition fitness value. This competition mechanism creates two complementary evolutionary pressures: individuals must either improve their fitness or discover distinct behaviors that differ from better-performing solutions. Solutions that have no fitter neighbors (D\ud835\udc56 = \u2205) receive an infinite competition fitness, ensuring their preservation in the population. Parameters: descriptors ( ndarray ) \u2013 Numpy array with the descriptors of the instances performances ( ndarray ) \u2013 Numpy array with the performance values of the instances k ( int , default: 15 ) \u2013 Number of nearest neighbours to calculate the competition fitness. Default to 15. force_feasible_only ( bool , default: True ) \u2013 Allow only instances with performance >= 0 to be considered. Default True. Raises: ValueError: If len(d) where d is the descriptor of each instance i differs from another ValueError: If k >= len(instances) Returns: Tuple [ ndarray , ndarray , ndarray , ndarray ] \u2013 Tuple[np.ndarray]: Tuple with the descriptors, performances and competition fitness values sorted, plus the sorted indexing (descending order). Source code in digneapy/_core/_novelty_search.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def dominated_novelty_search ( descriptors : np . ndarray , performances : np . ndarray , k : int = 15 , force_feasible_only : bool = True , ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray , np . ndarray ]: \"\"\" Dominated Novelty Search (DNS) Bahlous-Boldi, R., Faldor, M., Grillotti, L., Janmohamed, H., Coiffard, L., Spector, L., & Cully, A. (2025). Dominated Novelty Search: Rethinking Local Competition in Quality-Diversity. 1. https://arxiv.org/abs/2502.00593v1 Quality-Diversity algorithm that implements local competition through dynamic fitness transformations, eliminating the need for predefined bounds or parameters. The competition fitness, also known as the dominated novelty score, is calculated as the average distance to the k nearest neighbors with higher fitness. The method returns a descending sorted list of instances by their competition fitness value. For each instance ``i'' in the sequence, we calculate all the other instances that dominate it. Then, we compute the distances between their descriptors using the norm of the difference for each dimension of the descriptors. Novel instances will get a competition fitness of np.inf (assuring they will survive). Less novel instances will be selected by their competition fitness value. This competition mechanism creates two complementary evolutionary pressures: individuals must either improve their fitness or discover distinct behaviors that differ from better-performing solutions. Solutions that have no fitter neighbors (D\ud835\udc56 = \u2205) receive an infinite competition fitness, ensuring their preservation in the population. Args: descriptors (np.ndarray): Numpy array with the descriptors of the instances performances (np.ndarray): Numpy array with the performance values of the instances k (int): Number of nearest neighbours to calculate the competition fitness. Default to 15. force_feasible_only (bool): Allow only instances with performance >= 0 to be considered. Default True. Raises: ValueError: If len(d) where d is the descriptor of each instance i differs from another ValueError: If k >= len(instances) Returns: Tuple[np.ndarray]: Tuple with the descriptors, performances and competition fitness values sorted, plus the sorted indexing (descending order). \"\"\" num_instances = len ( descriptors ) if num_instances <= k : msg = f \"Trying to calculate the dominated novelty search with k( { k } ) > len(instances) = { num_instances } \" raise ValueError ( msg ) if len ( performances ) != len ( descriptors ): raise ValueError ( f \"Array mismatch between peformances and descriptors. len(performance) = { len ( performances ) } != { len ( descriptors ) } len(descriptors)\" ) # Try to force only feasible performances to get proper biased instances is_unfeasible = ( performances < 0.0 if force_feasible_only else ( performances == - np . inf ) ) fitter = performances [:, None ] <= performances [ None , :] fitter = np . where ( is_unfeasible [ None , :], False , fitter ) np . fill_diagonal ( fitter , False ) distance = np . linalg . norm ( descriptors [:, None , :] - descriptors [ None , :, :], axis =- 1 ) distance = np . where ( fitter , distance , np . inf ) neg_dist = - distance indices = np . argpartition ( neg_dist , - k , axis =- 1 )[ ... , - k :] values = np . take_along_axis ( neg_dist , indices , axis =- 1 ) indices = np . argsort ( values , axis =- 1 )[ ... , :: - 1 ] values = np . take_along_axis ( values , indices , axis =- 1 ) indices = np . take_along_axis ( indices , indices , axis =- 1 ) distance = np . mean ( - values , where = np . take_along_axis ( fitter , indices , axis = 1 ), axis =- 1 ) distance = np . where ( np . isnan ( distance ), np . inf , distance ) distance = np . where ( is_unfeasible , - np . inf , distance ) sorted_indices = np . argsort ( - distance ) return ( descriptors [ sorted_indices ], performances [ sorted_indices ], distance [ sorted_indices ], sorted_indices , ) qd_score ( instances_fitness ) Calculates the Quality Diversity score of a set of instances fitness. Parameters: instances ( Sequence [ float ] ) \u2013 List with the fitness of several instances to calculate the QD score. Returns: float ( float64 ) \u2013 Sum of the fitness of all instances. Source code in digneapy/_core/_metrics.py 23 24 25 26 27 28 29 30 31 32 def qd_score ( instances_fitness : np . ndarray ) -> np . float64 : \"\"\"Calculates the Quality Diversity score of a set of instances fitness. Args: instances (Sequence[float]): List with the fitness of several instances to calculate the QD score. Returns: float: Sum of the fitness of all instances. \"\"\" return np . sum ( instances_fitness ) qd_score_auc ( qd_scores , batch_size ) Calculates the Quantifying Efficiency in Quality Diversity Optimization In quality diversity (QD) optimization, the QD score is a holistic metric which sums the objective values of all cells in the archive. Since the QD score only measures the performance of a QD algorithm at a single point in time, it fails to reflect algorithm efficiency. Two algorithms may have the same QD score even though one algorithm achieved that score with fewer evaluations. We propose a metric called \u201cQD score AUC\u201d which quantifies this efficiency. Parameters: qd_scores ( Sequence [ float ] ) \u2013 Sequence of QD scores. batch_size ( int ) \u2013 Number of instances evaluated in each generation. Returns: float64 \u2013 np.float64: QD score AUC metric. Source code in digneapy/_core/_metrics.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def qd_score_auc ( qd_scores : np . ndarray , batch_size : int ) -> np . float64 : \"\"\"Calculates the Quantifying Efficiency in Quality Diversity Optimization In quality diversity (QD) optimization, the QD score is a holistic metric which sums the objective values of all cells in the archive. Since the QD score only measures the performance of a QD algorithm at a single point in time, it fails to reflect algorithm efficiency. Two algorithms may have the same QD score even though one algorithm achieved that score with fewer evaluations. We propose a metric called \u201cQD score AUC\u201d which quantifies this efficiency. Args: qd_scores (Sequence[float]): Sequence of QD scores. batch_size (int): Number of instances evaluated in each generation. Returns: np.float64: QD score AUC metric. \"\"\" return np . sum ( qd_scores ) * batch_size","title":"Index"},{"location":"reference/_core/#_core.Domain","text":"Bases: ABC , RNG Domain is a class that defines the domain of the problem. The domain is defined by its dimension and the bounds of each variable. Parameters: RNG \u2013 Subclass that implements the RNG protocol Source code in digneapy/_core/_domain.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 class Domain ( ABC , RNG ): \"\"\"Domain is a class that defines the domain of the problem. The domain is defined by its dimension and the bounds of each variable. Args: RNG: Subclass that implements the RNG protocol \"\"\" def __init__ ( self , dimension : int , bounds : Sequence [ tuple ], dtype = np . float64 , name : str = \"Domain\" , feat_names : Optional [ Sequence [ str ]] = None , seed : Optional [ int ] = None , * args , ** kwargs , ): self . name = name self . __name__ = name self . _dimension = dimension self . _bounds = bounds self . _dtype = dtype self . feat_names = feat_names if feat_names else list () self . initialize_rng ( seed = seed ) if len ( self . _bounds ) != 0 : ranges = list ( zip ( * bounds )) self . _lbs = np . array ( ranges [ 0 ], dtype = dtype ) self . _ubs = np . array ( ranges [ 1 ], dtype = dtype ) @abstractmethod def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances for the domain. Args: n (int, optional): Number of instances to generate. Defaults to 1. Returns: List[Instance]: A list of Instance objects created from the raw numpy generation \"\"\" raise NotImplementedError ( \"generate_n_instances is not implemented in Domain class.\" ) @abstractmethod def generate_problems_from_instances ( self , instances : Sequence [ Instance ] | np . ndarray ) -> List [ Problem ]: msg = \"generate_problems_from_instances is not implemented in Domain class.\" raise NotImplementedError ( msg ) @abstractmethod def extract_features ( self , instances : Sequence [ Instance ] | np . ndarray ) -> np . ndarray : \"\"\"Extract the features of the instances based on the domain Args: instance (Instance): Instance to extract the features from Returns: Tuple: Values of each feature \"\"\" msg = \"extract_features is not implemented in Domain class.\" raise NotImplementedError ( msg ) @abstractmethod def extract_features_as_dict ( self , instances : Sequence [ Instance ] | np . ndarray ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Args: instance (Instance): Instance to extract the features from Returns: Mapping[str, float]: Dictionary with the names/values of each feature \"\"\" msg = \"extract_features_as_dict is not implemented in Domain class.\" raise NotImplementedError ( msg ) @property def bounds ( self ): return self . _bounds def get_bounds_at ( self , i : int ) -> tuple : if i < 0 or i > len ( self . _bounds ): raise ValueError ( f \"Index { i } out-of-range. The bounds are 0- { len ( self . _bounds ) } \" ) return ( self . _lbs [ i ], self . _ubs [ i ]) @property def dimension ( self ): return self . _dimension def __len__ ( self ): return self . _dimension","title":"Domain"},{"location":"reference/_core/#_core.Domain.extract_features","text":"Extract the features of the instances based on the domain Parameters: instance ( Instance ) \u2013 Instance to extract the features from Returns: Tuple ( ndarray ) \u2013 Values of each feature Source code in digneapy/_core/_domain.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 @abstractmethod def extract_features ( self , instances : Sequence [ Instance ] | np . ndarray ) -> np . ndarray : \"\"\"Extract the features of the instances based on the domain Args: instance (Instance): Instance to extract the features from Returns: Tuple: Values of each feature \"\"\" msg = \"extract_features is not implemented in Domain class.\" raise NotImplementedError ( msg )","title":"extract_features"},{"location":"reference/_core/#_core.Domain.extract_features_as_dict","text":"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Parameters: instance ( Instance ) \u2013 Instance to extract the features from Returns: List [ Dict [ str , float32 ]] \u2013 Mapping[str, float]: Dictionary with the names/values of each feature Source code in digneapy/_core/_domain.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 @abstractmethod def extract_features_as_dict ( self , instances : Sequence [ Instance ] | np . ndarray ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Args: instance (Instance): Instance to extract the features from Returns: Mapping[str, float]: Dictionary with the names/values of each feature \"\"\" msg = \"extract_features_as_dict is not implemented in Domain class.\" raise NotImplementedError ( msg )","title":"extract_features_as_dict"},{"location":"reference/_core/#_core.Domain.generate_instances","text":"Generates N instances for the domain. Parameters: n ( int , default: 1 ) \u2013 Number of instances to generate. Defaults to 1. Returns: List [ Instance ] \u2013 List[Instance]: A list of Instance objects created from the raw numpy generation Source code in digneapy/_core/_domain.py 57 58 59 60 61 62 63 64 65 66 67 68 69 @abstractmethod def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances for the domain. Args: n (int, optional): Number of instances to generate. Defaults to 1. Returns: List[Instance]: A list of Instance objects created from the raw numpy generation \"\"\" raise NotImplementedError ( \"generate_n_instances is not implemented in Domain class.\" )","title":"generate_instances"},{"location":"reference/_core/#_core.Instance","text":"Source code in digneapy/_core/_instance.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 class Instance : __slots__ = ( \"_vars\" , \"_fit\" , \"_p\" , \"_s\" , \"_features\" , \"_desc\" , \"_pscores\" , \"_otype\" , \"_dtype\" , ) def __init__ ( self , variables : Optional [ npt . ArrayLike ] = None , fitness : np . float64 = np . float64 ( 0.0 ), p : np . float64 = np . float64 ( 0.0 ), s : np . float64 = np . float64 ( 0.0 ), features : Optional [ tuple [ np . float32 ]] = None , descriptor : Optional [ tuple [ np . float32 ]] = None , portfolio_scores : Optional [ tuple [ np . float64 ]] = None , otype = np . float64 , dtype = np . uint32 , ): \"\"\"Creates an instance of a Instance (unstructured) for QD algorithms This class is used to represent a solution in a QD algorithm. It contains the variables, fitness, performance, novelty, features, descriptor and portfolio scores of the solution. The variables are stored as a numpy array, and the fitness, performance and novelty are stored as floats. The features, descriptor and portfolio scores are stored as numpy arrays. Args: variables (Optional[npt.ArrayLike], optional): Variables or genome of the instance. Defaults to None. fitness (float, optional): Fitness of the instance. Defaults to 0.0. p (float, optional): Performance score. Defaults to 0.0. s (float, optional): Novelty score. Defaults to 0.0. features (Optional[tuple[float]], optional): Tuple of features extracted from the domain. Defaults to None. descriptor (Optional[tuple[float]], optional): Tuple with the descriptor information of the instance. Defaults to None. portfolio_scores (Optional[tuple[float]], optional): Scores of the solvers in the portfolio. Defaults to None. Raises: ValueError: If fitness, p or s are not convertible to float. \"\"\" self . _otype = otype self . _dtype = dtype try : fitness = self . _otype ( fitness ) p = self . _otype ( p ) s = self . _otype ( s ) except ValueError : raise ValueError ( \"The fitness, p and s parameters must be convertible to float\" ) self . _vars = ( np . array ( variables , dtype = self . _dtype ) if variables is not None else np . empty ( 0 , dtype = self . _dtype ) ) self . _fit = fitness self . _p = p self . _s = s self . _features = ( np . array ( features , dtype = np . float32 ) if features is not None else np . empty ( 0 , dtype = np . float32 ) ) self . _pscores = ( np . array ( portfolio_scores , dtype = self . _otype ) if portfolio_scores is not None else np . empty ( 0 , dtype = self . _otype ) ) self . _desc = ( np . array ( descriptor , dtype = np . float32 ) if descriptor is not None else np . empty ( 0 , dtype = np . float32 ) ) @property def dtype ( self ): return self . _dtype @property def otype ( self ): return self . _otype def clone ( self ) -> Self : \"\"\"Create a clone of the current instance. More efficient than using copy.deepcopy. Returns: Self: Instance object \"\"\" return Instance ( variables = list ( self . _vars ), fitness = self . _fit , p = self . _p , s = self . _s , features = tuple ( self . _features ), portfolio_scores = tuple ( self . _pscores ), descriptor = tuple ( self . _desc ), ) def clone_with ( self , ** overrides ): \"\"\"Clones an Instance with overriden attributes Returns: Instance \"\"\" new_object = self . clone () for key , value in overrides . items (): setattr ( new_object , key , value ) return new_object @property def variables ( self ): return self . _vars @variables . setter def variables ( self , new_variables : npt . ArrayLike ): if len ( new_variables ) != len ( self . _vars ): raise ValueError ( \"Updating the variables of an Instance object with a different number of values.\" f \"Instance have { len ( self . _vars ) } \" f \"variables and the new_variables sequence have { len ( new_variables ) } \" ) self . _vars = np . asarray ( new_variables ) @property def p ( self ) -> np . float64 : return self . _p @p . setter def p ( self , performance : np . float64 ): try : performance = np . float64 ( performance ) except ValueError : # if performance != 0.0 and not float(performance): msg = f \"The performance value { performance } is not a float in 'p' setter of class { self . __class__ . __name__ } \" raise ValueError ( msg ) self . _p = performance @property def s ( self ) -> np . float64 : return self . _s @s . setter def s ( self , novelty : np . float64 ): try : novelty = np . float64 ( novelty ) except ValueError : # if novelty != 0.0 and not float(novelty): msg = f \"The novelty value { novelty } is not a float in 's' setter of class { self . __class__ . __name__ } \" raise ValueError ( msg ) self . _s = novelty @property def fitness ( self ) -> np . float64 : return self . _fit @fitness . setter def fitness ( self , f : np . float64 ): try : f = np . float64 ( f ) except ValueError : # if f != 0.0 and not float(f): msg = f \"The fitness value { f } is not a float in fitness setter of class { self . __class__ . __name__ } \" raise ValueError ( msg ) self . _fit = f @property def features ( self ) -> np . ndarray : return self . _features @features . setter def features ( self , features : npt . ArrayLike ): self . _features = np . asarray ( features ) @property def descriptor ( self ) -> np . ndarray : return self . _desc @descriptor . setter def descriptor ( self , desc : npt . ArrayLike ): self . _desc = np . array ( desc ) @property def portfolio_scores ( self ): return self . _pscores @portfolio_scores . setter def portfolio_scores ( self , p : npt . ArrayLike ): self . _pscores = np . asarray ( p ) def __repr__ ( self ): return f \"Instance<f= { self . fitness } ,p= { self . p } ,s= { self . s } ,vars= { len ( self . _vars ) } ,features= { len ( self . features ) } ,descriptor= { len ( self . descriptor ) } ,performance= { len ( self . portfolio_scores ) } >\" def __str__ ( self ): import reprlib descriptor = reprlib . repr ( self . descriptor ) performance = reprlib . repr ( self . portfolio_scores ) performance = performance [ performance . find ( \"(\" ) : performance . rfind ( \")\" ) + 1 ] return f \"Instance(f= { self . fitness } ,p= { self . p } ,s= { self . s } ,features= { len ( self . features ) } ,descriptor= { descriptor } ,performance= { performance } )\" def __iter__ ( self ): return iter ( self . _vars ) def __len__ ( self ): return len ( self . _vars ) def __getitem__ ( self , key ): if isinstance ( key , slice ): cls = type ( self ) # To facilitate subclassing return cls ( self . _vars [ key ]) index = operator . index ( key ) return self . _vars [ index ] def __setitem__ ( self , key , value ): self . _vars [ key ] = value def __eq__ ( self , other ): if not isinstance ( other , Instance ): print ( f \"Other of type { other . __class__ . __name__ } can not be compared with with { self . __class__ . __name__ } \" ) return NotImplemented else : try : return all ( a == b for a , b in zip ( self , other , strict = True )) except ValueError : return False def __gt__ ( self , other ): if not isinstance ( other , Instance ): print ( f \"Other of type { other . __class__ . __name__ } can not be compared with with { self . __class__ . __name__ } \" ) return NotImplemented return self . fitness > other . fitness def __ge__ ( self , other ): if not isinstance ( other , Instance ): print ( f \"Other of type { other . __class__ . __name__ } can not be compared with with { self . __class__ . __name__ } \" ) return NotImplemented return self . fitness >= other . fitness def __hash__ ( self ): from functools import reduce hashes = ( hash ( x ) for x in self ) return reduce ( operator . or_ , hashes , 0 ) def __bool__ ( self ): return self . _vars . size != 0 def __format__ ( self , fmt_spec = \"\" ): if fmt_spec . endswith ( \"p\" ): # We are showing only the performances fmt_spec = fmt_spec [: - 1 ] components = self . portfolio_scores else : fmt_spec = fmt_spec [: - 1 ] components = self . descriptor components = ( format ( c , fmt_spec ) for c in components ) decriptor = \"descriptor=( {} )\" . format ( \",\" . join ( components )) msg = f \"Instance(f= { self . fitness } ,p= { format ( self . p , fmt_spec ) } , s= { format ( self . s , fmt_spec ) } , { decriptor } )\" return msg def asdict ( self , only_genotype : bool = False , variables_names : Optional [ Sequence [ str ]] = None , features_names : Optional [ Sequence [ str ]] = None , score_names : Optional [ Sequence [ str ]] = None , ) -> dict : \"\"\"Convert the instance to a dictionary. The keys are the names of the attributes and the values are the values of the attributes. Args: only_genotype (bool, Default True): Whether to return the Instance as a dictionary containing only the variables. variables_names (Optional[Sequence[str]], optional): Names of the variables in the dictionary, otherwise v_i. Defaults to None. features_names (Optional[Sequence[str]], optional): Name of the features in the dictionary, otherwise f_i. Defaults to None. score_names (Optional[Sequence[str]], optional): Name of the solvers, otherwise solver_i. Defaults to None. Returns: dict: Dictionary with the attributes of the instance as keys and the values of the attributes as values. \"\"\" _data = {} if variables_names : if len ( variables_names ) != len ( self . _vars ): print ( f \"Error in asdict(). len(variables_names) = { len ( variables_names ) } != len(variables) ( { len ( self . _vars ) } ). Fallback to v#\" ) _data [ \"variables\" ] = { f \"v { i } \" : v for i , v in enumerate ( self . _vars )} else : _data [ \"variables\" ] = { vk : v for vk , v in zip ( variables_names , self . _vars ) } else : _data [ \"variables\" ] = { f \"v { i } \" : v for i , v in enumerate ( self . _vars )} if only_genotype : return _data else : sckeys = ( [ f \"solver_ { i } \" for i in range ( len ( self . _pscores ))] if score_names is None else score_names ) _data = { \"fitness\" : self . _fit , \"s\" : self . _s , \"p\" : self . _p , \"portfolio_scores\" : { sk : v for sk , v in zip ( sckeys , self . _pscores )}, ** _data , } if len ( self . _desc ) not in ( len ( self . _vars ), len ( self . _features ), len ( self . _pscores ), ): # Transformed descriptor _data [ \"descriptor\" ] = { f \"d { i } \" : v for i , v in enumerate ( self . _desc )} if len ( self . features ) != 0 : f_keys = ( [ f \"f { i } \" for i in range ( len ( self . _features ))] if features_names is None or len ( features_names ) == 0 else features_names ) _data [ \"features\" ] = { fk : v for fk , v in zip ( f_keys , self . _features )} return _data def to_json ( self ) -> str : \"\"\"Convert the instance to a JSON string. The keys are the names of the attributes and the values are the values of the attributes. Returns: str: JSON string with the attributes of the instance as keys and the values of the attributes as values. \"\"\" import json return json . dumps ( self . asdict (), sort_keys = True , indent = 4 ) def to_series ( self , only_genotype : bool = False , variables_names : Optional [ Sequence [ str ]] = None , features_names : Optional [ Sequence [ str ]] = None , score_names : Optional [ Sequence [ str ]] = None , ) -> pd . Series : \"\"\"Creates a pandas Series from the instance. Args: only_genotype (bool, Default True): Whether to return the Instance as a pd.Series containing only the variables. variables_names (Optional[Sequence[str]], optional): Names of the variables in the dictionary, otherwise v_i. Defaults to None. features_names (Optional[Sequence[str]], optional): Name of the features in the dictionary, otherwise f_i. Defaults to None. score_names (Optional[Sequence[str]], optional): Name of the solvers, otherwise solver_i. Defaults to None. Returns: pd.Series: Pandas Series with the attributes of the instance as keys and the values of the attributes as values. \"\"\" _flatten_data = {} for key , value in self . asdict ( only_genotype = only_genotype , variables_names = variables_names , features_names = features_names , score_names = score_names , ) . items (): if isinstance ( value , dict ): # Flatten nested dicts for sub_key , sub_value in value . items (): _flatten_data [ f \" { sub_key } \" ] = sub_value else : _flatten_data [ key ] = value return pd . Series ( _flatten_data )","title":"Instance"},{"location":"reference/_core/#_core.Instance.__init__","text":"Creates an instance of a Instance (unstructured) for QD algorithms This class is used to represent a solution in a QD algorithm. It contains the variables, fitness, performance, novelty, features, descriptor and portfolio scores of the solution. The variables are stored as a numpy array, and the fitness, performance and novelty are stored as floats. The features, descriptor and portfolio scores are stored as numpy arrays. Parameters: variables ( Optional [ ArrayLike ] , default: None ) \u2013 Variables or genome of the instance. Defaults to None. fitness ( float , default: float64 (0.0) ) \u2013 Fitness of the instance. Defaults to 0.0. p ( float , default: float64 (0.0) ) \u2013 Performance score. Defaults to 0.0. s ( float , default: float64 (0.0) ) \u2013 Novelty score. Defaults to 0.0. features ( Optional [ tuple [ float ]] , default: None ) \u2013 Tuple of features extracted from the domain. Defaults to None. descriptor ( Optional [ tuple [ float ]] , default: None ) \u2013 Tuple with the descriptor information of the instance. Defaults to None. portfolio_scores ( Optional [ tuple [ float ]] , default: None ) \u2013 Scores of the solvers in the portfolio. Defaults to None. Raises: ValueError \u2013 If fitness, p or s are not convertible to float. Source code in digneapy/_core/_instance.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def __init__ ( self , variables : Optional [ npt . ArrayLike ] = None , fitness : np . float64 = np . float64 ( 0.0 ), p : np . float64 = np . float64 ( 0.0 ), s : np . float64 = np . float64 ( 0.0 ), features : Optional [ tuple [ np . float32 ]] = None , descriptor : Optional [ tuple [ np . float32 ]] = None , portfolio_scores : Optional [ tuple [ np . float64 ]] = None , otype = np . float64 , dtype = np . uint32 , ): \"\"\"Creates an instance of a Instance (unstructured) for QD algorithms This class is used to represent a solution in a QD algorithm. It contains the variables, fitness, performance, novelty, features, descriptor and portfolio scores of the solution. The variables are stored as a numpy array, and the fitness, performance and novelty are stored as floats. The features, descriptor and portfolio scores are stored as numpy arrays. Args: variables (Optional[npt.ArrayLike], optional): Variables or genome of the instance. Defaults to None. fitness (float, optional): Fitness of the instance. Defaults to 0.0. p (float, optional): Performance score. Defaults to 0.0. s (float, optional): Novelty score. Defaults to 0.0. features (Optional[tuple[float]], optional): Tuple of features extracted from the domain. Defaults to None. descriptor (Optional[tuple[float]], optional): Tuple with the descriptor information of the instance. Defaults to None. portfolio_scores (Optional[tuple[float]], optional): Scores of the solvers in the portfolio. Defaults to None. Raises: ValueError: If fitness, p or s are not convertible to float. \"\"\" self . _otype = otype self . _dtype = dtype try : fitness = self . _otype ( fitness ) p = self . _otype ( p ) s = self . _otype ( s ) except ValueError : raise ValueError ( \"The fitness, p and s parameters must be convertible to float\" ) self . _vars = ( np . array ( variables , dtype = self . _dtype ) if variables is not None else np . empty ( 0 , dtype = self . _dtype ) ) self . _fit = fitness self . _p = p self . _s = s self . _features = ( np . array ( features , dtype = np . float32 ) if features is not None else np . empty ( 0 , dtype = np . float32 ) ) self . _pscores = ( np . array ( portfolio_scores , dtype = self . _otype ) if portfolio_scores is not None else np . empty ( 0 , dtype = self . _otype ) ) self . _desc = ( np . array ( descriptor , dtype = np . float32 ) if descriptor is not None else np . empty ( 0 , dtype = np . float32 ) )","title":"__init__"},{"location":"reference/_core/#_core.Instance.asdict","text":"Convert the instance to a dictionary. The keys are the names of the attributes and the values are the values of the attributes. Parameters: only_genotype ( bool, Default True , default: False ) \u2013 Whether to return the Instance as a dictionary containing only the variables. variables_names ( Optional [ Sequence [ str ]] , default: None ) \u2013 Names of the variables in the dictionary, otherwise v_i. Defaults to None. features_names ( Optional [ Sequence [ str ]] , default: None ) \u2013 Name of the features in the dictionary, otherwise f_i. Defaults to None. score_names ( Optional [ Sequence [ str ]] , default: None ) \u2013 Name of the solvers, otherwise solver_i. Defaults to None. Returns: dict ( dict ) \u2013 Dictionary with the attributes of the instance as keys and the values of the attributes as values. Source code in digneapy/_core/_instance.py 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 def asdict ( self , only_genotype : bool = False , variables_names : Optional [ Sequence [ str ]] = None , features_names : Optional [ Sequence [ str ]] = None , score_names : Optional [ Sequence [ str ]] = None , ) -> dict : \"\"\"Convert the instance to a dictionary. The keys are the names of the attributes and the values are the values of the attributes. Args: only_genotype (bool, Default True): Whether to return the Instance as a dictionary containing only the variables. variables_names (Optional[Sequence[str]], optional): Names of the variables in the dictionary, otherwise v_i. Defaults to None. features_names (Optional[Sequence[str]], optional): Name of the features in the dictionary, otherwise f_i. Defaults to None. score_names (Optional[Sequence[str]], optional): Name of the solvers, otherwise solver_i. Defaults to None. Returns: dict: Dictionary with the attributes of the instance as keys and the values of the attributes as values. \"\"\" _data = {} if variables_names : if len ( variables_names ) != len ( self . _vars ): print ( f \"Error in asdict(). len(variables_names) = { len ( variables_names ) } != len(variables) ( { len ( self . _vars ) } ). Fallback to v#\" ) _data [ \"variables\" ] = { f \"v { i } \" : v for i , v in enumerate ( self . _vars )} else : _data [ \"variables\" ] = { vk : v for vk , v in zip ( variables_names , self . _vars ) } else : _data [ \"variables\" ] = { f \"v { i } \" : v for i , v in enumerate ( self . _vars )} if only_genotype : return _data else : sckeys = ( [ f \"solver_ { i } \" for i in range ( len ( self . _pscores ))] if score_names is None else score_names ) _data = { \"fitness\" : self . _fit , \"s\" : self . _s , \"p\" : self . _p , \"portfolio_scores\" : { sk : v for sk , v in zip ( sckeys , self . _pscores )}, ** _data , } if len ( self . _desc ) not in ( len ( self . _vars ), len ( self . _features ), len ( self . _pscores ), ): # Transformed descriptor _data [ \"descriptor\" ] = { f \"d { i } \" : v for i , v in enumerate ( self . _desc )} if len ( self . features ) != 0 : f_keys = ( [ f \"f { i } \" for i in range ( len ( self . _features ))] if features_names is None or len ( features_names ) == 0 else features_names ) _data [ \"features\" ] = { fk : v for fk , v in zip ( f_keys , self . _features )} return _data","title":"asdict"},{"location":"reference/_core/#_core.Instance.clone","text":"Create a clone of the current instance. More efficient than using copy.deepcopy. Returns: Self ( Self ) \u2013 Instance object Source code in digneapy/_core/_instance.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def clone ( self ) -> Self : \"\"\"Create a clone of the current instance. More efficient than using copy.deepcopy. Returns: Self: Instance object \"\"\" return Instance ( variables = list ( self . _vars ), fitness = self . _fit , p = self . _p , s = self . _s , features = tuple ( self . _features ), portfolio_scores = tuple ( self . _pscores ), descriptor = tuple ( self . _desc ), )","title":"clone"},{"location":"reference/_core/#_core.Instance.clone_with","text":"Clones an Instance with overriden attributes Returns: \u2013 Instance Source code in digneapy/_core/_instance.py 126 127 128 129 130 131 132 133 134 135 def clone_with ( self , ** overrides ): \"\"\"Clones an Instance with overriden attributes Returns: Instance \"\"\" new_object = self . clone () for key , value in overrides . items (): setattr ( new_object , key , value ) return new_object","title":"clone_with"},{"location":"reference/_core/#_core.Instance.to_json","text":"Convert the instance to a JSON string. The keys are the names of the attributes and the values are the values of the attributes. Returns: str ( str ) \u2013 JSON string with the attributes of the instance as keys and the values of the attributes as values. Source code in digneapy/_core/_instance.py 366 367 368 369 370 371 372 373 374 375 def to_json ( self ) -> str : \"\"\"Convert the instance to a JSON string. The keys are the names of the attributes and the values are the values of the attributes. Returns: str: JSON string with the attributes of the instance as keys and the values of the attributes as values. \"\"\" import json return json . dumps ( self . asdict (), sort_keys = True , indent = 4 )","title":"to_json"},{"location":"reference/_core/#_core.Instance.to_series","text":"Creates a pandas Series from the instance. Parameters: only_genotype ( bool, Default True , default: False ) \u2013 Whether to return the Instance as a pd.Series containing only the variables. variables_names ( Optional [ Sequence [ str ]] , default: None ) \u2013 Names of the variables in the dictionary, otherwise v_i. Defaults to None. features_names ( Optional [ Sequence [ str ]] , default: None ) \u2013 Name of the features in the dictionary, otherwise f_i. Defaults to None. score_names ( Optional [ Sequence [ str ]] , default: None ) \u2013 Name of the solvers, otherwise solver_i. Defaults to None. Returns: Series \u2013 pd.Series: Pandas Series with the attributes of the instance as keys and the values of the attributes as values. Source code in digneapy/_core/_instance.py 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 def to_series ( self , only_genotype : bool = False , variables_names : Optional [ Sequence [ str ]] = None , features_names : Optional [ Sequence [ str ]] = None , score_names : Optional [ Sequence [ str ]] = None , ) -> pd . Series : \"\"\"Creates a pandas Series from the instance. Args: only_genotype (bool, Default True): Whether to return the Instance as a pd.Series containing only the variables. variables_names (Optional[Sequence[str]], optional): Names of the variables in the dictionary, otherwise v_i. Defaults to None. features_names (Optional[Sequence[str]], optional): Name of the features in the dictionary, otherwise f_i. Defaults to None. score_names (Optional[Sequence[str]], optional): Name of the solvers, otherwise solver_i. Defaults to None. Returns: pd.Series: Pandas Series with the attributes of the instance as keys and the values of the attributes as values. \"\"\" _flatten_data = {} for key , value in self . asdict ( only_genotype = only_genotype , variables_names = variables_names , features_names = features_names , score_names = score_names , ) . items (): if isinstance ( value , dict ): # Flatten nested dicts for sub_key , sub_value in value . items (): _flatten_data [ f \" { sub_key } \" ] = sub_value else : _flatten_data [ key ] = value return pd . Series ( _flatten_data )","title":"to_series"},{"location":"reference/_core/#_core.NS","text":"Source code in digneapy/_core/_novelty_search.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 class NS : def __init__ ( self , archive : Optional [ Archive ] = None , k : int = 15 , ): \"\"\"Creates an instance of the Novelty Search Algorithm Args: archive (Archive): Archive to store the instances to guide the evolution. Defaults to Archive(threshold=0.001). k (int, optional): Number of neighbours to calculate the sparseness. Defaults to 15. \"\"\" if k < 0 : raise ValueError ( f \" { __name__ } k must be a positive integer and less than the number of instances.\" ) if archive is not None and not isinstance ( archive , Archive ): raise ValueError ( \"You must provide a valid Archive object\" ) self . _k = k self . _archive = archive if archive is not None else Archive ( threshold = 0.001 ) @property def archive ( self ): return self . _archive @property def k ( self ): return self . _k def __str__ ( self ): return f \"NS(k= { self . _k } ,A= { self . _archive } )\" def __repr__ ( self ) -> str : return f \"NS<k= { self . _k } ,A= { self . _archive } >\" def __call__ ( self , instances_descriptors : np . ndarray ) -> np . ndarray : \"\"\"Computes the Novelty Search of the instance descriptors with respect to the archive. It uses the Euclidean distance to compute the sparseness. Args: instance_descriptors (np.ndarray): Numpy array with the descriptors of the instances archive (Archive): Archive which stores the novelty instances found so far k (int, optional): Number of neighbors to consider in the computation of the sparseness. Defaults to 15. Raises: ValueError: If len(instance_descriptors) <= k Returns: np.ndarray: novelty scores (s) of the instances descriptors \"\"\" if len ( instances_descriptors ) == 0 : raise ValueError ( f \"NS was given an empty population to compute the sparseness. Shape is: { instances_descriptors . shape } \" ) num_instances = len ( instances_descriptors ) num_archive = len ( self . archive ) result = np . zeros ( num_instances , dtype = np . float64 ) if num_archive == 0 and num_instances <= self . _k : # Initially, the archive is empty and we may not have enough instances to evaluate print ( f \"NS has an empty archive at this moment and the given population is not large enough to compute the sparseness. { num_instances } < k ( { self . _k } ). Returning zeros.\" , file = sys . stderr , ) return result if num_instances + num_archive <= self . _k : msg = f \"Trying to calculate novelty search with k( { self . _k } ) >= { num_instances } (instances) + { num_archive } (archive).\" raise ValueError ( msg ) combined = ( instances_descriptors if num_archive == 0 else np . vstack ([ instances_descriptors , self . _archive . descriptors ]) ) for i in range ( num_instances ): mask = np . ones ( num_instances , bool ) mask [ i ] = False differences = combined [ i ] - combined [ np . nonzero ( mask )] distances = np . linalg . norm ( differences , axis = 1 ) _neighbors = np . partition ( distances , self . _k + 1 )[ 1 : self . _k + 1 ] result [ i ] = np . sum ( _neighbors ) / self . _k return result","title":"NS"},{"location":"reference/_core/#_core.NS.__call__","text":"Computes the Novelty Search of the instance descriptors with respect to the archive. It uses the Euclidean distance to compute the sparseness. Parameters: instance_descriptors ( ndarray ) \u2013 Numpy array with the descriptors of the instances archive ( Archive ) \u2013 Archive which stores the novelty instances found so far k ( int ) \u2013 Number of neighbors to consider in the computation of the sparseness. Defaults to 15. Raises: ValueError \u2013 If len(instance_descriptors) <= k Returns: ndarray \u2013 np.ndarray: novelty scores (s) of the instances descriptors Source code in digneapy/_core/_novelty_search.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def __call__ ( self , instances_descriptors : np . ndarray ) -> np . ndarray : \"\"\"Computes the Novelty Search of the instance descriptors with respect to the archive. It uses the Euclidean distance to compute the sparseness. Args: instance_descriptors (np.ndarray): Numpy array with the descriptors of the instances archive (Archive): Archive which stores the novelty instances found so far k (int, optional): Number of neighbors to consider in the computation of the sparseness. Defaults to 15. Raises: ValueError: If len(instance_descriptors) <= k Returns: np.ndarray: novelty scores (s) of the instances descriptors \"\"\" if len ( instances_descriptors ) == 0 : raise ValueError ( f \"NS was given an empty population to compute the sparseness. Shape is: { instances_descriptors . shape } \" ) num_instances = len ( instances_descriptors ) num_archive = len ( self . archive ) result = np . zeros ( num_instances , dtype = np . float64 ) if num_archive == 0 and num_instances <= self . _k : # Initially, the archive is empty and we may not have enough instances to evaluate print ( f \"NS has an empty archive at this moment and the given population is not large enough to compute the sparseness. { num_instances } < k ( { self . _k } ). Returning zeros.\" , file = sys . stderr , ) return result if num_instances + num_archive <= self . _k : msg = f \"Trying to calculate novelty search with k( { self . _k } ) >= { num_instances } (instances) + { num_archive } (archive).\" raise ValueError ( msg ) combined = ( instances_descriptors if num_archive == 0 else np . vstack ([ instances_descriptors , self . _archive . descriptors ]) ) for i in range ( num_instances ): mask = np . ones ( num_instances , bool ) mask [ i ] = False differences = combined [ i ] - combined [ np . nonzero ( mask )] distances = np . linalg . norm ( differences , axis = 1 ) _neighbors = np . partition ( distances , self . _k + 1 )[ 1 : self . _k + 1 ] result [ i ] = np . sum ( _neighbors ) / self . _k return result","title":"__call__"},{"location":"reference/_core/#_core.NS.__init__","text":"Creates an instance of the Novelty Search Algorithm Args: archive (Archive): Archive to store the instances to guide the evolution. Defaults to Archive(threshold=0.001). k (int, optional): Number of neighbours to calculate the sparseness. Defaults to 15. Source code in digneapy/_core/_novelty_search.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def __init__ ( self , archive : Optional [ Archive ] = None , k : int = 15 , ): \"\"\"Creates an instance of the Novelty Search Algorithm Args: archive (Archive): Archive to store the instances to guide the evolution. Defaults to Archive(threshold=0.001). k (int, optional): Number of neighbours to calculate the sparseness. Defaults to 15. \"\"\" if k < 0 : raise ValueError ( f \" { __name__ } k must be a positive integer and less than the number of instances.\" ) if archive is not None and not isinstance ( archive , Archive ): raise ValueError ( \"You must provide a valid Archive object\" ) self . _k = k self . _archive = archive if archive is not None else Archive ( threshold = 0.001 )","title":"__init__"},{"location":"reference/_core/#_core.Problem","text":"Bases: ABC , RNG Source code in digneapy/_core/_problem.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 class Problem ( ABC , RNG ): def __init__ ( self , dimension : int , bounds : Sequence [ tuple ], name : str = \"DefaultProblem\" , dtype = np . float64 , seed : int = 42 , * args , ** kwargs , ): \"\"\"Creates a new problem instance. The problem is defined by its dimension and the bounds of each variable. Args: dimension (int): Number of variables in the problem bounds (Sequence[tuple]): Bounds of each variable in the problem name (str, optional): Name of the problem for printing and logging purposes. Defaults to \"DefaultProblem\". dtype (_type_, optional): Type of the variables. Defaults to np.float64. seed (int, optional): Seed for the RNG. Defaults to 42. \"\"\" self . _name = name self . __name__ = name self . _dimension = dimension self . _bounds = bounds self . _dtype = dtype self . initialize_rng ( seed = seed ) if len ( self . _bounds ) != 0 : ranges = list ( zip ( * bounds )) self . _lbs = np . array ( ranges [ 0 ], dtype = dtype ) self . _ubs = np . array ( ranges [ 1 ], dtype = dtype ) @property def dimension ( self ): return self . _dimension @property def bounds ( self ): return self . _bounds def get_bounds_at ( self , i : int ) -> tuple : if i < 0 or i > len ( self . _bounds ): raise ValueError ( f \"Index { i } out-of-range. The bounds are 0- { len ( self . _bounds ) } \" ) return ( self . _lbs [ i ], self . _ubs [ i ]) @abstractmethod def create_solution ( self ) -> Solution | np . ndarray : \"\"\"Creates a random solution to the problem. This method can be used to initialise the solutions for any algorithm \"\"\" msg = \"create_solution method not implemented in Problem\" raise NotImplementedError ( msg ) @abstractmethod def __array__ ( self , dtype : Any = None , copy : Optional [ bool ] = None ) -> npt . ArrayLike : msg = \"__array__ method not implemented in Problem\" raise NotImplementedError ( msg ) @abstractmethod def evaluate ( self , individual : Sequence | Solution | np . ndarray ) -> Tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Knapsack Args: individual (Sequence | Solution | np.ndarray): Individual to evaluate Raises: ValueError: Raises an error if the len(individual) != len(instance) / 2 Returns: Tuple[float]: fitness \"\"\" msg = \"evaluate method not implemented in Problem\" raise NotImplementedError ( msg ) @abstractmethod def __call__ ( self , individual : Sequence | Solution | np . ndarray ) -> Tuple [ float ]: msg = \"__call__ method not implemented in Problem\" raise NotImplementedError ( msg ) @abstractmethod def to_instance ( self ) -> Instance : \"\"\"Creates an instance from the information of the problem. This method is used in the generators to create instances to evolve \"\"\" msg = \"to_instance method not implemented in Problem\" raise NotImplementedError ( msg ) @abstractmethod def to_file ( self , filename : str ): msg = \"to_file method not implemented in Problem\" raise NotImplementedError ( msg ) @classmethod def from_file ( cls , filename : str ): msg = \"from_file method not implemented in Problem\" raise NotImplementedError ( msg )","title":"Problem"},{"location":"reference/_core/#_core.Problem.__init__","text":"Creates a new problem instance. The problem is defined by its dimension and the bounds of each variable. Parameters: dimension ( int ) \u2013 Number of variables in the problem bounds ( Sequence [ tuple ] ) \u2013 Bounds of each variable in the problem name ( str , default: 'DefaultProblem' ) \u2013 Name of the problem for printing and logging purposes. Defaults to \"DefaultProblem\". dtype ( _type_ , default: float64 ) \u2013 Type of the variables. Defaults to np.float64. seed ( int , default: 42 ) \u2013 Seed for the RNG. Defaults to 42. Source code in digneapy/_core/_problem.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , dimension : int , bounds : Sequence [ tuple ], name : str = \"DefaultProblem\" , dtype = np . float64 , seed : int = 42 , * args , ** kwargs , ): \"\"\"Creates a new problem instance. The problem is defined by its dimension and the bounds of each variable. Args: dimension (int): Number of variables in the problem bounds (Sequence[tuple]): Bounds of each variable in the problem name (str, optional): Name of the problem for printing and logging purposes. Defaults to \"DefaultProblem\". dtype (_type_, optional): Type of the variables. Defaults to np.float64. seed (int, optional): Seed for the RNG. Defaults to 42. \"\"\" self . _name = name self . __name__ = name self . _dimension = dimension self . _bounds = bounds self . _dtype = dtype self . initialize_rng ( seed = seed ) if len ( self . _bounds ) != 0 : ranges = list ( zip ( * bounds )) self . _lbs = np . array ( ranges [ 0 ], dtype = dtype ) self . _ubs = np . array ( ranges [ 1 ], dtype = dtype )","title":"__init__"},{"location":"reference/_core/#_core.Problem.create_solution","text":"Creates a random solution to the problem. This method can be used to initialise the solutions for any algorithm Source code in digneapy/_core/_problem.py 72 73 74 75 76 77 78 79 @abstractmethod def create_solution ( self ) -> Solution | np . ndarray : \"\"\"Creates a random solution to the problem. This method can be used to initialise the solutions for any algorithm \"\"\" msg = \"create_solution method not implemented in Problem\" raise NotImplementedError ( msg )","title":"create_solution"},{"location":"reference/_core/#_core.Problem.evaluate","text":"Evaluates the candidate individual with the information of the Knapsack Parameters: individual ( Sequence | Solution | ndarray ) \u2013 Individual to evaluate Raises: ValueError \u2013 Raises an error if the len(individual) != len(instance) / 2 Returns: Tuple [ float ] \u2013 Tuple[float]: fitness Source code in digneapy/_core/_problem.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 @abstractmethod def evaluate ( self , individual : Sequence | Solution | np . ndarray ) -> Tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Knapsack Args: individual (Sequence | Solution | np.ndarray): Individual to evaluate Raises: ValueError: Raises an error if the len(individual) != len(instance) / 2 Returns: Tuple[float]: fitness \"\"\" msg = \"evaluate method not implemented in Problem\" raise NotImplementedError ( msg )","title":"evaluate"},{"location":"reference/_core/#_core.Problem.to_instance","text":"Creates an instance from the information of the problem. This method is used in the generators to create instances to evolve Source code in digneapy/_core/_problem.py 109 110 111 112 113 114 115 @abstractmethod def to_instance ( self ) -> Instance : \"\"\"Creates an instance from the information of the problem. This method is used in the generators to create instances to evolve \"\"\" msg = \"to_instance method not implemented in Problem\" raise NotImplementedError ( msg )","title":"to_instance"},{"location":"reference/_core/#_core.RNG","text":"Bases: Protocol Protocol to type check all operators have _rng of instances types in digneapy Source code in digneapy/_core/types.py 18 19 20 21 22 23 24 25 26 class RNG ( Protocol ): \"\"\"Protocol to type check all operators have _rng of instances types in digneapy\"\"\" _rng : Generator _seed : int | None def initialize_rng ( self , seed : Optional [ int ] = None ): self . _seed = seed self . _rng = np . random . default_rng ()","title":"RNG"},{"location":"reference/_core/#_core.Solution","text":"Class representing a solution in a genetic algorithm. It contains the variables, objectives, constraints, and fitness of the solution. Source code in digneapy/_core/_solution.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 class Solution : \"\"\" Class representing a solution in a genetic algorithm. It contains the variables, objectives, constraints, and fitness of the solution. \"\"\" def __init__ ( self , variables : Optional [ Iterable ] = [], objectives : Optional [ Iterable ] = [], constraints : Optional [ Iterable ] = [], fitness : np . float64 = np . float64 ( 0.0 ), dtype = np . uint32 , otype = np . float64 , ): \"\"\"Creates a new solution object. The variables is a numpy array of the solution's genes. The objectives and constraints are numpy arrays of the solution's objectives and constraints. The fitness is a float representing the solution's fitness value. Args: variables (Optional[Iterable], optional): Tuple or any other iterable with the variables/variables. Defaults to None. objectives (Optional[Iterable], optional): Tuple or any other iterable with the objectives values. Defaults to None. constraints (Optional[Iterable], optional): Tuple or any other iterable with the constraint values. Defaults to None. fitness (float, optional): Fitness of the solution. Defaults to 0.0. \"\"\" self . _otype = otype self . _dtype = dtype self . variables = np . asarray ( variables , dtype = self . dtype ) self . objectives = np . array ( objectives , dtype = self . otype ) self . constraints = np . array ( constraints , dtype = self . otype ) self . fitness = otype ( fitness ) @property def dtype ( self ): return self . _dtype @property def otype ( self ): return self . _otype def clone ( self ) -> Self : \"\"\"Returns a deep copy of the solution. It is more efficient than using the copy module. Returns: Self: Solution object \"\"\" return Solution ( variables = list ( self . variables ), objectives = list ( self . objectives ), constraints = list ( self . constraints ), fitness = self . fitness , otype = self . otype , ) def clone_with ( self , ** overrides ): \"\"\"Clones an Instance with overriden attributes Returns: Instance \"\"\" new_object = self . clone () for key , value in overrides . items (): setattr ( new_object , key , value ) return new_object def __str__ ( self ) -> str : return f \"Solution(dim= { len ( self . variables ) } ,f= { self . fitness } ,objs= { self . objectives } ,const= { self . constraints } )\" def __repr__ ( self ) -> str : return f \"Solution<dim= { len ( self . variables ) } ,f= { self . fitness } ,objs= { self . objectives } ,const= { self . constraints } >\" def __len__ ( self ) -> int : return len ( self . variables ) def __iter__ ( self ): return iter ( self . variables ) def __bool__ ( self ): return len ( self ) != 0 def __eq__ ( self , other ) -> bool : if isinstance ( other , Solution ): try : return all ( a == b for a , b in zip ( self , other , strict = True )) except ValueError : return False else : return NotImplemented def __gt__ ( self , other ): if not isinstance ( other , Solution ): msg = f \"Other of type { other . __class__ . __name__ } can not be compared with with { self . __class__ . __name__ } \" print ( msg ) return NotImplemented return self . fitness > other . fitness def __getitem__ ( self , key ): if isinstance ( key , slice ): cls = type ( self ) # To facilitate subclassing return cls ( self . variables [ key ]) index = operator . index ( key ) return self . variables [ index ] def __setitem__ ( self , key , value ): self . variables [ key ] = value","title":"Solution"},{"location":"reference/_core/#_core.Solution.__init__","text":"Creates a new solution object. The variables is a numpy array of the solution's genes. The objectives and constraints are numpy arrays of the solution's objectives and constraints. The fitness is a float representing the solution's fitness value. Parameters: variables ( Optional [ Iterable ] , default: [] ) \u2013 Tuple or any other iterable with the variables/variables. Defaults to None. objectives ( Optional [ Iterable ] , default: [] ) \u2013 Tuple or any other iterable with the objectives values. Defaults to None. constraints ( Optional [ Iterable ] , default: [] ) \u2013 Tuple or any other iterable with the constraint values. Defaults to None. fitness ( float , default: float64 (0.0) ) \u2013 Fitness of the solution. Defaults to 0.0. Source code in digneapy/_core/_solution.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def __init__ ( self , variables : Optional [ Iterable ] = [], objectives : Optional [ Iterable ] = [], constraints : Optional [ Iterable ] = [], fitness : np . float64 = np . float64 ( 0.0 ), dtype = np . uint32 , otype = np . float64 , ): \"\"\"Creates a new solution object. The variables is a numpy array of the solution's genes. The objectives and constraints are numpy arrays of the solution's objectives and constraints. The fitness is a float representing the solution's fitness value. Args: variables (Optional[Iterable], optional): Tuple or any other iterable with the variables/variables. Defaults to None. objectives (Optional[Iterable], optional): Tuple or any other iterable with the objectives values. Defaults to None. constraints (Optional[Iterable], optional): Tuple or any other iterable with the constraint values. Defaults to None. fitness (float, optional): Fitness of the solution. Defaults to 0.0. \"\"\" self . _otype = otype self . _dtype = dtype self . variables = np . asarray ( variables , dtype = self . dtype ) self . objectives = np . array ( objectives , dtype = self . otype ) self . constraints = np . array ( constraints , dtype = self . otype ) self . fitness = otype ( fitness )","title":"__init__"},{"location":"reference/_core/#_core.Solution.clone","text":"Returns a deep copy of the solution. It is more efficient than using the copy module. Returns: Self ( Self ) \u2013 Solution object Source code in digneapy/_core/_solution.py 62 63 64 65 66 67 68 69 70 71 72 73 74 def clone ( self ) -> Self : \"\"\"Returns a deep copy of the solution. It is more efficient than using the copy module. Returns: Self: Solution object \"\"\" return Solution ( variables = list ( self . variables ), objectives = list ( self . objectives ), constraints = list ( self . constraints ), fitness = self . fitness , otype = self . otype , )","title":"clone"},{"location":"reference/_core/#_core.Solution.clone_with","text":"Clones an Instance with overriden attributes Returns: \u2013 Instance Source code in digneapy/_core/_solution.py 76 77 78 79 80 81 82 83 84 85 def clone_with ( self , ** overrides ): \"\"\"Clones an Instance with overriden attributes Returns: Instance \"\"\" new_object = self . clone () for key , value in overrides . items (): setattr ( new_object , key , value ) return new_object","title":"clone_with"},{"location":"reference/_core/#_core.Solver","text":"Bases: ABC , SupportsSolve [ P ] Solver is any callable type that receives a OptProblem as its argument and returns a tuple with the solution found Source code in digneapy/_core/_solver.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class Solver ( ABC , SupportsSolve [ P ]): \"\"\"Solver is any callable type that receives a OptProblem as its argument and returns a tuple with the solution found \"\"\" @abstractmethod def __call__ ( self , problem : P , * args , ** kwargs ) -> list [ Solution ]: \"\"\"Solves a optimisation problem Args: problem (OptProblem): Any optimisation problem or callablle that receives a Sequence and returns a Tuple[float] Raises: NotImplementedError: Must be implemented by subclasses Returns: List[Solution]: Returns a sequence of olutions \"\"\" msg = \"__call__ method not implemented in Solver\" raise NotImplementedError ( msg )","title":"Solver"},{"location":"reference/_core/#_core.Solver.__call__","text":"Solves a optimisation problem Parameters: problem ( OptProblem ) \u2013 Any optimisation problem or callablle that receives a Sequence and returns a Tuple[float] Raises: NotImplementedError \u2013 Must be implemented by subclasses Returns: list [ Solution ] \u2013 List[Solution]: Returns a sequence of olutions Source code in digneapy/_core/_solver.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 @abstractmethod def __call__ ( self , problem : P , * args , ** kwargs ) -> list [ Solution ]: \"\"\"Solves a optimisation problem Args: problem (OptProblem): Any optimisation problem or callablle that receives a Sequence and returns a Tuple[float] Raises: NotImplementedError: Must be implemented by subclasses Returns: List[Solution]: Returns a sequence of olutions \"\"\" msg = \"__call__ method not implemented in Solver\" raise NotImplementedError ( msg )","title":"__call__"},{"location":"reference/_core/#_core.Statistics","text":"Source code in digneapy/_core/_metrics.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 class Statistics : def __init__ ( self ): self . _stats_s = tools . Statistics ( key = attrgetter ( \"s\" )) self . _stats_p = tools . Statistics ( key = attrgetter ( \"p\" )) self . _stats_f = tools . Statistics ( key = attrgetter ( \"fitness\" )) self . _stats = tools . MultiStatistics ( s = self . _stats_s , p = self . _stats_p , fitness = self . _stats_f ) self . _stats . register ( \"mean\" , np . mean ) self . _stats . register ( \"std\" , np . std ) self . _stats . register ( \"min\" , np . min ) self . _stats . register ( \"max\" , np . max ) self . _stats . register ( \"qd_score\" , np . sum ) def __call__ ( self , population : Sequence [ Instance ], as_series : bool = False ) -> dict | pd . Series : \"\"\"Calculates the statistics of the population. Args: population (Sequence[Instance]): List of instances to calculate the statistics. Returns: dict: Dictionary with the statistics of the population. \"\"\" if len ( population ) == 0 : raise ValueError ( \"Error: Trying to calculate the metrics with an empty population\" ) if not all ( isinstance ( ind , Instance ) for ind in population ): raise TypeError ( \"Error: Population must be a sequence of Instance objects\" ) record = self . _stats . compile ( population ) if as_series : _flatten_record = {} for key , value in record . items (): if isinstance ( value , dict ): # Flatten nested dicts for sub_key , sub_value in value . items (): _flatten_record [ f \" { key } _ { sub_key } \" ] = sub_value else : _flatten_record [ key ] = value return pd . Series ( _flatten_record ) else : return record","title":"Statistics"},{"location":"reference/_core/#_core.Statistics.__call__","text":"Calculates the statistics of the population. Args: population (Sequence[Instance]): List of instances to calculate the statistics. Returns: dict: Dictionary with the statistics of the population. Source code in digneapy/_core/_metrics.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def __call__ ( self , population : Sequence [ Instance ], as_series : bool = False ) -> dict | pd . Series : \"\"\"Calculates the statistics of the population. Args: population (Sequence[Instance]): List of instances to calculate the statistics. Returns: dict: Dictionary with the statistics of the population. \"\"\" if len ( population ) == 0 : raise ValueError ( \"Error: Trying to calculate the metrics with an empty population\" ) if not all ( isinstance ( ind , Instance ) for ind in population ): raise TypeError ( \"Error: Population must be a sequence of Instance objects\" ) record = self . _stats . compile ( population ) if as_series : _flatten_record = {} for key , value in record . items (): if isinstance ( value , dict ): # Flatten nested dicts for sub_key , sub_value in value . items (): _flatten_record [ f \" { key } _ { sub_key } \" ] = sub_value else : _flatten_record [ key ] = value return pd . Series ( _flatten_record ) else : return record","title":"__call__"},{"location":"reference/_core/#_core.SupportsSolve","text":"Bases: Protocol [ P ] Protocol to type check all the solver types in digneapy. A solver is any callable type that receives at least a problem (Problem) and returns a list of object of the Solution class. Source code in digneapy/_core/_solver.py 20 21 22 23 24 25 26 class SupportsSolve ( Protocol [ P ]): \"\"\"Protocol to type check all the solver types in digneapy. A solver is any callable type that receives at least a problem (Problem) and returns a list of object of the Solution class. \"\"\" def __call__ ( self , problem : P , * args , ** kwargs ) -> list [ Solution ]: ...","title":"SupportsSolve"},{"location":"reference/_core/#_core.dominated_novelty_search","text":"Dominated Novelty Search (DNS) Bahlous-Boldi, R., Faldor, M., Grillotti, L., Janmohamed, H., Coiffard, L., Spector, L., & Cully, A. (2025). Dominated Novelty Search: Rethinking Local Competition in Quality-Diversity. 1. https://arxiv.org/abs/2502.00593v1 Quality-Diversity algorithm that implements local competition through dynamic fitness transformations, eliminating the need for predefined bounds or parameters. The competition fitness, also known as the dominated novelty score, is calculated as the average distance to the k nearest neighbors with higher fitness. The method returns a descending sorted list of instances by their competition fitness value. For each instance ``i'' in the sequence, we calculate all the other instances that dominate it. Then, we compute the distances between their descriptors using the norm of the difference for each dimension of the descriptors. Novel instances will get a competition fitness of np.inf (assuring they will survive). Less novel instances will be selected by their competition fitness value. This competition mechanism creates two complementary evolutionary pressures: individuals must either improve their fitness or discover distinct behaviors that differ from better-performing solutions. Solutions that have no fitter neighbors (D\ud835\udc56 = \u2205) receive an infinite competition fitness, ensuring their preservation in the population. Parameters: descriptors ( ndarray ) \u2013 Numpy array with the descriptors of the instances performances ( ndarray ) \u2013 Numpy array with the performance values of the instances k ( int , default: 15 ) \u2013 Number of nearest neighbours to calculate the competition fitness. Default to 15. force_feasible_only ( bool , default: True ) \u2013 Allow only instances with performance >= 0 to be considered. Default True. Raises: ValueError: If len(d) where d is the descriptor of each instance i differs from another ValueError: If k >= len(instances) Returns: Tuple [ ndarray , ndarray , ndarray , ndarray ] \u2013 Tuple[np.ndarray]: Tuple with the descriptors, performances and competition fitness values sorted, plus the sorted indexing (descending order). Source code in digneapy/_core/_novelty_search.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def dominated_novelty_search ( descriptors : np . ndarray , performances : np . ndarray , k : int = 15 , force_feasible_only : bool = True , ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray , np . ndarray ]: \"\"\" Dominated Novelty Search (DNS) Bahlous-Boldi, R., Faldor, M., Grillotti, L., Janmohamed, H., Coiffard, L., Spector, L., & Cully, A. (2025). Dominated Novelty Search: Rethinking Local Competition in Quality-Diversity. 1. https://arxiv.org/abs/2502.00593v1 Quality-Diversity algorithm that implements local competition through dynamic fitness transformations, eliminating the need for predefined bounds or parameters. The competition fitness, also known as the dominated novelty score, is calculated as the average distance to the k nearest neighbors with higher fitness. The method returns a descending sorted list of instances by their competition fitness value. For each instance ``i'' in the sequence, we calculate all the other instances that dominate it. Then, we compute the distances between their descriptors using the norm of the difference for each dimension of the descriptors. Novel instances will get a competition fitness of np.inf (assuring they will survive). Less novel instances will be selected by their competition fitness value. This competition mechanism creates two complementary evolutionary pressures: individuals must either improve their fitness or discover distinct behaviors that differ from better-performing solutions. Solutions that have no fitter neighbors (D\ud835\udc56 = \u2205) receive an infinite competition fitness, ensuring their preservation in the population. Args: descriptors (np.ndarray): Numpy array with the descriptors of the instances performances (np.ndarray): Numpy array with the performance values of the instances k (int): Number of nearest neighbours to calculate the competition fitness. Default to 15. force_feasible_only (bool): Allow only instances with performance >= 0 to be considered. Default True. Raises: ValueError: If len(d) where d is the descriptor of each instance i differs from another ValueError: If k >= len(instances) Returns: Tuple[np.ndarray]: Tuple with the descriptors, performances and competition fitness values sorted, plus the sorted indexing (descending order). \"\"\" num_instances = len ( descriptors ) if num_instances <= k : msg = f \"Trying to calculate the dominated novelty search with k( { k } ) > len(instances) = { num_instances } \" raise ValueError ( msg ) if len ( performances ) != len ( descriptors ): raise ValueError ( f \"Array mismatch between peformances and descriptors. len(performance) = { len ( performances ) } != { len ( descriptors ) } len(descriptors)\" ) # Try to force only feasible performances to get proper biased instances is_unfeasible = ( performances < 0.0 if force_feasible_only else ( performances == - np . inf ) ) fitter = performances [:, None ] <= performances [ None , :] fitter = np . where ( is_unfeasible [ None , :], False , fitter ) np . fill_diagonal ( fitter , False ) distance = np . linalg . norm ( descriptors [:, None , :] - descriptors [ None , :, :], axis =- 1 ) distance = np . where ( fitter , distance , np . inf ) neg_dist = - distance indices = np . argpartition ( neg_dist , - k , axis =- 1 )[ ... , - k :] values = np . take_along_axis ( neg_dist , indices , axis =- 1 ) indices = np . argsort ( values , axis =- 1 )[ ... , :: - 1 ] values = np . take_along_axis ( values , indices , axis =- 1 ) indices = np . take_along_axis ( indices , indices , axis =- 1 ) distance = np . mean ( - values , where = np . take_along_axis ( fitter , indices , axis = 1 ), axis =- 1 ) distance = np . where ( np . isnan ( distance ), np . inf , distance ) distance = np . where ( is_unfeasible , - np . inf , distance ) sorted_indices = np . argsort ( - distance ) return ( descriptors [ sorted_indices ], performances [ sorted_indices ], distance [ sorted_indices ], sorted_indices , )","title":"dominated_novelty_search"},{"location":"reference/_core/#_core.qd_score","text":"Calculates the Quality Diversity score of a set of instances fitness. Parameters: instances ( Sequence [ float ] ) \u2013 List with the fitness of several instances to calculate the QD score. Returns: float ( float64 ) \u2013 Sum of the fitness of all instances. Source code in digneapy/_core/_metrics.py 23 24 25 26 27 28 29 30 31 32 def qd_score ( instances_fitness : np . ndarray ) -> np . float64 : \"\"\"Calculates the Quality Diversity score of a set of instances fitness. Args: instances (Sequence[float]): List with the fitness of several instances to calculate the QD score. Returns: float: Sum of the fitness of all instances. \"\"\" return np . sum ( instances_fitness )","title":"qd_score"},{"location":"reference/_core/#_core.qd_score_auc","text":"Calculates the Quantifying Efficiency in Quality Diversity Optimization In quality diversity (QD) optimization, the QD score is a holistic metric which sums the objective values of all cells in the archive. Since the QD score only measures the performance of a QD algorithm at a single point in time, it fails to reflect algorithm efficiency. Two algorithms may have the same QD score even though one algorithm achieved that score with fewer evaluations. We propose a metric called \u201cQD score AUC\u201d which quantifies this efficiency. Parameters: qd_scores ( Sequence [ float ] ) \u2013 Sequence of QD scores. batch_size ( int ) \u2013 Number of instances evaluated in each generation. Returns: float64 \u2013 np.float64: QD score AUC metric. Source code in digneapy/_core/_metrics.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def qd_score_auc ( qd_scores : np . ndarray , batch_size : int ) -> np . float64 : \"\"\"Calculates the Quantifying Efficiency in Quality Diversity Optimization In quality diversity (QD) optimization, the QD score is a holistic metric which sums the objective values of all cells in the archive. Since the QD score only measures the performance of a QD algorithm at a single point in time, it fails to reflect algorithm efficiency. Two algorithms may have the same QD score even though one algorithm achieved that score with fewer evaluations. We propose a metric called \u201cQD score AUC\u201d which quantifies this efficiency. Args: qd_scores (Sequence[float]): Sequence of QD scores. batch_size (int): Number of instances evaluated in each generation. Returns: np.float64: QD score AUC metric. \"\"\" return np . sum ( qd_scores ) * batch_size","title":"qd_score_auc"},{"location":"reference/_core/_constants/","text":"@File : _constants.py @Time : 2024/06/07 11:20:47 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2024, Alejandro Marrero @Desc : None","title":" constants"},{"location":"reference/_core/_domain/","text":"@File : domain.py @Time : 2024/06/07 14:08:47 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2024, Alejandro Marrero @Desc : None Domain Bases: ABC , RNG Domain is a class that defines the domain of the problem. The domain is defined by its dimension and the bounds of each variable. Parameters: RNG \u2013 Subclass that implements the RNG protocol Source code in digneapy/_core/_domain.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 class Domain ( ABC , RNG ): \"\"\"Domain is a class that defines the domain of the problem. The domain is defined by its dimension and the bounds of each variable. Args: RNG: Subclass that implements the RNG protocol \"\"\" def __init__ ( self , dimension : int , bounds : Sequence [ tuple ], dtype = np . float64 , name : str = \"Domain\" , feat_names : Optional [ Sequence [ str ]] = None , seed : Optional [ int ] = None , * args , ** kwargs , ): self . name = name self . __name__ = name self . _dimension = dimension self . _bounds = bounds self . _dtype = dtype self . feat_names = feat_names if feat_names else list () self . initialize_rng ( seed = seed ) if len ( self . _bounds ) != 0 : ranges = list ( zip ( * bounds )) self . _lbs = np . array ( ranges [ 0 ], dtype = dtype ) self . _ubs = np . array ( ranges [ 1 ], dtype = dtype ) @abstractmethod def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances for the domain. Args: n (int, optional): Number of instances to generate. Defaults to 1. Returns: List[Instance]: A list of Instance objects created from the raw numpy generation \"\"\" raise NotImplementedError ( \"generate_n_instances is not implemented in Domain class.\" ) @abstractmethod def generate_problems_from_instances ( self , instances : Sequence [ Instance ] | np . ndarray ) -> List [ Problem ]: msg = \"generate_problems_from_instances is not implemented in Domain class.\" raise NotImplementedError ( msg ) @abstractmethod def extract_features ( self , instances : Sequence [ Instance ] | np . ndarray ) -> np . ndarray : \"\"\"Extract the features of the instances based on the domain Args: instance (Instance): Instance to extract the features from Returns: Tuple: Values of each feature \"\"\" msg = \"extract_features is not implemented in Domain class.\" raise NotImplementedError ( msg ) @abstractmethod def extract_features_as_dict ( self , instances : Sequence [ Instance ] | np . ndarray ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Args: instance (Instance): Instance to extract the features from Returns: Mapping[str, float]: Dictionary with the names/values of each feature \"\"\" msg = \"extract_features_as_dict is not implemented in Domain class.\" raise NotImplementedError ( msg ) @property def bounds ( self ): return self . _bounds def get_bounds_at ( self , i : int ) -> tuple : if i < 0 or i > len ( self . _bounds ): raise ValueError ( f \"Index { i } out-of-range. The bounds are 0- { len ( self . _bounds ) } \" ) return ( self . _lbs [ i ], self . _ubs [ i ]) @property def dimension ( self ): return self . _dimension def __len__ ( self ): return self . _dimension extract_features ( instances ) abstractmethod Extract the features of the instances based on the domain Parameters: instance ( Instance ) \u2013 Instance to extract the features from Returns: Tuple ( ndarray ) \u2013 Values of each feature Source code in digneapy/_core/_domain.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 @abstractmethod def extract_features ( self , instances : Sequence [ Instance ] | np . ndarray ) -> np . ndarray : \"\"\"Extract the features of the instances based on the domain Args: instance (Instance): Instance to extract the features from Returns: Tuple: Values of each feature \"\"\" msg = \"extract_features is not implemented in Domain class.\" raise NotImplementedError ( msg ) extract_features_as_dict ( instances ) abstractmethod Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Parameters: instance ( Instance ) \u2013 Instance to extract the features from Returns: List [ Dict [ str , float32 ]] \u2013 Mapping[str, float]: Dictionary with the names/values of each feature Source code in digneapy/_core/_domain.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 @abstractmethod def extract_features_as_dict ( self , instances : Sequence [ Instance ] | np . ndarray ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Args: instance (Instance): Instance to extract the features from Returns: Mapping[str, float]: Dictionary with the names/values of each feature \"\"\" msg = \"extract_features_as_dict is not implemented in Domain class.\" raise NotImplementedError ( msg ) generate_instances ( n = 1 ) abstractmethod Generates N instances for the domain. Parameters: n ( int , default: 1 ) \u2013 Number of instances to generate. Defaults to 1. Returns: List [ Instance ] \u2013 List[Instance]: A list of Instance objects created from the raw numpy generation Source code in digneapy/_core/_domain.py 57 58 59 60 61 62 63 64 65 66 67 68 69 @abstractmethod def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances for the domain. Args: n (int, optional): Number of instances to generate. Defaults to 1. Returns: List[Instance]: A list of Instance objects created from the raw numpy generation \"\"\" raise NotImplementedError ( \"generate_n_instances is not implemented in Domain class.\" )","title":" domain"},{"location":"reference/_core/_domain/#_core._domain.Domain","text":"Bases: ABC , RNG Domain is a class that defines the domain of the problem. The domain is defined by its dimension and the bounds of each variable. Parameters: RNG \u2013 Subclass that implements the RNG protocol Source code in digneapy/_core/_domain.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 class Domain ( ABC , RNG ): \"\"\"Domain is a class that defines the domain of the problem. The domain is defined by its dimension and the bounds of each variable. Args: RNG: Subclass that implements the RNG protocol \"\"\" def __init__ ( self , dimension : int , bounds : Sequence [ tuple ], dtype = np . float64 , name : str = \"Domain\" , feat_names : Optional [ Sequence [ str ]] = None , seed : Optional [ int ] = None , * args , ** kwargs , ): self . name = name self . __name__ = name self . _dimension = dimension self . _bounds = bounds self . _dtype = dtype self . feat_names = feat_names if feat_names else list () self . initialize_rng ( seed = seed ) if len ( self . _bounds ) != 0 : ranges = list ( zip ( * bounds )) self . _lbs = np . array ( ranges [ 0 ], dtype = dtype ) self . _ubs = np . array ( ranges [ 1 ], dtype = dtype ) @abstractmethod def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances for the domain. Args: n (int, optional): Number of instances to generate. Defaults to 1. Returns: List[Instance]: A list of Instance objects created from the raw numpy generation \"\"\" raise NotImplementedError ( \"generate_n_instances is not implemented in Domain class.\" ) @abstractmethod def generate_problems_from_instances ( self , instances : Sequence [ Instance ] | np . ndarray ) -> List [ Problem ]: msg = \"generate_problems_from_instances is not implemented in Domain class.\" raise NotImplementedError ( msg ) @abstractmethod def extract_features ( self , instances : Sequence [ Instance ] | np . ndarray ) -> np . ndarray : \"\"\"Extract the features of the instances based on the domain Args: instance (Instance): Instance to extract the features from Returns: Tuple: Values of each feature \"\"\" msg = \"extract_features is not implemented in Domain class.\" raise NotImplementedError ( msg ) @abstractmethod def extract_features_as_dict ( self , instances : Sequence [ Instance ] | np . ndarray ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Args: instance (Instance): Instance to extract the features from Returns: Mapping[str, float]: Dictionary with the names/values of each feature \"\"\" msg = \"extract_features_as_dict is not implemented in Domain class.\" raise NotImplementedError ( msg ) @property def bounds ( self ): return self . _bounds def get_bounds_at ( self , i : int ) -> tuple : if i < 0 or i > len ( self . _bounds ): raise ValueError ( f \"Index { i } out-of-range. The bounds are 0- { len ( self . _bounds ) } \" ) return ( self . _lbs [ i ], self . _ubs [ i ]) @property def dimension ( self ): return self . _dimension def __len__ ( self ): return self . _dimension","title":"Domain"},{"location":"reference/_core/_domain/#_core._domain.Domain.extract_features","text":"Extract the features of the instances based on the domain Parameters: instance ( Instance ) \u2013 Instance to extract the features from Returns: Tuple ( ndarray ) \u2013 Values of each feature Source code in digneapy/_core/_domain.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 @abstractmethod def extract_features ( self , instances : Sequence [ Instance ] | np . ndarray ) -> np . ndarray : \"\"\"Extract the features of the instances based on the domain Args: instance (Instance): Instance to extract the features from Returns: Tuple: Values of each feature \"\"\" msg = \"extract_features is not implemented in Domain class.\" raise NotImplementedError ( msg )","title":"extract_features"},{"location":"reference/_core/_domain/#_core._domain.Domain.extract_features_as_dict","text":"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Parameters: instance ( Instance ) \u2013 Instance to extract the features from Returns: List [ Dict [ str , float32 ]] \u2013 Mapping[str, float]: Dictionary with the names/values of each feature Source code in digneapy/_core/_domain.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 @abstractmethod def extract_features_as_dict ( self , instances : Sequence [ Instance ] | np . ndarray ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Args: instance (Instance): Instance to extract the features from Returns: Mapping[str, float]: Dictionary with the names/values of each feature \"\"\" msg = \"extract_features_as_dict is not implemented in Domain class.\" raise NotImplementedError ( msg )","title":"extract_features_as_dict"},{"location":"reference/_core/_domain/#_core._domain.Domain.generate_instances","text":"Generates N instances for the domain. Parameters: n ( int , default: 1 ) \u2013 Number of instances to generate. Defaults to 1. Returns: List [ Instance ] \u2013 List[Instance]: A list of Instance objects created from the raw numpy generation Source code in digneapy/_core/_domain.py 57 58 59 60 61 62 63 64 65 66 67 68 69 @abstractmethod def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances for the domain. Args: n (int, optional): Number of instances to generate. Defaults to 1. Returns: List[Instance]: A list of Instance objects created from the raw numpy generation \"\"\" raise NotImplementedError ( \"generate_n_instances is not implemented in Domain class.\" )","title":"generate_instances"},{"location":"reference/_core/_instance/","text":"@File : instance.py @Time : 2024/06/07 14:09:43 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2024, Alejandro Marrero @Desc : None Instance Source code in digneapy/_core/_instance.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 class Instance : __slots__ = ( \"_vars\" , \"_fit\" , \"_p\" , \"_s\" , \"_features\" , \"_desc\" , \"_pscores\" , \"_otype\" , \"_dtype\" , ) def __init__ ( self , variables : Optional [ npt . ArrayLike ] = None , fitness : np . float64 = np . float64 ( 0.0 ), p : np . float64 = np . float64 ( 0.0 ), s : np . float64 = np . float64 ( 0.0 ), features : Optional [ tuple [ np . float32 ]] = None , descriptor : Optional [ tuple [ np . float32 ]] = None , portfolio_scores : Optional [ tuple [ np . float64 ]] = None , otype = np . float64 , dtype = np . uint32 , ): \"\"\"Creates an instance of a Instance (unstructured) for QD algorithms This class is used to represent a solution in a QD algorithm. It contains the variables, fitness, performance, novelty, features, descriptor and portfolio scores of the solution. The variables are stored as a numpy array, and the fitness, performance and novelty are stored as floats. The features, descriptor and portfolio scores are stored as numpy arrays. Args: variables (Optional[npt.ArrayLike], optional): Variables or genome of the instance. Defaults to None. fitness (float, optional): Fitness of the instance. Defaults to 0.0. p (float, optional): Performance score. Defaults to 0.0. s (float, optional): Novelty score. Defaults to 0.0. features (Optional[tuple[float]], optional): Tuple of features extracted from the domain. Defaults to None. descriptor (Optional[tuple[float]], optional): Tuple with the descriptor information of the instance. Defaults to None. portfolio_scores (Optional[tuple[float]], optional): Scores of the solvers in the portfolio. Defaults to None. Raises: ValueError: If fitness, p or s are not convertible to float. \"\"\" self . _otype = otype self . _dtype = dtype try : fitness = self . _otype ( fitness ) p = self . _otype ( p ) s = self . _otype ( s ) except ValueError : raise ValueError ( \"The fitness, p and s parameters must be convertible to float\" ) self . _vars = ( np . array ( variables , dtype = self . _dtype ) if variables is not None else np . empty ( 0 , dtype = self . _dtype ) ) self . _fit = fitness self . _p = p self . _s = s self . _features = ( np . array ( features , dtype = np . float32 ) if features is not None else np . empty ( 0 , dtype = np . float32 ) ) self . _pscores = ( np . array ( portfolio_scores , dtype = self . _otype ) if portfolio_scores is not None else np . empty ( 0 , dtype = self . _otype ) ) self . _desc = ( np . array ( descriptor , dtype = np . float32 ) if descriptor is not None else np . empty ( 0 , dtype = np . float32 ) ) @property def dtype ( self ): return self . _dtype @property def otype ( self ): return self . _otype def clone ( self ) -> Self : \"\"\"Create a clone of the current instance. More efficient than using copy.deepcopy. Returns: Self: Instance object \"\"\" return Instance ( variables = list ( self . _vars ), fitness = self . _fit , p = self . _p , s = self . _s , features = tuple ( self . _features ), portfolio_scores = tuple ( self . _pscores ), descriptor = tuple ( self . _desc ), ) def clone_with ( self , ** overrides ): \"\"\"Clones an Instance with overriden attributes Returns: Instance \"\"\" new_object = self . clone () for key , value in overrides . items (): setattr ( new_object , key , value ) return new_object @property def variables ( self ): return self . _vars @variables . setter def variables ( self , new_variables : npt . ArrayLike ): if len ( new_variables ) != len ( self . _vars ): raise ValueError ( \"Updating the variables of an Instance object with a different number of values.\" f \"Instance have { len ( self . _vars ) } \" f \"variables and the new_variables sequence have { len ( new_variables ) } \" ) self . _vars = np . asarray ( new_variables ) @property def p ( self ) -> np . float64 : return self . _p @p . setter def p ( self , performance : np . float64 ): try : performance = np . float64 ( performance ) except ValueError : # if performance != 0.0 and not float(performance): msg = f \"The performance value { performance } is not a float in 'p' setter of class { self . __class__ . __name__ } \" raise ValueError ( msg ) self . _p = performance @property def s ( self ) -> np . float64 : return self . _s @s . setter def s ( self , novelty : np . float64 ): try : novelty = np . float64 ( novelty ) except ValueError : # if novelty != 0.0 and not float(novelty): msg = f \"The novelty value { novelty } is not a float in 's' setter of class { self . __class__ . __name__ } \" raise ValueError ( msg ) self . _s = novelty @property def fitness ( self ) -> np . float64 : return self . _fit @fitness . setter def fitness ( self , f : np . float64 ): try : f = np . float64 ( f ) except ValueError : # if f != 0.0 and not float(f): msg = f \"The fitness value { f } is not a float in fitness setter of class { self . __class__ . __name__ } \" raise ValueError ( msg ) self . _fit = f @property def features ( self ) -> np . ndarray : return self . _features @features . setter def features ( self , features : npt . ArrayLike ): self . _features = np . asarray ( features ) @property def descriptor ( self ) -> np . ndarray : return self . _desc @descriptor . setter def descriptor ( self , desc : npt . ArrayLike ): self . _desc = np . array ( desc ) @property def portfolio_scores ( self ): return self . _pscores @portfolio_scores . setter def portfolio_scores ( self , p : npt . ArrayLike ): self . _pscores = np . asarray ( p ) def __repr__ ( self ): return f \"Instance<f= { self . fitness } ,p= { self . p } ,s= { self . s } ,vars= { len ( self . _vars ) } ,features= { len ( self . features ) } ,descriptor= { len ( self . descriptor ) } ,performance= { len ( self . portfolio_scores ) } >\" def __str__ ( self ): import reprlib descriptor = reprlib . repr ( self . descriptor ) performance = reprlib . repr ( self . portfolio_scores ) performance = performance [ performance . find ( \"(\" ) : performance . rfind ( \")\" ) + 1 ] return f \"Instance(f= { self . fitness } ,p= { self . p } ,s= { self . s } ,features= { len ( self . features ) } ,descriptor= { descriptor } ,performance= { performance } )\" def __iter__ ( self ): return iter ( self . _vars ) def __len__ ( self ): return len ( self . _vars ) def __getitem__ ( self , key ): if isinstance ( key , slice ): cls = type ( self ) # To facilitate subclassing return cls ( self . _vars [ key ]) index = operator . index ( key ) return self . _vars [ index ] def __setitem__ ( self , key , value ): self . _vars [ key ] = value def __eq__ ( self , other ): if not isinstance ( other , Instance ): print ( f \"Other of type { other . __class__ . __name__ } can not be compared with with { self . __class__ . __name__ } \" ) return NotImplemented else : try : return all ( a == b for a , b in zip ( self , other , strict = True )) except ValueError : return False def __gt__ ( self , other ): if not isinstance ( other , Instance ): print ( f \"Other of type { other . __class__ . __name__ } can not be compared with with { self . __class__ . __name__ } \" ) return NotImplemented return self . fitness > other . fitness def __ge__ ( self , other ): if not isinstance ( other , Instance ): print ( f \"Other of type { other . __class__ . __name__ } can not be compared with with { self . __class__ . __name__ } \" ) return NotImplemented return self . fitness >= other . fitness def __hash__ ( self ): from functools import reduce hashes = ( hash ( x ) for x in self ) return reduce ( operator . or_ , hashes , 0 ) def __bool__ ( self ): return self . _vars . size != 0 def __format__ ( self , fmt_spec = \"\" ): if fmt_spec . endswith ( \"p\" ): # We are showing only the performances fmt_spec = fmt_spec [: - 1 ] components = self . portfolio_scores else : fmt_spec = fmt_spec [: - 1 ] components = self . descriptor components = ( format ( c , fmt_spec ) for c in components ) decriptor = \"descriptor=( {} )\" . format ( \",\" . join ( components )) msg = f \"Instance(f= { self . fitness } ,p= { format ( self . p , fmt_spec ) } , s= { format ( self . s , fmt_spec ) } , { decriptor } )\" return msg def asdict ( self , only_genotype : bool = False , variables_names : Optional [ Sequence [ str ]] = None , features_names : Optional [ Sequence [ str ]] = None , score_names : Optional [ Sequence [ str ]] = None , ) -> dict : \"\"\"Convert the instance to a dictionary. The keys are the names of the attributes and the values are the values of the attributes. Args: only_genotype (bool, Default True): Whether to return the Instance as a dictionary containing only the variables. variables_names (Optional[Sequence[str]], optional): Names of the variables in the dictionary, otherwise v_i. Defaults to None. features_names (Optional[Sequence[str]], optional): Name of the features in the dictionary, otherwise f_i. Defaults to None. score_names (Optional[Sequence[str]], optional): Name of the solvers, otherwise solver_i. Defaults to None. Returns: dict: Dictionary with the attributes of the instance as keys and the values of the attributes as values. \"\"\" _data = {} if variables_names : if len ( variables_names ) != len ( self . _vars ): print ( f \"Error in asdict(). len(variables_names) = { len ( variables_names ) } != len(variables) ( { len ( self . _vars ) } ). Fallback to v#\" ) _data [ \"variables\" ] = { f \"v { i } \" : v for i , v in enumerate ( self . _vars )} else : _data [ \"variables\" ] = { vk : v for vk , v in zip ( variables_names , self . _vars ) } else : _data [ \"variables\" ] = { f \"v { i } \" : v for i , v in enumerate ( self . _vars )} if only_genotype : return _data else : sckeys = ( [ f \"solver_ { i } \" for i in range ( len ( self . _pscores ))] if score_names is None else score_names ) _data = { \"fitness\" : self . _fit , \"s\" : self . _s , \"p\" : self . _p , \"portfolio_scores\" : { sk : v for sk , v in zip ( sckeys , self . _pscores )}, ** _data , } if len ( self . _desc ) not in ( len ( self . _vars ), len ( self . _features ), len ( self . _pscores ), ): # Transformed descriptor _data [ \"descriptor\" ] = { f \"d { i } \" : v for i , v in enumerate ( self . _desc )} if len ( self . features ) != 0 : f_keys = ( [ f \"f { i } \" for i in range ( len ( self . _features ))] if features_names is None or len ( features_names ) == 0 else features_names ) _data [ \"features\" ] = { fk : v for fk , v in zip ( f_keys , self . _features )} return _data def to_json ( self ) -> str : \"\"\"Convert the instance to a JSON string. The keys are the names of the attributes and the values are the values of the attributes. Returns: str: JSON string with the attributes of the instance as keys and the values of the attributes as values. \"\"\" import json return json . dumps ( self . asdict (), sort_keys = True , indent = 4 ) def to_series ( self , only_genotype : bool = False , variables_names : Optional [ Sequence [ str ]] = None , features_names : Optional [ Sequence [ str ]] = None , score_names : Optional [ Sequence [ str ]] = None , ) -> pd . Series : \"\"\"Creates a pandas Series from the instance. Args: only_genotype (bool, Default True): Whether to return the Instance as a pd.Series containing only the variables. variables_names (Optional[Sequence[str]], optional): Names of the variables in the dictionary, otherwise v_i. Defaults to None. features_names (Optional[Sequence[str]], optional): Name of the features in the dictionary, otherwise f_i. Defaults to None. score_names (Optional[Sequence[str]], optional): Name of the solvers, otherwise solver_i. Defaults to None. Returns: pd.Series: Pandas Series with the attributes of the instance as keys and the values of the attributes as values. \"\"\" _flatten_data = {} for key , value in self . asdict ( only_genotype = only_genotype , variables_names = variables_names , features_names = features_names , score_names = score_names , ) . items (): if isinstance ( value , dict ): # Flatten nested dicts for sub_key , sub_value in value . items (): _flatten_data [ f \" { sub_key } \" ] = sub_value else : _flatten_data [ key ] = value return pd . Series ( _flatten_data ) __init__ ( variables = None , fitness = np . float64 ( 0.0 ), p = np . float64 ( 0.0 ), s = np . float64 ( 0.0 ), features = None , descriptor = None , portfolio_scores = None , otype = np . float64 , dtype = np . uint32 ) Creates an instance of a Instance (unstructured) for QD algorithms This class is used to represent a solution in a QD algorithm. It contains the variables, fitness, performance, novelty, features, descriptor and portfolio scores of the solution. The variables are stored as a numpy array, and the fitness, performance and novelty are stored as floats. The features, descriptor and portfolio scores are stored as numpy arrays. Parameters: variables ( Optional [ ArrayLike ] , default: None ) \u2013 Variables or genome of the instance. Defaults to None. fitness ( float , default: float64 (0.0) ) \u2013 Fitness of the instance. Defaults to 0.0. p ( float , default: float64 (0.0) ) \u2013 Performance score. Defaults to 0.0. s ( float , default: float64 (0.0) ) \u2013 Novelty score. Defaults to 0.0. features ( Optional [ tuple [ float ]] , default: None ) \u2013 Tuple of features extracted from the domain. Defaults to None. descriptor ( Optional [ tuple [ float ]] , default: None ) \u2013 Tuple with the descriptor information of the instance. Defaults to None. portfolio_scores ( Optional [ tuple [ float ]] , default: None ) \u2013 Scores of the solvers in the portfolio. Defaults to None. Raises: ValueError \u2013 If fitness, p or s are not convertible to float. Source code in digneapy/_core/_instance.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def __init__ ( self , variables : Optional [ npt . ArrayLike ] = None , fitness : np . float64 = np . float64 ( 0.0 ), p : np . float64 = np . float64 ( 0.0 ), s : np . float64 = np . float64 ( 0.0 ), features : Optional [ tuple [ np . float32 ]] = None , descriptor : Optional [ tuple [ np . float32 ]] = None , portfolio_scores : Optional [ tuple [ np . float64 ]] = None , otype = np . float64 , dtype = np . uint32 , ): \"\"\"Creates an instance of a Instance (unstructured) for QD algorithms This class is used to represent a solution in a QD algorithm. It contains the variables, fitness, performance, novelty, features, descriptor and portfolio scores of the solution. The variables are stored as a numpy array, and the fitness, performance and novelty are stored as floats. The features, descriptor and portfolio scores are stored as numpy arrays. Args: variables (Optional[npt.ArrayLike], optional): Variables or genome of the instance. Defaults to None. fitness (float, optional): Fitness of the instance. Defaults to 0.0. p (float, optional): Performance score. Defaults to 0.0. s (float, optional): Novelty score. Defaults to 0.0. features (Optional[tuple[float]], optional): Tuple of features extracted from the domain. Defaults to None. descriptor (Optional[tuple[float]], optional): Tuple with the descriptor information of the instance. Defaults to None. portfolio_scores (Optional[tuple[float]], optional): Scores of the solvers in the portfolio. Defaults to None. Raises: ValueError: If fitness, p or s are not convertible to float. \"\"\" self . _otype = otype self . _dtype = dtype try : fitness = self . _otype ( fitness ) p = self . _otype ( p ) s = self . _otype ( s ) except ValueError : raise ValueError ( \"The fitness, p and s parameters must be convertible to float\" ) self . _vars = ( np . array ( variables , dtype = self . _dtype ) if variables is not None else np . empty ( 0 , dtype = self . _dtype ) ) self . _fit = fitness self . _p = p self . _s = s self . _features = ( np . array ( features , dtype = np . float32 ) if features is not None else np . empty ( 0 , dtype = np . float32 ) ) self . _pscores = ( np . array ( portfolio_scores , dtype = self . _otype ) if portfolio_scores is not None else np . empty ( 0 , dtype = self . _otype ) ) self . _desc = ( np . array ( descriptor , dtype = np . float32 ) if descriptor is not None else np . empty ( 0 , dtype = np . float32 ) ) asdict ( only_genotype = False , variables_names = None , features_names = None , score_names = None ) Convert the instance to a dictionary. The keys are the names of the attributes and the values are the values of the attributes. Parameters: only_genotype ( bool, Default True , default: False ) \u2013 Whether to return the Instance as a dictionary containing only the variables. variables_names ( Optional [ Sequence [ str ]] , default: None ) \u2013 Names of the variables in the dictionary, otherwise v_i. Defaults to None. features_names ( Optional [ Sequence [ str ]] , default: None ) \u2013 Name of the features in the dictionary, otherwise f_i. Defaults to None. score_names ( Optional [ Sequence [ str ]] , default: None ) \u2013 Name of the solvers, otherwise solver_i. Defaults to None. Returns: dict ( dict ) \u2013 Dictionary with the attributes of the instance as keys and the values of the attributes as values. Source code in digneapy/_core/_instance.py 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 def asdict ( self , only_genotype : bool = False , variables_names : Optional [ Sequence [ str ]] = None , features_names : Optional [ Sequence [ str ]] = None , score_names : Optional [ Sequence [ str ]] = None , ) -> dict : \"\"\"Convert the instance to a dictionary. The keys are the names of the attributes and the values are the values of the attributes. Args: only_genotype (bool, Default True): Whether to return the Instance as a dictionary containing only the variables. variables_names (Optional[Sequence[str]], optional): Names of the variables in the dictionary, otherwise v_i. Defaults to None. features_names (Optional[Sequence[str]], optional): Name of the features in the dictionary, otherwise f_i. Defaults to None. score_names (Optional[Sequence[str]], optional): Name of the solvers, otherwise solver_i. Defaults to None. Returns: dict: Dictionary with the attributes of the instance as keys and the values of the attributes as values. \"\"\" _data = {} if variables_names : if len ( variables_names ) != len ( self . _vars ): print ( f \"Error in asdict(). len(variables_names) = { len ( variables_names ) } != len(variables) ( { len ( self . _vars ) } ). Fallback to v#\" ) _data [ \"variables\" ] = { f \"v { i } \" : v for i , v in enumerate ( self . _vars )} else : _data [ \"variables\" ] = { vk : v for vk , v in zip ( variables_names , self . _vars ) } else : _data [ \"variables\" ] = { f \"v { i } \" : v for i , v in enumerate ( self . _vars )} if only_genotype : return _data else : sckeys = ( [ f \"solver_ { i } \" for i in range ( len ( self . _pscores ))] if score_names is None else score_names ) _data = { \"fitness\" : self . _fit , \"s\" : self . _s , \"p\" : self . _p , \"portfolio_scores\" : { sk : v for sk , v in zip ( sckeys , self . _pscores )}, ** _data , } if len ( self . _desc ) not in ( len ( self . _vars ), len ( self . _features ), len ( self . _pscores ), ): # Transformed descriptor _data [ \"descriptor\" ] = { f \"d { i } \" : v for i , v in enumerate ( self . _desc )} if len ( self . features ) != 0 : f_keys = ( [ f \"f { i } \" for i in range ( len ( self . _features ))] if features_names is None or len ( features_names ) == 0 else features_names ) _data [ \"features\" ] = { fk : v for fk , v in zip ( f_keys , self . _features )} return _data clone () Create a clone of the current instance. More efficient than using copy.deepcopy. Returns: Self ( Self ) \u2013 Instance object Source code in digneapy/_core/_instance.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def clone ( self ) -> Self : \"\"\"Create a clone of the current instance. More efficient than using copy.deepcopy. Returns: Self: Instance object \"\"\" return Instance ( variables = list ( self . _vars ), fitness = self . _fit , p = self . _p , s = self . _s , features = tuple ( self . _features ), portfolio_scores = tuple ( self . _pscores ), descriptor = tuple ( self . _desc ), ) clone_with ( ** overrides ) Clones an Instance with overriden attributes Returns: \u2013 Instance Source code in digneapy/_core/_instance.py 126 127 128 129 130 131 132 133 134 135 def clone_with ( self , ** overrides ): \"\"\"Clones an Instance with overriden attributes Returns: Instance \"\"\" new_object = self . clone () for key , value in overrides . items (): setattr ( new_object , key , value ) return new_object to_json () Convert the instance to a JSON string. The keys are the names of the attributes and the values are the values of the attributes. Returns: str ( str ) \u2013 JSON string with the attributes of the instance as keys and the values of the attributes as values. Source code in digneapy/_core/_instance.py 366 367 368 369 370 371 372 373 374 375 def to_json ( self ) -> str : \"\"\"Convert the instance to a JSON string. The keys are the names of the attributes and the values are the values of the attributes. Returns: str: JSON string with the attributes of the instance as keys and the values of the attributes as values. \"\"\" import json return json . dumps ( self . asdict (), sort_keys = True , indent = 4 ) to_series ( only_genotype = False , variables_names = None , features_names = None , score_names = None ) Creates a pandas Series from the instance. Parameters: only_genotype ( bool, Default True , default: False ) \u2013 Whether to return the Instance as a pd.Series containing only the variables. variables_names ( Optional [ Sequence [ str ]] , default: None ) \u2013 Names of the variables in the dictionary, otherwise v_i. Defaults to None. features_names ( Optional [ Sequence [ str ]] , default: None ) \u2013 Name of the features in the dictionary, otherwise f_i. Defaults to None. score_names ( Optional [ Sequence [ str ]] , default: None ) \u2013 Name of the solvers, otherwise solver_i. Defaults to None. Returns: Series \u2013 pd.Series: Pandas Series with the attributes of the instance as keys and the values of the attributes as values. Source code in digneapy/_core/_instance.py 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 def to_series ( self , only_genotype : bool = False , variables_names : Optional [ Sequence [ str ]] = None , features_names : Optional [ Sequence [ str ]] = None , score_names : Optional [ Sequence [ str ]] = None , ) -> pd . Series : \"\"\"Creates a pandas Series from the instance. Args: only_genotype (bool, Default True): Whether to return the Instance as a pd.Series containing only the variables. variables_names (Optional[Sequence[str]], optional): Names of the variables in the dictionary, otherwise v_i. Defaults to None. features_names (Optional[Sequence[str]], optional): Name of the features in the dictionary, otherwise f_i. Defaults to None. score_names (Optional[Sequence[str]], optional): Name of the solvers, otherwise solver_i. Defaults to None. Returns: pd.Series: Pandas Series with the attributes of the instance as keys and the values of the attributes as values. \"\"\" _flatten_data = {} for key , value in self . asdict ( only_genotype = only_genotype , variables_names = variables_names , features_names = features_names , score_names = score_names , ) . items (): if isinstance ( value , dict ): # Flatten nested dicts for sub_key , sub_value in value . items (): _flatten_data [ f \" { sub_key } \" ] = sub_value else : _flatten_data [ key ] = value return pd . Series ( _flatten_data )","title":" instance"},{"location":"reference/_core/_instance/#_core._instance.Instance","text":"Source code in digneapy/_core/_instance.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 class Instance : __slots__ = ( \"_vars\" , \"_fit\" , \"_p\" , \"_s\" , \"_features\" , \"_desc\" , \"_pscores\" , \"_otype\" , \"_dtype\" , ) def __init__ ( self , variables : Optional [ npt . ArrayLike ] = None , fitness : np . float64 = np . float64 ( 0.0 ), p : np . float64 = np . float64 ( 0.0 ), s : np . float64 = np . float64 ( 0.0 ), features : Optional [ tuple [ np . float32 ]] = None , descriptor : Optional [ tuple [ np . float32 ]] = None , portfolio_scores : Optional [ tuple [ np . float64 ]] = None , otype = np . float64 , dtype = np . uint32 , ): \"\"\"Creates an instance of a Instance (unstructured) for QD algorithms This class is used to represent a solution in a QD algorithm. It contains the variables, fitness, performance, novelty, features, descriptor and portfolio scores of the solution. The variables are stored as a numpy array, and the fitness, performance and novelty are stored as floats. The features, descriptor and portfolio scores are stored as numpy arrays. Args: variables (Optional[npt.ArrayLike], optional): Variables or genome of the instance. Defaults to None. fitness (float, optional): Fitness of the instance. Defaults to 0.0. p (float, optional): Performance score. Defaults to 0.0. s (float, optional): Novelty score. Defaults to 0.0. features (Optional[tuple[float]], optional): Tuple of features extracted from the domain. Defaults to None. descriptor (Optional[tuple[float]], optional): Tuple with the descriptor information of the instance. Defaults to None. portfolio_scores (Optional[tuple[float]], optional): Scores of the solvers in the portfolio. Defaults to None. Raises: ValueError: If fitness, p or s are not convertible to float. \"\"\" self . _otype = otype self . _dtype = dtype try : fitness = self . _otype ( fitness ) p = self . _otype ( p ) s = self . _otype ( s ) except ValueError : raise ValueError ( \"The fitness, p and s parameters must be convertible to float\" ) self . _vars = ( np . array ( variables , dtype = self . _dtype ) if variables is not None else np . empty ( 0 , dtype = self . _dtype ) ) self . _fit = fitness self . _p = p self . _s = s self . _features = ( np . array ( features , dtype = np . float32 ) if features is not None else np . empty ( 0 , dtype = np . float32 ) ) self . _pscores = ( np . array ( portfolio_scores , dtype = self . _otype ) if portfolio_scores is not None else np . empty ( 0 , dtype = self . _otype ) ) self . _desc = ( np . array ( descriptor , dtype = np . float32 ) if descriptor is not None else np . empty ( 0 , dtype = np . float32 ) ) @property def dtype ( self ): return self . _dtype @property def otype ( self ): return self . _otype def clone ( self ) -> Self : \"\"\"Create a clone of the current instance. More efficient than using copy.deepcopy. Returns: Self: Instance object \"\"\" return Instance ( variables = list ( self . _vars ), fitness = self . _fit , p = self . _p , s = self . _s , features = tuple ( self . _features ), portfolio_scores = tuple ( self . _pscores ), descriptor = tuple ( self . _desc ), ) def clone_with ( self , ** overrides ): \"\"\"Clones an Instance with overriden attributes Returns: Instance \"\"\" new_object = self . clone () for key , value in overrides . items (): setattr ( new_object , key , value ) return new_object @property def variables ( self ): return self . _vars @variables . setter def variables ( self , new_variables : npt . ArrayLike ): if len ( new_variables ) != len ( self . _vars ): raise ValueError ( \"Updating the variables of an Instance object with a different number of values.\" f \"Instance have { len ( self . _vars ) } \" f \"variables and the new_variables sequence have { len ( new_variables ) } \" ) self . _vars = np . asarray ( new_variables ) @property def p ( self ) -> np . float64 : return self . _p @p . setter def p ( self , performance : np . float64 ): try : performance = np . float64 ( performance ) except ValueError : # if performance != 0.0 and not float(performance): msg = f \"The performance value { performance } is not a float in 'p' setter of class { self . __class__ . __name__ } \" raise ValueError ( msg ) self . _p = performance @property def s ( self ) -> np . float64 : return self . _s @s . setter def s ( self , novelty : np . float64 ): try : novelty = np . float64 ( novelty ) except ValueError : # if novelty != 0.0 and not float(novelty): msg = f \"The novelty value { novelty } is not a float in 's' setter of class { self . __class__ . __name__ } \" raise ValueError ( msg ) self . _s = novelty @property def fitness ( self ) -> np . float64 : return self . _fit @fitness . setter def fitness ( self , f : np . float64 ): try : f = np . float64 ( f ) except ValueError : # if f != 0.0 and not float(f): msg = f \"The fitness value { f } is not a float in fitness setter of class { self . __class__ . __name__ } \" raise ValueError ( msg ) self . _fit = f @property def features ( self ) -> np . ndarray : return self . _features @features . setter def features ( self , features : npt . ArrayLike ): self . _features = np . asarray ( features ) @property def descriptor ( self ) -> np . ndarray : return self . _desc @descriptor . setter def descriptor ( self , desc : npt . ArrayLike ): self . _desc = np . array ( desc ) @property def portfolio_scores ( self ): return self . _pscores @portfolio_scores . setter def portfolio_scores ( self , p : npt . ArrayLike ): self . _pscores = np . asarray ( p ) def __repr__ ( self ): return f \"Instance<f= { self . fitness } ,p= { self . p } ,s= { self . s } ,vars= { len ( self . _vars ) } ,features= { len ( self . features ) } ,descriptor= { len ( self . descriptor ) } ,performance= { len ( self . portfolio_scores ) } >\" def __str__ ( self ): import reprlib descriptor = reprlib . repr ( self . descriptor ) performance = reprlib . repr ( self . portfolio_scores ) performance = performance [ performance . find ( \"(\" ) : performance . rfind ( \")\" ) + 1 ] return f \"Instance(f= { self . fitness } ,p= { self . p } ,s= { self . s } ,features= { len ( self . features ) } ,descriptor= { descriptor } ,performance= { performance } )\" def __iter__ ( self ): return iter ( self . _vars ) def __len__ ( self ): return len ( self . _vars ) def __getitem__ ( self , key ): if isinstance ( key , slice ): cls = type ( self ) # To facilitate subclassing return cls ( self . _vars [ key ]) index = operator . index ( key ) return self . _vars [ index ] def __setitem__ ( self , key , value ): self . _vars [ key ] = value def __eq__ ( self , other ): if not isinstance ( other , Instance ): print ( f \"Other of type { other . __class__ . __name__ } can not be compared with with { self . __class__ . __name__ } \" ) return NotImplemented else : try : return all ( a == b for a , b in zip ( self , other , strict = True )) except ValueError : return False def __gt__ ( self , other ): if not isinstance ( other , Instance ): print ( f \"Other of type { other . __class__ . __name__ } can not be compared with with { self . __class__ . __name__ } \" ) return NotImplemented return self . fitness > other . fitness def __ge__ ( self , other ): if not isinstance ( other , Instance ): print ( f \"Other of type { other . __class__ . __name__ } can not be compared with with { self . __class__ . __name__ } \" ) return NotImplemented return self . fitness >= other . fitness def __hash__ ( self ): from functools import reduce hashes = ( hash ( x ) for x in self ) return reduce ( operator . or_ , hashes , 0 ) def __bool__ ( self ): return self . _vars . size != 0 def __format__ ( self , fmt_spec = \"\" ): if fmt_spec . endswith ( \"p\" ): # We are showing only the performances fmt_spec = fmt_spec [: - 1 ] components = self . portfolio_scores else : fmt_spec = fmt_spec [: - 1 ] components = self . descriptor components = ( format ( c , fmt_spec ) for c in components ) decriptor = \"descriptor=( {} )\" . format ( \",\" . join ( components )) msg = f \"Instance(f= { self . fitness } ,p= { format ( self . p , fmt_spec ) } , s= { format ( self . s , fmt_spec ) } , { decriptor } )\" return msg def asdict ( self , only_genotype : bool = False , variables_names : Optional [ Sequence [ str ]] = None , features_names : Optional [ Sequence [ str ]] = None , score_names : Optional [ Sequence [ str ]] = None , ) -> dict : \"\"\"Convert the instance to a dictionary. The keys are the names of the attributes and the values are the values of the attributes. Args: only_genotype (bool, Default True): Whether to return the Instance as a dictionary containing only the variables. variables_names (Optional[Sequence[str]], optional): Names of the variables in the dictionary, otherwise v_i. Defaults to None. features_names (Optional[Sequence[str]], optional): Name of the features in the dictionary, otherwise f_i. Defaults to None. score_names (Optional[Sequence[str]], optional): Name of the solvers, otherwise solver_i. Defaults to None. Returns: dict: Dictionary with the attributes of the instance as keys and the values of the attributes as values. \"\"\" _data = {} if variables_names : if len ( variables_names ) != len ( self . _vars ): print ( f \"Error in asdict(). len(variables_names) = { len ( variables_names ) } != len(variables) ( { len ( self . _vars ) } ). Fallback to v#\" ) _data [ \"variables\" ] = { f \"v { i } \" : v for i , v in enumerate ( self . _vars )} else : _data [ \"variables\" ] = { vk : v for vk , v in zip ( variables_names , self . _vars ) } else : _data [ \"variables\" ] = { f \"v { i } \" : v for i , v in enumerate ( self . _vars )} if only_genotype : return _data else : sckeys = ( [ f \"solver_ { i } \" for i in range ( len ( self . _pscores ))] if score_names is None else score_names ) _data = { \"fitness\" : self . _fit , \"s\" : self . _s , \"p\" : self . _p , \"portfolio_scores\" : { sk : v for sk , v in zip ( sckeys , self . _pscores )}, ** _data , } if len ( self . _desc ) not in ( len ( self . _vars ), len ( self . _features ), len ( self . _pscores ), ): # Transformed descriptor _data [ \"descriptor\" ] = { f \"d { i } \" : v for i , v in enumerate ( self . _desc )} if len ( self . features ) != 0 : f_keys = ( [ f \"f { i } \" for i in range ( len ( self . _features ))] if features_names is None or len ( features_names ) == 0 else features_names ) _data [ \"features\" ] = { fk : v for fk , v in zip ( f_keys , self . _features )} return _data def to_json ( self ) -> str : \"\"\"Convert the instance to a JSON string. The keys are the names of the attributes and the values are the values of the attributes. Returns: str: JSON string with the attributes of the instance as keys and the values of the attributes as values. \"\"\" import json return json . dumps ( self . asdict (), sort_keys = True , indent = 4 ) def to_series ( self , only_genotype : bool = False , variables_names : Optional [ Sequence [ str ]] = None , features_names : Optional [ Sequence [ str ]] = None , score_names : Optional [ Sequence [ str ]] = None , ) -> pd . Series : \"\"\"Creates a pandas Series from the instance. Args: only_genotype (bool, Default True): Whether to return the Instance as a pd.Series containing only the variables. variables_names (Optional[Sequence[str]], optional): Names of the variables in the dictionary, otherwise v_i. Defaults to None. features_names (Optional[Sequence[str]], optional): Name of the features in the dictionary, otherwise f_i. Defaults to None. score_names (Optional[Sequence[str]], optional): Name of the solvers, otherwise solver_i. Defaults to None. Returns: pd.Series: Pandas Series with the attributes of the instance as keys and the values of the attributes as values. \"\"\" _flatten_data = {} for key , value in self . asdict ( only_genotype = only_genotype , variables_names = variables_names , features_names = features_names , score_names = score_names , ) . items (): if isinstance ( value , dict ): # Flatten nested dicts for sub_key , sub_value in value . items (): _flatten_data [ f \" { sub_key } \" ] = sub_value else : _flatten_data [ key ] = value return pd . Series ( _flatten_data )","title":"Instance"},{"location":"reference/_core/_instance/#_core._instance.Instance.__init__","text":"Creates an instance of a Instance (unstructured) for QD algorithms This class is used to represent a solution in a QD algorithm. It contains the variables, fitness, performance, novelty, features, descriptor and portfolio scores of the solution. The variables are stored as a numpy array, and the fitness, performance and novelty are stored as floats. The features, descriptor and portfolio scores are stored as numpy arrays. Parameters: variables ( Optional [ ArrayLike ] , default: None ) \u2013 Variables or genome of the instance. Defaults to None. fitness ( float , default: float64 (0.0) ) \u2013 Fitness of the instance. Defaults to 0.0. p ( float , default: float64 (0.0) ) \u2013 Performance score. Defaults to 0.0. s ( float , default: float64 (0.0) ) \u2013 Novelty score. Defaults to 0.0. features ( Optional [ tuple [ float ]] , default: None ) \u2013 Tuple of features extracted from the domain. Defaults to None. descriptor ( Optional [ tuple [ float ]] , default: None ) \u2013 Tuple with the descriptor information of the instance. Defaults to None. portfolio_scores ( Optional [ tuple [ float ]] , default: None ) \u2013 Scores of the solvers in the portfolio. Defaults to None. Raises: ValueError \u2013 If fitness, p or s are not convertible to float. Source code in digneapy/_core/_instance.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def __init__ ( self , variables : Optional [ npt . ArrayLike ] = None , fitness : np . float64 = np . float64 ( 0.0 ), p : np . float64 = np . float64 ( 0.0 ), s : np . float64 = np . float64 ( 0.0 ), features : Optional [ tuple [ np . float32 ]] = None , descriptor : Optional [ tuple [ np . float32 ]] = None , portfolio_scores : Optional [ tuple [ np . float64 ]] = None , otype = np . float64 , dtype = np . uint32 , ): \"\"\"Creates an instance of a Instance (unstructured) for QD algorithms This class is used to represent a solution in a QD algorithm. It contains the variables, fitness, performance, novelty, features, descriptor and portfolio scores of the solution. The variables are stored as a numpy array, and the fitness, performance and novelty are stored as floats. The features, descriptor and portfolio scores are stored as numpy arrays. Args: variables (Optional[npt.ArrayLike], optional): Variables or genome of the instance. Defaults to None. fitness (float, optional): Fitness of the instance. Defaults to 0.0. p (float, optional): Performance score. Defaults to 0.0. s (float, optional): Novelty score. Defaults to 0.0. features (Optional[tuple[float]], optional): Tuple of features extracted from the domain. Defaults to None. descriptor (Optional[tuple[float]], optional): Tuple with the descriptor information of the instance. Defaults to None. portfolio_scores (Optional[tuple[float]], optional): Scores of the solvers in the portfolio. Defaults to None. Raises: ValueError: If fitness, p or s are not convertible to float. \"\"\" self . _otype = otype self . _dtype = dtype try : fitness = self . _otype ( fitness ) p = self . _otype ( p ) s = self . _otype ( s ) except ValueError : raise ValueError ( \"The fitness, p and s parameters must be convertible to float\" ) self . _vars = ( np . array ( variables , dtype = self . _dtype ) if variables is not None else np . empty ( 0 , dtype = self . _dtype ) ) self . _fit = fitness self . _p = p self . _s = s self . _features = ( np . array ( features , dtype = np . float32 ) if features is not None else np . empty ( 0 , dtype = np . float32 ) ) self . _pscores = ( np . array ( portfolio_scores , dtype = self . _otype ) if portfolio_scores is not None else np . empty ( 0 , dtype = self . _otype ) ) self . _desc = ( np . array ( descriptor , dtype = np . float32 ) if descriptor is not None else np . empty ( 0 , dtype = np . float32 ) )","title":"__init__"},{"location":"reference/_core/_instance/#_core._instance.Instance.asdict","text":"Convert the instance to a dictionary. The keys are the names of the attributes and the values are the values of the attributes. Parameters: only_genotype ( bool, Default True , default: False ) \u2013 Whether to return the Instance as a dictionary containing only the variables. variables_names ( Optional [ Sequence [ str ]] , default: None ) \u2013 Names of the variables in the dictionary, otherwise v_i. Defaults to None. features_names ( Optional [ Sequence [ str ]] , default: None ) \u2013 Name of the features in the dictionary, otherwise f_i. Defaults to None. score_names ( Optional [ Sequence [ str ]] , default: None ) \u2013 Name of the solvers, otherwise solver_i. Defaults to None. Returns: dict ( dict ) \u2013 Dictionary with the attributes of the instance as keys and the values of the attributes as values. Source code in digneapy/_core/_instance.py 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 def asdict ( self , only_genotype : bool = False , variables_names : Optional [ Sequence [ str ]] = None , features_names : Optional [ Sequence [ str ]] = None , score_names : Optional [ Sequence [ str ]] = None , ) -> dict : \"\"\"Convert the instance to a dictionary. The keys are the names of the attributes and the values are the values of the attributes. Args: only_genotype (bool, Default True): Whether to return the Instance as a dictionary containing only the variables. variables_names (Optional[Sequence[str]], optional): Names of the variables in the dictionary, otherwise v_i. Defaults to None. features_names (Optional[Sequence[str]], optional): Name of the features in the dictionary, otherwise f_i. Defaults to None. score_names (Optional[Sequence[str]], optional): Name of the solvers, otherwise solver_i. Defaults to None. Returns: dict: Dictionary with the attributes of the instance as keys and the values of the attributes as values. \"\"\" _data = {} if variables_names : if len ( variables_names ) != len ( self . _vars ): print ( f \"Error in asdict(). len(variables_names) = { len ( variables_names ) } != len(variables) ( { len ( self . _vars ) } ). Fallback to v#\" ) _data [ \"variables\" ] = { f \"v { i } \" : v for i , v in enumerate ( self . _vars )} else : _data [ \"variables\" ] = { vk : v for vk , v in zip ( variables_names , self . _vars ) } else : _data [ \"variables\" ] = { f \"v { i } \" : v for i , v in enumerate ( self . _vars )} if only_genotype : return _data else : sckeys = ( [ f \"solver_ { i } \" for i in range ( len ( self . _pscores ))] if score_names is None else score_names ) _data = { \"fitness\" : self . _fit , \"s\" : self . _s , \"p\" : self . _p , \"portfolio_scores\" : { sk : v for sk , v in zip ( sckeys , self . _pscores )}, ** _data , } if len ( self . _desc ) not in ( len ( self . _vars ), len ( self . _features ), len ( self . _pscores ), ): # Transformed descriptor _data [ \"descriptor\" ] = { f \"d { i } \" : v for i , v in enumerate ( self . _desc )} if len ( self . features ) != 0 : f_keys = ( [ f \"f { i } \" for i in range ( len ( self . _features ))] if features_names is None or len ( features_names ) == 0 else features_names ) _data [ \"features\" ] = { fk : v for fk , v in zip ( f_keys , self . _features )} return _data","title":"asdict"},{"location":"reference/_core/_instance/#_core._instance.Instance.clone","text":"Create a clone of the current instance. More efficient than using copy.deepcopy. Returns: Self ( Self ) \u2013 Instance object Source code in digneapy/_core/_instance.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def clone ( self ) -> Self : \"\"\"Create a clone of the current instance. More efficient than using copy.deepcopy. Returns: Self: Instance object \"\"\" return Instance ( variables = list ( self . _vars ), fitness = self . _fit , p = self . _p , s = self . _s , features = tuple ( self . _features ), portfolio_scores = tuple ( self . _pscores ), descriptor = tuple ( self . _desc ), )","title":"clone"},{"location":"reference/_core/_instance/#_core._instance.Instance.clone_with","text":"Clones an Instance with overriden attributes Returns: \u2013 Instance Source code in digneapy/_core/_instance.py 126 127 128 129 130 131 132 133 134 135 def clone_with ( self , ** overrides ): \"\"\"Clones an Instance with overriden attributes Returns: Instance \"\"\" new_object = self . clone () for key , value in overrides . items (): setattr ( new_object , key , value ) return new_object","title":"clone_with"},{"location":"reference/_core/_instance/#_core._instance.Instance.to_json","text":"Convert the instance to a JSON string. The keys are the names of the attributes and the values are the values of the attributes. Returns: str ( str ) \u2013 JSON string with the attributes of the instance as keys and the values of the attributes as values. Source code in digneapy/_core/_instance.py 366 367 368 369 370 371 372 373 374 375 def to_json ( self ) -> str : \"\"\"Convert the instance to a JSON string. The keys are the names of the attributes and the values are the values of the attributes. Returns: str: JSON string with the attributes of the instance as keys and the values of the attributes as values. \"\"\" import json return json . dumps ( self . asdict (), sort_keys = True , indent = 4 )","title":"to_json"},{"location":"reference/_core/_instance/#_core._instance.Instance.to_series","text":"Creates a pandas Series from the instance. Parameters: only_genotype ( bool, Default True , default: False ) \u2013 Whether to return the Instance as a pd.Series containing only the variables. variables_names ( Optional [ Sequence [ str ]] , default: None ) \u2013 Names of the variables in the dictionary, otherwise v_i. Defaults to None. features_names ( Optional [ Sequence [ str ]] , default: None ) \u2013 Name of the features in the dictionary, otherwise f_i. Defaults to None. score_names ( Optional [ Sequence [ str ]] , default: None ) \u2013 Name of the solvers, otherwise solver_i. Defaults to None. Returns: Series \u2013 pd.Series: Pandas Series with the attributes of the instance as keys and the values of the attributes as values. Source code in digneapy/_core/_instance.py 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 def to_series ( self , only_genotype : bool = False , variables_names : Optional [ Sequence [ str ]] = None , features_names : Optional [ Sequence [ str ]] = None , score_names : Optional [ Sequence [ str ]] = None , ) -> pd . Series : \"\"\"Creates a pandas Series from the instance. Args: only_genotype (bool, Default True): Whether to return the Instance as a pd.Series containing only the variables. variables_names (Optional[Sequence[str]], optional): Names of the variables in the dictionary, otherwise v_i. Defaults to None. features_names (Optional[Sequence[str]], optional): Name of the features in the dictionary, otherwise f_i. Defaults to None. score_names (Optional[Sequence[str]], optional): Name of the solvers, otherwise solver_i. Defaults to None. Returns: pd.Series: Pandas Series with the attributes of the instance as keys and the values of the attributes as values. \"\"\" _flatten_data = {} for key , value in self . asdict ( only_genotype = only_genotype , variables_names = variables_names , features_names = features_names , score_names = score_names , ) . items (): if isinstance ( value , dict ): # Flatten nested dicts for sub_key , sub_value in value . items (): _flatten_data [ f \" { sub_key } \" ] = sub_value else : _flatten_data [ key ] = value return pd . Series ( _flatten_data )","title":"to_series"},{"location":"reference/_core/_metrics/","text":"@File : _metrics.py @Time : 2025/02/05 14:40:24 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2025, Alejandro Marrero @Desc : None Statistics Source code in digneapy/_core/_metrics.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 class Statistics : def __init__ ( self ): self . _stats_s = tools . Statistics ( key = attrgetter ( \"s\" )) self . _stats_p = tools . Statistics ( key = attrgetter ( \"p\" )) self . _stats_f = tools . Statistics ( key = attrgetter ( \"fitness\" )) self . _stats = tools . MultiStatistics ( s = self . _stats_s , p = self . _stats_p , fitness = self . _stats_f ) self . _stats . register ( \"mean\" , np . mean ) self . _stats . register ( \"std\" , np . std ) self . _stats . register ( \"min\" , np . min ) self . _stats . register ( \"max\" , np . max ) self . _stats . register ( \"qd_score\" , np . sum ) def __call__ ( self , population : Sequence [ Instance ], as_series : bool = False ) -> dict | pd . Series : \"\"\"Calculates the statistics of the population. Args: population (Sequence[Instance]): List of instances to calculate the statistics. Returns: dict: Dictionary with the statistics of the population. \"\"\" if len ( population ) == 0 : raise ValueError ( \"Error: Trying to calculate the metrics with an empty population\" ) if not all ( isinstance ( ind , Instance ) for ind in population ): raise TypeError ( \"Error: Population must be a sequence of Instance objects\" ) record = self . _stats . compile ( population ) if as_series : _flatten_record = {} for key , value in record . items (): if isinstance ( value , dict ): # Flatten nested dicts for sub_key , sub_value in value . items (): _flatten_record [ f \" { key } _ { sub_key } \" ] = sub_value else : _flatten_record [ key ] = value return pd . Series ( _flatten_record ) else : return record __call__ ( population , as_series = False ) Calculates the statistics of the population. Args: population (Sequence[Instance]): List of instances to calculate the statistics. Returns: dict: Dictionary with the statistics of the population. Source code in digneapy/_core/_metrics.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def __call__ ( self , population : Sequence [ Instance ], as_series : bool = False ) -> dict | pd . Series : \"\"\"Calculates the statistics of the population. Args: population (Sequence[Instance]): List of instances to calculate the statistics. Returns: dict: Dictionary with the statistics of the population. \"\"\" if len ( population ) == 0 : raise ValueError ( \"Error: Trying to calculate the metrics with an empty population\" ) if not all ( isinstance ( ind , Instance ) for ind in population ): raise TypeError ( \"Error: Population must be a sequence of Instance objects\" ) record = self . _stats . compile ( population ) if as_series : _flatten_record = {} for key , value in record . items (): if isinstance ( value , dict ): # Flatten nested dicts for sub_key , sub_value in value . items (): _flatten_record [ f \" { key } _ { sub_key } \" ] = sub_value else : _flatten_record [ key ] = value return pd . Series ( _flatten_record ) else : return record qd_score ( instances_fitness ) Calculates the Quality Diversity score of a set of instances fitness. Parameters: instances ( Sequence [ float ] ) \u2013 List with the fitness of several instances to calculate the QD score. Returns: float ( float64 ) \u2013 Sum of the fitness of all instances. Source code in digneapy/_core/_metrics.py 23 24 25 26 27 28 29 30 31 32 def qd_score ( instances_fitness : np . ndarray ) -> np . float64 : \"\"\"Calculates the Quality Diversity score of a set of instances fitness. Args: instances (Sequence[float]): List with the fitness of several instances to calculate the QD score. Returns: float: Sum of the fitness of all instances. \"\"\" return np . sum ( instances_fitness ) qd_score_auc ( qd_scores , batch_size ) Calculates the Quantifying Efficiency in Quality Diversity Optimization In quality diversity (QD) optimization, the QD score is a holistic metric which sums the objective values of all cells in the archive. Since the QD score only measures the performance of a QD algorithm at a single point in time, it fails to reflect algorithm efficiency. Two algorithms may have the same QD score even though one algorithm achieved that score with fewer evaluations. We propose a metric called \u201cQD score AUC\u201d which quantifies this efficiency. Parameters: qd_scores ( Sequence [ float ] ) \u2013 Sequence of QD scores. batch_size ( int ) \u2013 Number of instances evaluated in each generation. Returns: float64 \u2013 np.float64: QD score AUC metric. Source code in digneapy/_core/_metrics.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def qd_score_auc ( qd_scores : np . ndarray , batch_size : int ) -> np . float64 : \"\"\"Calculates the Quantifying Efficiency in Quality Diversity Optimization In quality diversity (QD) optimization, the QD score is a holistic metric which sums the objective values of all cells in the archive. Since the QD score only measures the performance of a QD algorithm at a single point in time, it fails to reflect algorithm efficiency. Two algorithms may have the same QD score even though one algorithm achieved that score with fewer evaluations. We propose a metric called \u201cQD score AUC\u201d which quantifies this efficiency. Args: qd_scores (Sequence[float]): Sequence of QD scores. batch_size (int): Number of instances evaluated in each generation. Returns: np.float64: QD score AUC metric. \"\"\" return np . sum ( qd_scores ) * batch_size","title":" metrics"},{"location":"reference/_core/_metrics/#_core._metrics.Statistics","text":"Source code in digneapy/_core/_metrics.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 class Statistics : def __init__ ( self ): self . _stats_s = tools . Statistics ( key = attrgetter ( \"s\" )) self . _stats_p = tools . Statistics ( key = attrgetter ( \"p\" )) self . _stats_f = tools . Statistics ( key = attrgetter ( \"fitness\" )) self . _stats = tools . MultiStatistics ( s = self . _stats_s , p = self . _stats_p , fitness = self . _stats_f ) self . _stats . register ( \"mean\" , np . mean ) self . _stats . register ( \"std\" , np . std ) self . _stats . register ( \"min\" , np . min ) self . _stats . register ( \"max\" , np . max ) self . _stats . register ( \"qd_score\" , np . sum ) def __call__ ( self , population : Sequence [ Instance ], as_series : bool = False ) -> dict | pd . Series : \"\"\"Calculates the statistics of the population. Args: population (Sequence[Instance]): List of instances to calculate the statistics. Returns: dict: Dictionary with the statistics of the population. \"\"\" if len ( population ) == 0 : raise ValueError ( \"Error: Trying to calculate the metrics with an empty population\" ) if not all ( isinstance ( ind , Instance ) for ind in population ): raise TypeError ( \"Error: Population must be a sequence of Instance objects\" ) record = self . _stats . compile ( population ) if as_series : _flatten_record = {} for key , value in record . items (): if isinstance ( value , dict ): # Flatten nested dicts for sub_key , sub_value in value . items (): _flatten_record [ f \" { key } _ { sub_key } \" ] = sub_value else : _flatten_record [ key ] = value return pd . Series ( _flatten_record ) else : return record","title":"Statistics"},{"location":"reference/_core/_metrics/#_core._metrics.Statistics.__call__","text":"Calculates the statistics of the population. Args: population (Sequence[Instance]): List of instances to calculate the statistics. Returns: dict: Dictionary with the statistics of the population. Source code in digneapy/_core/_metrics.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def __call__ ( self , population : Sequence [ Instance ], as_series : bool = False ) -> dict | pd . Series : \"\"\"Calculates the statistics of the population. Args: population (Sequence[Instance]): List of instances to calculate the statistics. Returns: dict: Dictionary with the statistics of the population. \"\"\" if len ( population ) == 0 : raise ValueError ( \"Error: Trying to calculate the metrics with an empty population\" ) if not all ( isinstance ( ind , Instance ) for ind in population ): raise TypeError ( \"Error: Population must be a sequence of Instance objects\" ) record = self . _stats . compile ( population ) if as_series : _flatten_record = {} for key , value in record . items (): if isinstance ( value , dict ): # Flatten nested dicts for sub_key , sub_value in value . items (): _flatten_record [ f \" { key } _ { sub_key } \" ] = sub_value else : _flatten_record [ key ] = value return pd . Series ( _flatten_record ) else : return record","title":"__call__"},{"location":"reference/_core/_metrics/#_core._metrics.qd_score","text":"Calculates the Quality Diversity score of a set of instances fitness. Parameters: instances ( Sequence [ float ] ) \u2013 List with the fitness of several instances to calculate the QD score. Returns: float ( float64 ) \u2013 Sum of the fitness of all instances. Source code in digneapy/_core/_metrics.py 23 24 25 26 27 28 29 30 31 32 def qd_score ( instances_fitness : np . ndarray ) -> np . float64 : \"\"\"Calculates the Quality Diversity score of a set of instances fitness. Args: instances (Sequence[float]): List with the fitness of several instances to calculate the QD score. Returns: float: Sum of the fitness of all instances. \"\"\" return np . sum ( instances_fitness )","title":"qd_score"},{"location":"reference/_core/_metrics/#_core._metrics.qd_score_auc","text":"Calculates the Quantifying Efficiency in Quality Diversity Optimization In quality diversity (QD) optimization, the QD score is a holistic metric which sums the objective values of all cells in the archive. Since the QD score only measures the performance of a QD algorithm at a single point in time, it fails to reflect algorithm efficiency. Two algorithms may have the same QD score even though one algorithm achieved that score with fewer evaluations. We propose a metric called \u201cQD score AUC\u201d which quantifies this efficiency. Parameters: qd_scores ( Sequence [ float ] ) \u2013 Sequence of QD scores. batch_size ( int ) \u2013 Number of instances evaluated in each generation. Returns: float64 \u2013 np.float64: QD score AUC metric. Source code in digneapy/_core/_metrics.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def qd_score_auc ( qd_scores : np . ndarray , batch_size : int ) -> np . float64 : \"\"\"Calculates the Quantifying Efficiency in Quality Diversity Optimization In quality diversity (QD) optimization, the QD score is a holistic metric which sums the objective values of all cells in the archive. Since the QD score only measures the performance of a QD algorithm at a single point in time, it fails to reflect algorithm efficiency. Two algorithms may have the same QD score even though one algorithm achieved that score with fewer evaluations. We propose a metric called \u201cQD score AUC\u201d which quantifies this efficiency. Args: qd_scores (Sequence[float]): Sequence of QD scores. batch_size (int): Number of instances evaluated in each generation. Returns: np.float64: QD score AUC metric. \"\"\" return np . sum ( qd_scores ) * batch_size","title":"qd_score_auc"},{"location":"reference/_core/_novelty_search/","text":"@File : novelty_search.py @Time : 2023/10/24 11:23:08 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2023, Alejandro Marrero @Desc : None NS Source code in digneapy/_core/_novelty_search.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 class NS : def __init__ ( self , archive : Optional [ Archive ] = None , k : int = 15 , ): \"\"\"Creates an instance of the Novelty Search Algorithm Args: archive (Archive): Archive to store the instances to guide the evolution. Defaults to Archive(threshold=0.001). k (int, optional): Number of neighbours to calculate the sparseness. Defaults to 15. \"\"\" if k < 0 : raise ValueError ( f \" { __name__ } k must be a positive integer and less than the number of instances.\" ) if archive is not None and not isinstance ( archive , Archive ): raise ValueError ( \"You must provide a valid Archive object\" ) self . _k = k self . _archive = archive if archive is not None else Archive ( threshold = 0.001 ) @property def archive ( self ): return self . _archive @property def k ( self ): return self . _k def __str__ ( self ): return f \"NS(k= { self . _k } ,A= { self . _archive } )\" def __repr__ ( self ) -> str : return f \"NS<k= { self . _k } ,A= { self . _archive } >\" def __call__ ( self , instances_descriptors : np . ndarray ) -> np . ndarray : \"\"\"Computes the Novelty Search of the instance descriptors with respect to the archive. It uses the Euclidean distance to compute the sparseness. Args: instance_descriptors (np.ndarray): Numpy array with the descriptors of the instances archive (Archive): Archive which stores the novelty instances found so far k (int, optional): Number of neighbors to consider in the computation of the sparseness. Defaults to 15. Raises: ValueError: If len(instance_descriptors) <= k Returns: np.ndarray: novelty scores (s) of the instances descriptors \"\"\" if len ( instances_descriptors ) == 0 : raise ValueError ( f \"NS was given an empty population to compute the sparseness. Shape is: { instances_descriptors . shape } \" ) num_instances = len ( instances_descriptors ) num_archive = len ( self . archive ) result = np . zeros ( num_instances , dtype = np . float64 ) if num_archive == 0 and num_instances <= self . _k : # Initially, the archive is empty and we may not have enough instances to evaluate print ( f \"NS has an empty archive at this moment and the given population is not large enough to compute the sparseness. { num_instances } < k ( { self . _k } ). Returning zeros.\" , file = sys . stderr , ) return result if num_instances + num_archive <= self . _k : msg = f \"Trying to calculate novelty search with k( { self . _k } ) >= { num_instances } (instances) + { num_archive } (archive).\" raise ValueError ( msg ) combined = ( instances_descriptors if num_archive == 0 else np . vstack ([ instances_descriptors , self . _archive . descriptors ]) ) for i in range ( num_instances ): mask = np . ones ( num_instances , bool ) mask [ i ] = False differences = combined [ i ] - combined [ np . nonzero ( mask )] distances = np . linalg . norm ( differences , axis = 1 ) _neighbors = np . partition ( distances , self . _k + 1 )[ 1 : self . _k + 1 ] result [ i ] = np . sum ( _neighbors ) / self . _k return result __call__ ( instances_descriptors ) Computes the Novelty Search of the instance descriptors with respect to the archive. It uses the Euclidean distance to compute the sparseness. Parameters: instance_descriptors ( ndarray ) \u2013 Numpy array with the descriptors of the instances archive ( Archive ) \u2013 Archive which stores the novelty instances found so far k ( int ) \u2013 Number of neighbors to consider in the computation of the sparseness. Defaults to 15. Raises: ValueError \u2013 If len(instance_descriptors) <= k Returns: ndarray \u2013 np.ndarray: novelty scores (s) of the instances descriptors Source code in digneapy/_core/_novelty_search.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def __call__ ( self , instances_descriptors : np . ndarray ) -> np . ndarray : \"\"\"Computes the Novelty Search of the instance descriptors with respect to the archive. It uses the Euclidean distance to compute the sparseness. Args: instance_descriptors (np.ndarray): Numpy array with the descriptors of the instances archive (Archive): Archive which stores the novelty instances found so far k (int, optional): Number of neighbors to consider in the computation of the sparseness. Defaults to 15. Raises: ValueError: If len(instance_descriptors) <= k Returns: np.ndarray: novelty scores (s) of the instances descriptors \"\"\" if len ( instances_descriptors ) == 0 : raise ValueError ( f \"NS was given an empty population to compute the sparseness. Shape is: { instances_descriptors . shape } \" ) num_instances = len ( instances_descriptors ) num_archive = len ( self . archive ) result = np . zeros ( num_instances , dtype = np . float64 ) if num_archive == 0 and num_instances <= self . _k : # Initially, the archive is empty and we may not have enough instances to evaluate print ( f \"NS has an empty archive at this moment and the given population is not large enough to compute the sparseness. { num_instances } < k ( { self . _k } ). Returning zeros.\" , file = sys . stderr , ) return result if num_instances + num_archive <= self . _k : msg = f \"Trying to calculate novelty search with k( { self . _k } ) >= { num_instances } (instances) + { num_archive } (archive).\" raise ValueError ( msg ) combined = ( instances_descriptors if num_archive == 0 else np . vstack ([ instances_descriptors , self . _archive . descriptors ]) ) for i in range ( num_instances ): mask = np . ones ( num_instances , bool ) mask [ i ] = False differences = combined [ i ] - combined [ np . nonzero ( mask )] distances = np . linalg . norm ( differences , axis = 1 ) _neighbors = np . partition ( distances , self . _k + 1 )[ 1 : self . _k + 1 ] result [ i ] = np . sum ( _neighbors ) / self . _k return result __init__ ( archive = None , k = 15 ) Creates an instance of the Novelty Search Algorithm Args: archive (Archive): Archive to store the instances to guide the evolution. Defaults to Archive(threshold=0.001). k (int, optional): Number of neighbours to calculate the sparseness. Defaults to 15. Source code in digneapy/_core/_novelty_search.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def __init__ ( self , archive : Optional [ Archive ] = None , k : int = 15 , ): \"\"\"Creates an instance of the Novelty Search Algorithm Args: archive (Archive): Archive to store the instances to guide the evolution. Defaults to Archive(threshold=0.001). k (int, optional): Number of neighbours to calculate the sparseness. Defaults to 15. \"\"\" if k < 0 : raise ValueError ( f \" { __name__ } k must be a positive integer and less than the number of instances.\" ) if archive is not None and not isinstance ( archive , Archive ): raise ValueError ( \"You must provide a valid Archive object\" ) self . _k = k self . _archive = archive if archive is not None else Archive ( threshold = 0.001 ) dominated_novelty_search ( descriptors , performances , k = 15 , force_feasible_only = True ) Dominated Novelty Search (DNS) Bahlous-Boldi, R., Faldor, M., Grillotti, L., Janmohamed, H., Coiffard, L., Spector, L., & Cully, A. (2025). Dominated Novelty Search: Rethinking Local Competition in Quality-Diversity. 1. https://arxiv.org/abs/2502.00593v1 Quality-Diversity algorithm that implements local competition through dynamic fitness transformations, eliminating the need for predefined bounds or parameters. The competition fitness, also known as the dominated novelty score, is calculated as the average distance to the k nearest neighbors with higher fitness. The method returns a descending sorted list of instances by their competition fitness value. For each instance ``i'' in the sequence, we calculate all the other instances that dominate it. Then, we compute the distances between their descriptors using the norm of the difference for each dimension of the descriptors. Novel instances will get a competition fitness of np.inf (assuring they will survive). Less novel instances will be selected by their competition fitness value. This competition mechanism creates two complementary evolutionary pressures: individuals must either improve their fitness or discover distinct behaviors that differ from better-performing solutions. Solutions that have no fitter neighbors (D\ud835\udc56 = \u2205) receive an infinite competition fitness, ensuring their preservation in the population. Parameters: descriptors ( ndarray ) \u2013 Numpy array with the descriptors of the instances performances ( ndarray ) \u2013 Numpy array with the performance values of the instances k ( int , default: 15 ) \u2013 Number of nearest neighbours to calculate the competition fitness. Default to 15. force_feasible_only ( bool , default: True ) \u2013 Allow only instances with performance >= 0 to be considered. Default True. Raises: ValueError: If len(d) where d is the descriptor of each instance i differs from another ValueError: If k >= len(instances) Returns: Tuple [ ndarray , ndarray , ndarray , ndarray ] \u2013 Tuple[np.ndarray]: Tuple with the descriptors, performances and competition fitness values sorted, plus the sorted indexing (descending order). Source code in digneapy/_core/_novelty_search.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def dominated_novelty_search ( descriptors : np . ndarray , performances : np . ndarray , k : int = 15 , force_feasible_only : bool = True , ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray , np . ndarray ]: \"\"\" Dominated Novelty Search (DNS) Bahlous-Boldi, R., Faldor, M., Grillotti, L., Janmohamed, H., Coiffard, L., Spector, L., & Cully, A. (2025). Dominated Novelty Search: Rethinking Local Competition in Quality-Diversity. 1. https://arxiv.org/abs/2502.00593v1 Quality-Diversity algorithm that implements local competition through dynamic fitness transformations, eliminating the need for predefined bounds or parameters. The competition fitness, also known as the dominated novelty score, is calculated as the average distance to the k nearest neighbors with higher fitness. The method returns a descending sorted list of instances by their competition fitness value. For each instance ``i'' in the sequence, we calculate all the other instances that dominate it. Then, we compute the distances between their descriptors using the norm of the difference for each dimension of the descriptors. Novel instances will get a competition fitness of np.inf (assuring they will survive). Less novel instances will be selected by their competition fitness value. This competition mechanism creates two complementary evolutionary pressures: individuals must either improve their fitness or discover distinct behaviors that differ from better-performing solutions. Solutions that have no fitter neighbors (D\ud835\udc56 = \u2205) receive an infinite competition fitness, ensuring their preservation in the population. Args: descriptors (np.ndarray): Numpy array with the descriptors of the instances performances (np.ndarray): Numpy array with the performance values of the instances k (int): Number of nearest neighbours to calculate the competition fitness. Default to 15. force_feasible_only (bool): Allow only instances with performance >= 0 to be considered. Default True. Raises: ValueError: If len(d) where d is the descriptor of each instance i differs from another ValueError: If k >= len(instances) Returns: Tuple[np.ndarray]: Tuple with the descriptors, performances and competition fitness values sorted, plus the sorted indexing (descending order). \"\"\" num_instances = len ( descriptors ) if num_instances <= k : msg = f \"Trying to calculate the dominated novelty search with k( { k } ) > len(instances) = { num_instances } \" raise ValueError ( msg ) if len ( performances ) != len ( descriptors ): raise ValueError ( f \"Array mismatch between peformances and descriptors. len(performance) = { len ( performances ) } != { len ( descriptors ) } len(descriptors)\" ) # Try to force only feasible performances to get proper biased instances is_unfeasible = ( performances < 0.0 if force_feasible_only else ( performances == - np . inf ) ) fitter = performances [:, None ] <= performances [ None , :] fitter = np . where ( is_unfeasible [ None , :], False , fitter ) np . fill_diagonal ( fitter , False ) distance = np . linalg . norm ( descriptors [:, None , :] - descriptors [ None , :, :], axis =- 1 ) distance = np . where ( fitter , distance , np . inf ) neg_dist = - distance indices = np . argpartition ( neg_dist , - k , axis =- 1 )[ ... , - k :] values = np . take_along_axis ( neg_dist , indices , axis =- 1 ) indices = np . argsort ( values , axis =- 1 )[ ... , :: - 1 ] values = np . take_along_axis ( values , indices , axis =- 1 ) indices = np . take_along_axis ( indices , indices , axis =- 1 ) distance = np . mean ( - values , where = np . take_along_axis ( fitter , indices , axis = 1 ), axis =- 1 ) distance = np . where ( np . isnan ( distance ), np . inf , distance ) distance = np . where ( is_unfeasible , - np . inf , distance ) sorted_indices = np . argsort ( - distance ) return ( descriptors [ sorted_indices ], performances [ sorted_indices ], distance [ sorted_indices ], sorted_indices , )","title":" novelty search"},{"location":"reference/_core/_novelty_search/#_core._novelty_search.NS","text":"Source code in digneapy/_core/_novelty_search.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 class NS : def __init__ ( self , archive : Optional [ Archive ] = None , k : int = 15 , ): \"\"\"Creates an instance of the Novelty Search Algorithm Args: archive (Archive): Archive to store the instances to guide the evolution. Defaults to Archive(threshold=0.001). k (int, optional): Number of neighbours to calculate the sparseness. Defaults to 15. \"\"\" if k < 0 : raise ValueError ( f \" { __name__ } k must be a positive integer and less than the number of instances.\" ) if archive is not None and not isinstance ( archive , Archive ): raise ValueError ( \"You must provide a valid Archive object\" ) self . _k = k self . _archive = archive if archive is not None else Archive ( threshold = 0.001 ) @property def archive ( self ): return self . _archive @property def k ( self ): return self . _k def __str__ ( self ): return f \"NS(k= { self . _k } ,A= { self . _archive } )\" def __repr__ ( self ) -> str : return f \"NS<k= { self . _k } ,A= { self . _archive } >\" def __call__ ( self , instances_descriptors : np . ndarray ) -> np . ndarray : \"\"\"Computes the Novelty Search of the instance descriptors with respect to the archive. It uses the Euclidean distance to compute the sparseness. Args: instance_descriptors (np.ndarray): Numpy array with the descriptors of the instances archive (Archive): Archive which stores the novelty instances found so far k (int, optional): Number of neighbors to consider in the computation of the sparseness. Defaults to 15. Raises: ValueError: If len(instance_descriptors) <= k Returns: np.ndarray: novelty scores (s) of the instances descriptors \"\"\" if len ( instances_descriptors ) == 0 : raise ValueError ( f \"NS was given an empty population to compute the sparseness. Shape is: { instances_descriptors . shape } \" ) num_instances = len ( instances_descriptors ) num_archive = len ( self . archive ) result = np . zeros ( num_instances , dtype = np . float64 ) if num_archive == 0 and num_instances <= self . _k : # Initially, the archive is empty and we may not have enough instances to evaluate print ( f \"NS has an empty archive at this moment and the given population is not large enough to compute the sparseness. { num_instances } < k ( { self . _k } ). Returning zeros.\" , file = sys . stderr , ) return result if num_instances + num_archive <= self . _k : msg = f \"Trying to calculate novelty search with k( { self . _k } ) >= { num_instances } (instances) + { num_archive } (archive).\" raise ValueError ( msg ) combined = ( instances_descriptors if num_archive == 0 else np . vstack ([ instances_descriptors , self . _archive . descriptors ]) ) for i in range ( num_instances ): mask = np . ones ( num_instances , bool ) mask [ i ] = False differences = combined [ i ] - combined [ np . nonzero ( mask )] distances = np . linalg . norm ( differences , axis = 1 ) _neighbors = np . partition ( distances , self . _k + 1 )[ 1 : self . _k + 1 ] result [ i ] = np . sum ( _neighbors ) / self . _k return result","title":"NS"},{"location":"reference/_core/_novelty_search/#_core._novelty_search.NS.__call__","text":"Computes the Novelty Search of the instance descriptors with respect to the archive. It uses the Euclidean distance to compute the sparseness. Parameters: instance_descriptors ( ndarray ) \u2013 Numpy array with the descriptors of the instances archive ( Archive ) \u2013 Archive which stores the novelty instances found so far k ( int ) \u2013 Number of neighbors to consider in the computation of the sparseness. Defaults to 15. Raises: ValueError \u2013 If len(instance_descriptors) <= k Returns: ndarray \u2013 np.ndarray: novelty scores (s) of the instances descriptors Source code in digneapy/_core/_novelty_search.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def __call__ ( self , instances_descriptors : np . ndarray ) -> np . ndarray : \"\"\"Computes the Novelty Search of the instance descriptors with respect to the archive. It uses the Euclidean distance to compute the sparseness. Args: instance_descriptors (np.ndarray): Numpy array with the descriptors of the instances archive (Archive): Archive which stores the novelty instances found so far k (int, optional): Number of neighbors to consider in the computation of the sparseness. Defaults to 15. Raises: ValueError: If len(instance_descriptors) <= k Returns: np.ndarray: novelty scores (s) of the instances descriptors \"\"\" if len ( instances_descriptors ) == 0 : raise ValueError ( f \"NS was given an empty population to compute the sparseness. Shape is: { instances_descriptors . shape } \" ) num_instances = len ( instances_descriptors ) num_archive = len ( self . archive ) result = np . zeros ( num_instances , dtype = np . float64 ) if num_archive == 0 and num_instances <= self . _k : # Initially, the archive is empty and we may not have enough instances to evaluate print ( f \"NS has an empty archive at this moment and the given population is not large enough to compute the sparseness. { num_instances } < k ( { self . _k } ). Returning zeros.\" , file = sys . stderr , ) return result if num_instances + num_archive <= self . _k : msg = f \"Trying to calculate novelty search with k( { self . _k } ) >= { num_instances } (instances) + { num_archive } (archive).\" raise ValueError ( msg ) combined = ( instances_descriptors if num_archive == 0 else np . vstack ([ instances_descriptors , self . _archive . descriptors ]) ) for i in range ( num_instances ): mask = np . ones ( num_instances , bool ) mask [ i ] = False differences = combined [ i ] - combined [ np . nonzero ( mask )] distances = np . linalg . norm ( differences , axis = 1 ) _neighbors = np . partition ( distances , self . _k + 1 )[ 1 : self . _k + 1 ] result [ i ] = np . sum ( _neighbors ) / self . _k return result","title":"__call__"},{"location":"reference/_core/_novelty_search/#_core._novelty_search.NS.__init__","text":"Creates an instance of the Novelty Search Algorithm Args: archive (Archive): Archive to store the instances to guide the evolution. Defaults to Archive(threshold=0.001). k (int, optional): Number of neighbours to calculate the sparseness. Defaults to 15. Source code in digneapy/_core/_novelty_search.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def __init__ ( self , archive : Optional [ Archive ] = None , k : int = 15 , ): \"\"\"Creates an instance of the Novelty Search Algorithm Args: archive (Archive): Archive to store the instances to guide the evolution. Defaults to Archive(threshold=0.001). k (int, optional): Number of neighbours to calculate the sparseness. Defaults to 15. \"\"\" if k < 0 : raise ValueError ( f \" { __name__ } k must be a positive integer and less than the number of instances.\" ) if archive is not None and not isinstance ( archive , Archive ): raise ValueError ( \"You must provide a valid Archive object\" ) self . _k = k self . _archive = archive if archive is not None else Archive ( threshold = 0.001 )","title":"__init__"},{"location":"reference/_core/_novelty_search/#_core._novelty_search.dominated_novelty_search","text":"Dominated Novelty Search (DNS) Bahlous-Boldi, R., Faldor, M., Grillotti, L., Janmohamed, H., Coiffard, L., Spector, L., & Cully, A. (2025). Dominated Novelty Search: Rethinking Local Competition in Quality-Diversity. 1. https://arxiv.org/abs/2502.00593v1 Quality-Diversity algorithm that implements local competition through dynamic fitness transformations, eliminating the need for predefined bounds or parameters. The competition fitness, also known as the dominated novelty score, is calculated as the average distance to the k nearest neighbors with higher fitness. The method returns a descending sorted list of instances by their competition fitness value. For each instance ``i'' in the sequence, we calculate all the other instances that dominate it. Then, we compute the distances between their descriptors using the norm of the difference for each dimension of the descriptors. Novel instances will get a competition fitness of np.inf (assuring they will survive). Less novel instances will be selected by their competition fitness value. This competition mechanism creates two complementary evolutionary pressures: individuals must either improve their fitness or discover distinct behaviors that differ from better-performing solutions. Solutions that have no fitter neighbors (D\ud835\udc56 = \u2205) receive an infinite competition fitness, ensuring their preservation in the population. Parameters: descriptors ( ndarray ) \u2013 Numpy array with the descriptors of the instances performances ( ndarray ) \u2013 Numpy array with the performance values of the instances k ( int , default: 15 ) \u2013 Number of nearest neighbours to calculate the competition fitness. Default to 15. force_feasible_only ( bool , default: True ) \u2013 Allow only instances with performance >= 0 to be considered. Default True. Raises: ValueError: If len(d) where d is the descriptor of each instance i differs from another ValueError: If k >= len(instances) Returns: Tuple [ ndarray , ndarray , ndarray , ndarray ] \u2013 Tuple[np.ndarray]: Tuple with the descriptors, performances and competition fitness values sorted, plus the sorted indexing (descending order). Source code in digneapy/_core/_novelty_search.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def dominated_novelty_search ( descriptors : np . ndarray , performances : np . ndarray , k : int = 15 , force_feasible_only : bool = True , ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray , np . ndarray ]: \"\"\" Dominated Novelty Search (DNS) Bahlous-Boldi, R., Faldor, M., Grillotti, L., Janmohamed, H., Coiffard, L., Spector, L., & Cully, A. (2025). Dominated Novelty Search: Rethinking Local Competition in Quality-Diversity. 1. https://arxiv.org/abs/2502.00593v1 Quality-Diversity algorithm that implements local competition through dynamic fitness transformations, eliminating the need for predefined bounds or parameters. The competition fitness, also known as the dominated novelty score, is calculated as the average distance to the k nearest neighbors with higher fitness. The method returns a descending sorted list of instances by their competition fitness value. For each instance ``i'' in the sequence, we calculate all the other instances that dominate it. Then, we compute the distances between their descriptors using the norm of the difference for each dimension of the descriptors. Novel instances will get a competition fitness of np.inf (assuring they will survive). Less novel instances will be selected by their competition fitness value. This competition mechanism creates two complementary evolutionary pressures: individuals must either improve their fitness or discover distinct behaviors that differ from better-performing solutions. Solutions that have no fitter neighbors (D\ud835\udc56 = \u2205) receive an infinite competition fitness, ensuring their preservation in the population. Args: descriptors (np.ndarray): Numpy array with the descriptors of the instances performances (np.ndarray): Numpy array with the performance values of the instances k (int): Number of nearest neighbours to calculate the competition fitness. Default to 15. force_feasible_only (bool): Allow only instances with performance >= 0 to be considered. Default True. Raises: ValueError: If len(d) where d is the descriptor of each instance i differs from another ValueError: If k >= len(instances) Returns: Tuple[np.ndarray]: Tuple with the descriptors, performances and competition fitness values sorted, plus the sorted indexing (descending order). \"\"\" num_instances = len ( descriptors ) if num_instances <= k : msg = f \"Trying to calculate the dominated novelty search with k( { k } ) > len(instances) = { num_instances } \" raise ValueError ( msg ) if len ( performances ) != len ( descriptors ): raise ValueError ( f \"Array mismatch between peformances and descriptors. len(performance) = { len ( performances ) } != { len ( descriptors ) } len(descriptors)\" ) # Try to force only feasible performances to get proper biased instances is_unfeasible = ( performances < 0.0 if force_feasible_only else ( performances == - np . inf ) ) fitter = performances [:, None ] <= performances [ None , :] fitter = np . where ( is_unfeasible [ None , :], False , fitter ) np . fill_diagonal ( fitter , False ) distance = np . linalg . norm ( descriptors [:, None , :] - descriptors [ None , :, :], axis =- 1 ) distance = np . where ( fitter , distance , np . inf ) neg_dist = - distance indices = np . argpartition ( neg_dist , - k , axis =- 1 )[ ... , - k :] values = np . take_along_axis ( neg_dist , indices , axis =- 1 ) indices = np . argsort ( values , axis =- 1 )[ ... , :: - 1 ] values = np . take_along_axis ( values , indices , axis =- 1 ) indices = np . take_along_axis ( indices , indices , axis =- 1 ) distance = np . mean ( - values , where = np . take_along_axis ( fitter , indices , axis = 1 ), axis =- 1 ) distance = np . where ( np . isnan ( distance ), np . inf , distance ) distance = np . where ( is_unfeasible , - np . inf , distance ) sorted_indices = np . argsort ( - distance ) return ( descriptors [ sorted_indices ], performances [ sorted_indices ], distance [ sorted_indices ], sorted_indices , )","title":"dominated_novelty_search"},{"location":"reference/_core/_problem/","text":"@File : problem.py @Time : 2024/06/07 14:07:55 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2024, Alejandro Marrero @Desc : None Problem Bases: ABC , RNG Source code in digneapy/_core/_problem.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 class Problem ( ABC , RNG ): def __init__ ( self , dimension : int , bounds : Sequence [ tuple ], name : str = \"DefaultProblem\" , dtype = np . float64 , seed : int = 42 , * args , ** kwargs , ): \"\"\"Creates a new problem instance. The problem is defined by its dimension and the bounds of each variable. Args: dimension (int): Number of variables in the problem bounds (Sequence[tuple]): Bounds of each variable in the problem name (str, optional): Name of the problem for printing and logging purposes. Defaults to \"DefaultProblem\". dtype (_type_, optional): Type of the variables. Defaults to np.float64. seed (int, optional): Seed for the RNG. Defaults to 42. \"\"\" self . _name = name self . __name__ = name self . _dimension = dimension self . _bounds = bounds self . _dtype = dtype self . initialize_rng ( seed = seed ) if len ( self . _bounds ) != 0 : ranges = list ( zip ( * bounds )) self . _lbs = np . array ( ranges [ 0 ], dtype = dtype ) self . _ubs = np . array ( ranges [ 1 ], dtype = dtype ) @property def dimension ( self ): return self . _dimension @property def bounds ( self ): return self . _bounds def get_bounds_at ( self , i : int ) -> tuple : if i < 0 or i > len ( self . _bounds ): raise ValueError ( f \"Index { i } out-of-range. The bounds are 0- { len ( self . _bounds ) } \" ) return ( self . _lbs [ i ], self . _ubs [ i ]) @abstractmethod def create_solution ( self ) -> Solution | np . ndarray : \"\"\"Creates a random solution to the problem. This method can be used to initialise the solutions for any algorithm \"\"\" msg = \"create_solution method not implemented in Problem\" raise NotImplementedError ( msg ) @abstractmethod def __array__ ( self , dtype : Any = None , copy : Optional [ bool ] = None ) -> npt . ArrayLike : msg = \"__array__ method not implemented in Problem\" raise NotImplementedError ( msg ) @abstractmethod def evaluate ( self , individual : Sequence | Solution | np . ndarray ) -> Tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Knapsack Args: individual (Sequence | Solution | np.ndarray): Individual to evaluate Raises: ValueError: Raises an error if the len(individual) != len(instance) / 2 Returns: Tuple[float]: fitness \"\"\" msg = \"evaluate method not implemented in Problem\" raise NotImplementedError ( msg ) @abstractmethod def __call__ ( self , individual : Sequence | Solution | np . ndarray ) -> Tuple [ float ]: msg = \"__call__ method not implemented in Problem\" raise NotImplementedError ( msg ) @abstractmethod def to_instance ( self ) -> Instance : \"\"\"Creates an instance from the information of the problem. This method is used in the generators to create instances to evolve \"\"\" msg = \"to_instance method not implemented in Problem\" raise NotImplementedError ( msg ) @abstractmethod def to_file ( self , filename : str ): msg = \"to_file method not implemented in Problem\" raise NotImplementedError ( msg ) @classmethod def from_file ( cls , filename : str ): msg = \"from_file method not implemented in Problem\" raise NotImplementedError ( msg ) __init__ ( dimension , bounds , name = 'DefaultProblem' , dtype = np . float64 , seed = 42 , * args , ** kwargs ) Creates a new problem instance. The problem is defined by its dimension and the bounds of each variable. Parameters: dimension ( int ) \u2013 Number of variables in the problem bounds ( Sequence [ tuple ] ) \u2013 Bounds of each variable in the problem name ( str , default: 'DefaultProblem' ) \u2013 Name of the problem for printing and logging purposes. Defaults to \"DefaultProblem\". dtype ( _type_ , default: float64 ) \u2013 Type of the variables. Defaults to np.float64. seed ( int , default: 42 ) \u2013 Seed for the RNG. Defaults to 42. Source code in digneapy/_core/_problem.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , dimension : int , bounds : Sequence [ tuple ], name : str = \"DefaultProblem\" , dtype = np . float64 , seed : int = 42 , * args , ** kwargs , ): \"\"\"Creates a new problem instance. The problem is defined by its dimension and the bounds of each variable. Args: dimension (int): Number of variables in the problem bounds (Sequence[tuple]): Bounds of each variable in the problem name (str, optional): Name of the problem for printing and logging purposes. Defaults to \"DefaultProblem\". dtype (_type_, optional): Type of the variables. Defaults to np.float64. seed (int, optional): Seed for the RNG. Defaults to 42. \"\"\" self . _name = name self . __name__ = name self . _dimension = dimension self . _bounds = bounds self . _dtype = dtype self . initialize_rng ( seed = seed ) if len ( self . _bounds ) != 0 : ranges = list ( zip ( * bounds )) self . _lbs = np . array ( ranges [ 0 ], dtype = dtype ) self . _ubs = np . array ( ranges [ 1 ], dtype = dtype ) create_solution () abstractmethod Creates a random solution to the problem. This method can be used to initialise the solutions for any algorithm Source code in digneapy/_core/_problem.py 72 73 74 75 76 77 78 79 @abstractmethod def create_solution ( self ) -> Solution | np . ndarray : \"\"\"Creates a random solution to the problem. This method can be used to initialise the solutions for any algorithm \"\"\" msg = \"create_solution method not implemented in Problem\" raise NotImplementedError ( msg ) evaluate ( individual ) abstractmethod Evaluates the candidate individual with the information of the Knapsack Parameters: individual ( Sequence | Solution | ndarray ) \u2013 Individual to evaluate Raises: ValueError \u2013 Raises an error if the len(individual) != len(instance) / 2 Returns: Tuple [ float ] \u2013 Tuple[float]: fitness Source code in digneapy/_core/_problem.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 @abstractmethod def evaluate ( self , individual : Sequence | Solution | np . ndarray ) -> Tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Knapsack Args: individual (Sequence | Solution | np.ndarray): Individual to evaluate Raises: ValueError: Raises an error if the len(individual) != len(instance) / 2 Returns: Tuple[float]: fitness \"\"\" msg = \"evaluate method not implemented in Problem\" raise NotImplementedError ( msg ) to_instance () abstractmethod Creates an instance from the information of the problem. This method is used in the generators to create instances to evolve Source code in digneapy/_core/_problem.py 109 110 111 112 113 114 115 @abstractmethod def to_instance ( self ) -> Instance : \"\"\"Creates an instance from the information of the problem. This method is used in the generators to create instances to evolve \"\"\" msg = \"to_instance method not implemented in Problem\" raise NotImplementedError ( msg )","title":" problem"},{"location":"reference/_core/_problem/#_core._problem.Problem","text":"Bases: ABC , RNG Source code in digneapy/_core/_problem.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 class Problem ( ABC , RNG ): def __init__ ( self , dimension : int , bounds : Sequence [ tuple ], name : str = \"DefaultProblem\" , dtype = np . float64 , seed : int = 42 , * args , ** kwargs , ): \"\"\"Creates a new problem instance. The problem is defined by its dimension and the bounds of each variable. Args: dimension (int): Number of variables in the problem bounds (Sequence[tuple]): Bounds of each variable in the problem name (str, optional): Name of the problem for printing and logging purposes. Defaults to \"DefaultProblem\". dtype (_type_, optional): Type of the variables. Defaults to np.float64. seed (int, optional): Seed for the RNG. Defaults to 42. \"\"\" self . _name = name self . __name__ = name self . _dimension = dimension self . _bounds = bounds self . _dtype = dtype self . initialize_rng ( seed = seed ) if len ( self . _bounds ) != 0 : ranges = list ( zip ( * bounds )) self . _lbs = np . array ( ranges [ 0 ], dtype = dtype ) self . _ubs = np . array ( ranges [ 1 ], dtype = dtype ) @property def dimension ( self ): return self . _dimension @property def bounds ( self ): return self . _bounds def get_bounds_at ( self , i : int ) -> tuple : if i < 0 or i > len ( self . _bounds ): raise ValueError ( f \"Index { i } out-of-range. The bounds are 0- { len ( self . _bounds ) } \" ) return ( self . _lbs [ i ], self . _ubs [ i ]) @abstractmethod def create_solution ( self ) -> Solution | np . ndarray : \"\"\"Creates a random solution to the problem. This method can be used to initialise the solutions for any algorithm \"\"\" msg = \"create_solution method not implemented in Problem\" raise NotImplementedError ( msg ) @abstractmethod def __array__ ( self , dtype : Any = None , copy : Optional [ bool ] = None ) -> npt . ArrayLike : msg = \"__array__ method not implemented in Problem\" raise NotImplementedError ( msg ) @abstractmethod def evaluate ( self , individual : Sequence | Solution | np . ndarray ) -> Tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Knapsack Args: individual (Sequence | Solution | np.ndarray): Individual to evaluate Raises: ValueError: Raises an error if the len(individual) != len(instance) / 2 Returns: Tuple[float]: fitness \"\"\" msg = \"evaluate method not implemented in Problem\" raise NotImplementedError ( msg ) @abstractmethod def __call__ ( self , individual : Sequence | Solution | np . ndarray ) -> Tuple [ float ]: msg = \"__call__ method not implemented in Problem\" raise NotImplementedError ( msg ) @abstractmethod def to_instance ( self ) -> Instance : \"\"\"Creates an instance from the information of the problem. This method is used in the generators to create instances to evolve \"\"\" msg = \"to_instance method not implemented in Problem\" raise NotImplementedError ( msg ) @abstractmethod def to_file ( self , filename : str ): msg = \"to_file method not implemented in Problem\" raise NotImplementedError ( msg ) @classmethod def from_file ( cls , filename : str ): msg = \"from_file method not implemented in Problem\" raise NotImplementedError ( msg )","title":"Problem"},{"location":"reference/_core/_problem/#_core._problem.Problem.__init__","text":"Creates a new problem instance. The problem is defined by its dimension and the bounds of each variable. Parameters: dimension ( int ) \u2013 Number of variables in the problem bounds ( Sequence [ tuple ] ) \u2013 Bounds of each variable in the problem name ( str , default: 'DefaultProblem' ) \u2013 Name of the problem for printing and logging purposes. Defaults to \"DefaultProblem\". dtype ( _type_ , default: float64 ) \u2013 Type of the variables. Defaults to np.float64. seed ( int , default: 42 ) \u2013 Seed for the RNG. Defaults to 42. Source code in digneapy/_core/_problem.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , dimension : int , bounds : Sequence [ tuple ], name : str = \"DefaultProblem\" , dtype = np . float64 , seed : int = 42 , * args , ** kwargs , ): \"\"\"Creates a new problem instance. The problem is defined by its dimension and the bounds of each variable. Args: dimension (int): Number of variables in the problem bounds (Sequence[tuple]): Bounds of each variable in the problem name (str, optional): Name of the problem for printing and logging purposes. Defaults to \"DefaultProblem\". dtype (_type_, optional): Type of the variables. Defaults to np.float64. seed (int, optional): Seed for the RNG. Defaults to 42. \"\"\" self . _name = name self . __name__ = name self . _dimension = dimension self . _bounds = bounds self . _dtype = dtype self . initialize_rng ( seed = seed ) if len ( self . _bounds ) != 0 : ranges = list ( zip ( * bounds )) self . _lbs = np . array ( ranges [ 0 ], dtype = dtype ) self . _ubs = np . array ( ranges [ 1 ], dtype = dtype )","title":"__init__"},{"location":"reference/_core/_problem/#_core._problem.Problem.create_solution","text":"Creates a random solution to the problem. This method can be used to initialise the solutions for any algorithm Source code in digneapy/_core/_problem.py 72 73 74 75 76 77 78 79 @abstractmethod def create_solution ( self ) -> Solution | np . ndarray : \"\"\"Creates a random solution to the problem. This method can be used to initialise the solutions for any algorithm \"\"\" msg = \"create_solution method not implemented in Problem\" raise NotImplementedError ( msg )","title":"create_solution"},{"location":"reference/_core/_problem/#_core._problem.Problem.evaluate","text":"Evaluates the candidate individual with the information of the Knapsack Parameters: individual ( Sequence | Solution | ndarray ) \u2013 Individual to evaluate Raises: ValueError \u2013 Raises an error if the len(individual) != len(instance) / 2 Returns: Tuple [ float ] \u2013 Tuple[float]: fitness Source code in digneapy/_core/_problem.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 @abstractmethod def evaluate ( self , individual : Sequence | Solution | np . ndarray ) -> Tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Knapsack Args: individual (Sequence | Solution | np.ndarray): Individual to evaluate Raises: ValueError: Raises an error if the len(individual) != len(instance) / 2 Returns: Tuple[float]: fitness \"\"\" msg = \"evaluate method not implemented in Problem\" raise NotImplementedError ( msg )","title":"evaluate"},{"location":"reference/_core/_problem/#_core._problem.Problem.to_instance","text":"Creates an instance from the information of the problem. This method is used in the generators to create instances to evolve Source code in digneapy/_core/_problem.py 109 110 111 112 113 114 115 @abstractmethod def to_instance ( self ) -> Instance : \"\"\"Creates an instance from the information of the problem. This method is used in the generators to create instances to evolve \"\"\" msg = \"to_instance method not implemented in Problem\" raise NotImplementedError ( msg )","title":"to_instance"},{"location":"reference/_core/_solution/","text":"@File : solution.py @Time : 2024/06/07 14:09:54 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2024, Alejandro Marrero @Desc : None Solution Class representing a solution in a genetic algorithm. It contains the variables, objectives, constraints, and fitness of the solution. Source code in digneapy/_core/_solution.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 class Solution : \"\"\" Class representing a solution in a genetic algorithm. It contains the variables, objectives, constraints, and fitness of the solution. \"\"\" def __init__ ( self , variables : Optional [ Iterable ] = [], objectives : Optional [ Iterable ] = [], constraints : Optional [ Iterable ] = [], fitness : np . float64 = np . float64 ( 0.0 ), dtype = np . uint32 , otype = np . float64 , ): \"\"\"Creates a new solution object. The variables is a numpy array of the solution's genes. The objectives and constraints are numpy arrays of the solution's objectives and constraints. The fitness is a float representing the solution's fitness value. Args: variables (Optional[Iterable], optional): Tuple or any other iterable with the variables/variables. Defaults to None. objectives (Optional[Iterable], optional): Tuple or any other iterable with the objectives values. Defaults to None. constraints (Optional[Iterable], optional): Tuple or any other iterable with the constraint values. Defaults to None. fitness (float, optional): Fitness of the solution. Defaults to 0.0. \"\"\" self . _otype = otype self . _dtype = dtype self . variables = np . asarray ( variables , dtype = self . dtype ) self . objectives = np . array ( objectives , dtype = self . otype ) self . constraints = np . array ( constraints , dtype = self . otype ) self . fitness = otype ( fitness ) @property def dtype ( self ): return self . _dtype @property def otype ( self ): return self . _otype def clone ( self ) -> Self : \"\"\"Returns a deep copy of the solution. It is more efficient than using the copy module. Returns: Self: Solution object \"\"\" return Solution ( variables = list ( self . variables ), objectives = list ( self . objectives ), constraints = list ( self . constraints ), fitness = self . fitness , otype = self . otype , ) def clone_with ( self , ** overrides ): \"\"\"Clones an Instance with overriden attributes Returns: Instance \"\"\" new_object = self . clone () for key , value in overrides . items (): setattr ( new_object , key , value ) return new_object def __str__ ( self ) -> str : return f \"Solution(dim= { len ( self . variables ) } ,f= { self . fitness } ,objs= { self . objectives } ,const= { self . constraints } )\" def __repr__ ( self ) -> str : return f \"Solution<dim= { len ( self . variables ) } ,f= { self . fitness } ,objs= { self . objectives } ,const= { self . constraints } >\" def __len__ ( self ) -> int : return len ( self . variables ) def __iter__ ( self ): return iter ( self . variables ) def __bool__ ( self ): return len ( self ) != 0 def __eq__ ( self , other ) -> bool : if isinstance ( other , Solution ): try : return all ( a == b for a , b in zip ( self , other , strict = True )) except ValueError : return False else : return NotImplemented def __gt__ ( self , other ): if not isinstance ( other , Solution ): msg = f \"Other of type { other . __class__ . __name__ } can not be compared with with { self . __class__ . __name__ } \" print ( msg ) return NotImplemented return self . fitness > other . fitness def __getitem__ ( self , key ): if isinstance ( key , slice ): cls = type ( self ) # To facilitate subclassing return cls ( self . variables [ key ]) index = operator . index ( key ) return self . variables [ index ] def __setitem__ ( self , key , value ): self . variables [ key ] = value __init__ ( variables = [], objectives = [], constraints = [], fitness = np . float64 ( 0.0 ), dtype = np . uint32 , otype = np . float64 ) Creates a new solution object. The variables is a numpy array of the solution's genes. The objectives and constraints are numpy arrays of the solution's objectives and constraints. The fitness is a float representing the solution's fitness value. Parameters: variables ( Optional [ Iterable ] , default: [] ) \u2013 Tuple or any other iterable with the variables/variables. Defaults to None. objectives ( Optional [ Iterable ] , default: [] ) \u2013 Tuple or any other iterable with the objectives values. Defaults to None. constraints ( Optional [ Iterable ] , default: [] ) \u2013 Tuple or any other iterable with the constraint values. Defaults to None. fitness ( float , default: float64 (0.0) ) \u2013 Fitness of the solution. Defaults to 0.0. Source code in digneapy/_core/_solution.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def __init__ ( self , variables : Optional [ Iterable ] = [], objectives : Optional [ Iterable ] = [], constraints : Optional [ Iterable ] = [], fitness : np . float64 = np . float64 ( 0.0 ), dtype = np . uint32 , otype = np . float64 , ): \"\"\"Creates a new solution object. The variables is a numpy array of the solution's genes. The objectives and constraints are numpy arrays of the solution's objectives and constraints. The fitness is a float representing the solution's fitness value. Args: variables (Optional[Iterable], optional): Tuple or any other iterable with the variables/variables. Defaults to None. objectives (Optional[Iterable], optional): Tuple or any other iterable with the objectives values. Defaults to None. constraints (Optional[Iterable], optional): Tuple or any other iterable with the constraint values. Defaults to None. fitness (float, optional): Fitness of the solution. Defaults to 0.0. \"\"\" self . _otype = otype self . _dtype = dtype self . variables = np . asarray ( variables , dtype = self . dtype ) self . objectives = np . array ( objectives , dtype = self . otype ) self . constraints = np . array ( constraints , dtype = self . otype ) self . fitness = otype ( fitness ) clone () Returns a deep copy of the solution. It is more efficient than using the copy module. Returns: Self ( Self ) \u2013 Solution object Source code in digneapy/_core/_solution.py 62 63 64 65 66 67 68 69 70 71 72 73 74 def clone ( self ) -> Self : \"\"\"Returns a deep copy of the solution. It is more efficient than using the copy module. Returns: Self: Solution object \"\"\" return Solution ( variables = list ( self . variables ), objectives = list ( self . objectives ), constraints = list ( self . constraints ), fitness = self . fitness , otype = self . otype , ) clone_with ( ** overrides ) Clones an Instance with overriden attributes Returns: \u2013 Instance Source code in digneapy/_core/_solution.py 76 77 78 79 80 81 82 83 84 85 def clone_with ( self , ** overrides ): \"\"\"Clones an Instance with overriden attributes Returns: Instance \"\"\" new_object = self . clone () for key , value in overrides . items (): setattr ( new_object , key , value ) return new_object","title":" solution"},{"location":"reference/_core/_solution/#_core._solution.Solution","text":"Class representing a solution in a genetic algorithm. It contains the variables, objectives, constraints, and fitness of the solution. Source code in digneapy/_core/_solution.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 class Solution : \"\"\" Class representing a solution in a genetic algorithm. It contains the variables, objectives, constraints, and fitness of the solution. \"\"\" def __init__ ( self , variables : Optional [ Iterable ] = [], objectives : Optional [ Iterable ] = [], constraints : Optional [ Iterable ] = [], fitness : np . float64 = np . float64 ( 0.0 ), dtype = np . uint32 , otype = np . float64 , ): \"\"\"Creates a new solution object. The variables is a numpy array of the solution's genes. The objectives and constraints are numpy arrays of the solution's objectives and constraints. The fitness is a float representing the solution's fitness value. Args: variables (Optional[Iterable], optional): Tuple or any other iterable with the variables/variables. Defaults to None. objectives (Optional[Iterable], optional): Tuple or any other iterable with the objectives values. Defaults to None. constraints (Optional[Iterable], optional): Tuple or any other iterable with the constraint values. Defaults to None. fitness (float, optional): Fitness of the solution. Defaults to 0.0. \"\"\" self . _otype = otype self . _dtype = dtype self . variables = np . asarray ( variables , dtype = self . dtype ) self . objectives = np . array ( objectives , dtype = self . otype ) self . constraints = np . array ( constraints , dtype = self . otype ) self . fitness = otype ( fitness ) @property def dtype ( self ): return self . _dtype @property def otype ( self ): return self . _otype def clone ( self ) -> Self : \"\"\"Returns a deep copy of the solution. It is more efficient than using the copy module. Returns: Self: Solution object \"\"\" return Solution ( variables = list ( self . variables ), objectives = list ( self . objectives ), constraints = list ( self . constraints ), fitness = self . fitness , otype = self . otype , ) def clone_with ( self , ** overrides ): \"\"\"Clones an Instance with overriden attributes Returns: Instance \"\"\" new_object = self . clone () for key , value in overrides . items (): setattr ( new_object , key , value ) return new_object def __str__ ( self ) -> str : return f \"Solution(dim= { len ( self . variables ) } ,f= { self . fitness } ,objs= { self . objectives } ,const= { self . constraints } )\" def __repr__ ( self ) -> str : return f \"Solution<dim= { len ( self . variables ) } ,f= { self . fitness } ,objs= { self . objectives } ,const= { self . constraints } >\" def __len__ ( self ) -> int : return len ( self . variables ) def __iter__ ( self ): return iter ( self . variables ) def __bool__ ( self ): return len ( self ) != 0 def __eq__ ( self , other ) -> bool : if isinstance ( other , Solution ): try : return all ( a == b for a , b in zip ( self , other , strict = True )) except ValueError : return False else : return NotImplemented def __gt__ ( self , other ): if not isinstance ( other , Solution ): msg = f \"Other of type { other . __class__ . __name__ } can not be compared with with { self . __class__ . __name__ } \" print ( msg ) return NotImplemented return self . fitness > other . fitness def __getitem__ ( self , key ): if isinstance ( key , slice ): cls = type ( self ) # To facilitate subclassing return cls ( self . variables [ key ]) index = operator . index ( key ) return self . variables [ index ] def __setitem__ ( self , key , value ): self . variables [ key ] = value","title":"Solution"},{"location":"reference/_core/_solution/#_core._solution.Solution.__init__","text":"Creates a new solution object. The variables is a numpy array of the solution's genes. The objectives and constraints are numpy arrays of the solution's objectives and constraints. The fitness is a float representing the solution's fitness value. Parameters: variables ( Optional [ Iterable ] , default: [] ) \u2013 Tuple or any other iterable with the variables/variables. Defaults to None. objectives ( Optional [ Iterable ] , default: [] ) \u2013 Tuple or any other iterable with the objectives values. Defaults to None. constraints ( Optional [ Iterable ] , default: [] ) \u2013 Tuple or any other iterable with the constraint values. Defaults to None. fitness ( float , default: float64 (0.0) ) \u2013 Fitness of the solution. Defaults to 0.0. Source code in digneapy/_core/_solution.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def __init__ ( self , variables : Optional [ Iterable ] = [], objectives : Optional [ Iterable ] = [], constraints : Optional [ Iterable ] = [], fitness : np . float64 = np . float64 ( 0.0 ), dtype = np . uint32 , otype = np . float64 , ): \"\"\"Creates a new solution object. The variables is a numpy array of the solution's genes. The objectives and constraints are numpy arrays of the solution's objectives and constraints. The fitness is a float representing the solution's fitness value. Args: variables (Optional[Iterable], optional): Tuple or any other iterable with the variables/variables. Defaults to None. objectives (Optional[Iterable], optional): Tuple or any other iterable with the objectives values. Defaults to None. constraints (Optional[Iterable], optional): Tuple or any other iterable with the constraint values. Defaults to None. fitness (float, optional): Fitness of the solution. Defaults to 0.0. \"\"\" self . _otype = otype self . _dtype = dtype self . variables = np . asarray ( variables , dtype = self . dtype ) self . objectives = np . array ( objectives , dtype = self . otype ) self . constraints = np . array ( constraints , dtype = self . otype ) self . fitness = otype ( fitness )","title":"__init__"},{"location":"reference/_core/_solution/#_core._solution.Solution.clone","text":"Returns a deep copy of the solution. It is more efficient than using the copy module. Returns: Self ( Self ) \u2013 Solution object Source code in digneapy/_core/_solution.py 62 63 64 65 66 67 68 69 70 71 72 73 74 def clone ( self ) -> Self : \"\"\"Returns a deep copy of the solution. It is more efficient than using the copy module. Returns: Self: Solution object \"\"\" return Solution ( variables = list ( self . variables ), objectives = list ( self . objectives ), constraints = list ( self . constraints ), fitness = self . fitness , otype = self . otype , )","title":"clone"},{"location":"reference/_core/_solution/#_core._solution.Solution.clone_with","text":"Clones an Instance with overriden attributes Returns: \u2013 Instance Source code in digneapy/_core/_solution.py 76 77 78 79 80 81 82 83 84 85 def clone_with ( self , ** overrides ): \"\"\"Clones an Instance with overriden attributes Returns: Instance \"\"\" new_object = self . clone () for key , value in overrides . items (): setattr ( new_object , key , value ) return new_object","title":"clone_with"},{"location":"reference/_core/_solver/","text":"@File : solver.py @Time : 2024/06/07 14:10:11 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2024, Alejandro Marrero @Desc : None Solver Bases: ABC , SupportsSolve [ P ] Solver is any callable type that receives a OptProblem as its argument and returns a tuple with the solution found Source code in digneapy/_core/_solver.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class Solver ( ABC , SupportsSolve [ P ]): \"\"\"Solver is any callable type that receives a OptProblem as its argument and returns a tuple with the solution found \"\"\" @abstractmethod def __call__ ( self , problem : P , * args , ** kwargs ) -> list [ Solution ]: \"\"\"Solves a optimisation problem Args: problem (OptProblem): Any optimisation problem or callablle that receives a Sequence and returns a Tuple[float] Raises: NotImplementedError: Must be implemented by subclasses Returns: List[Solution]: Returns a sequence of olutions \"\"\" msg = \"__call__ method not implemented in Solver\" raise NotImplementedError ( msg ) __call__ ( problem , * args , ** kwargs ) abstractmethod Solves a optimisation problem Parameters: problem ( OptProblem ) \u2013 Any optimisation problem or callablle that receives a Sequence and returns a Tuple[float] Raises: NotImplementedError \u2013 Must be implemented by subclasses Returns: list [ Solution ] \u2013 List[Solution]: Returns a sequence of olutions Source code in digneapy/_core/_solver.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 @abstractmethod def __call__ ( self , problem : P , * args , ** kwargs ) -> list [ Solution ]: \"\"\"Solves a optimisation problem Args: problem (OptProblem): Any optimisation problem or callablle that receives a Sequence and returns a Tuple[float] Raises: NotImplementedError: Must be implemented by subclasses Returns: List[Solution]: Returns a sequence of olutions \"\"\" msg = \"__call__ method not implemented in Solver\" raise NotImplementedError ( msg ) SupportsSolve Bases: Protocol [ P ] Protocol to type check all the solver types in digneapy. A solver is any callable type that receives at least a problem (Problem) and returns a list of object of the Solution class. Source code in digneapy/_core/_solver.py 20 21 22 23 24 25 26 class SupportsSolve ( Protocol [ P ]): \"\"\"Protocol to type check all the solver types in digneapy. A solver is any callable type that receives at least a problem (Problem) and returns a list of object of the Solution class. \"\"\" def __call__ ( self , problem : P , * args , ** kwargs ) -> list [ Solution ]: ...","title":" solver"},{"location":"reference/_core/_solver/#_core._solver.Solver","text":"Bases: ABC , SupportsSolve [ P ] Solver is any callable type that receives a OptProblem as its argument and returns a tuple with the solution found Source code in digneapy/_core/_solver.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class Solver ( ABC , SupportsSolve [ P ]): \"\"\"Solver is any callable type that receives a OptProblem as its argument and returns a tuple with the solution found \"\"\" @abstractmethod def __call__ ( self , problem : P , * args , ** kwargs ) -> list [ Solution ]: \"\"\"Solves a optimisation problem Args: problem (OptProblem): Any optimisation problem or callablle that receives a Sequence and returns a Tuple[float] Raises: NotImplementedError: Must be implemented by subclasses Returns: List[Solution]: Returns a sequence of olutions \"\"\" msg = \"__call__ method not implemented in Solver\" raise NotImplementedError ( msg )","title":"Solver"},{"location":"reference/_core/_solver/#_core._solver.Solver.__call__","text":"Solves a optimisation problem Parameters: problem ( OptProblem ) \u2013 Any optimisation problem or callablle that receives a Sequence and returns a Tuple[float] Raises: NotImplementedError \u2013 Must be implemented by subclasses Returns: list [ Solution ] \u2013 List[Solution]: Returns a sequence of olutions Source code in digneapy/_core/_solver.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 @abstractmethod def __call__ ( self , problem : P , * args , ** kwargs ) -> list [ Solution ]: \"\"\"Solves a optimisation problem Args: problem (OptProblem): Any optimisation problem or callablle that receives a Sequence and returns a Tuple[float] Raises: NotImplementedError: Must be implemented by subclasses Returns: List[Solution]: Returns a sequence of olutions \"\"\" msg = \"__call__ method not implemented in Solver\" raise NotImplementedError ( msg )","title":"__call__"},{"location":"reference/_core/_solver/#_core._solver.SupportsSolve","text":"Bases: Protocol [ P ] Protocol to type check all the solver types in digneapy. A solver is any callable type that receives at least a problem (Problem) and returns a list of object of the Solution class. Source code in digneapy/_core/_solver.py 20 21 22 23 24 25 26 class SupportsSolve ( Protocol [ P ]): \"\"\"Protocol to type check all the solver types in digneapy. A solver is any callable type that receives at least a problem (Problem) and returns a list of object of the Solution class. \"\"\" def __call__ ( self , problem : P , * args , ** kwargs ) -> list [ Solution ]: ...","title":"SupportsSolve"},{"location":"reference/_core/descriptors/","text":"@File : _descriptor_strategies.py @Time : 2024/06/07 14:29:09 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2024, Alejandro Marrero @Desc : Descriptors Strategies for Instance Generation __property_strategy ( attr ) Returns a np.ndarray with the information required of the instances Parameters: iterable ( Iterable [ Instance ] ) \u2013 Instances to describe Returns: \u2013 np.ndarray: Array of the feature descriptors of each instance Source code in digneapy/_core/descriptors.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def __property_strategy ( attr : str ): \"\"\"Returns a np.ndarray with the information required of the instances Args: iterable (Iterable[Instance]): Instances to describe Returns: np.ndarray: Array of the feature descriptors of each instance \"\"\" try : if attr not in ( \"features\" , \"transformed\" ): raise AttributeError () except AttributeError : raise ValueError ( f \"Object of class Instance does not have a property named { attr } \" ) def strategy ( iterable : Iterable [ Instance ]) -> np . ndarray : return np . asarray ([ getattr ( i , attr ) for i in iterable ]) return strategy descriptor ( key , verbose = False ) Decorator to create new descriptor strategies Parameters: key ( str ) \u2013 Key to refer the descriptor-function verbose ( bool , default: False ) \u2013 Prints a message when the function is registered. Defaults to False. Source code in digneapy/_core/descriptors.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def descriptor ( key : str , verbose : bool = False ): \"\"\"Decorator to create new descriptor strategies Args: key (str): Key to refer the descriptor-function verbose (bool, optional): Prints a message when the function is registered. Defaults to False. \"\"\" def decorate ( func : DescStrategy ): if verbose : print ( f \"Registering descriptor function: { func . __name__ } with key: { key } \" ) DESCRIPTORS [ key ] = func return func return decorate instance_strategy ( iterable ) It returns the instance information as its descriptor Parameters: iterable ( Iterable [ Instance ] ) \u2013 Instances to describe Returns: ndarray \u2013 np.ndarray: Array of descriptor instance (whole instace data) Source code in digneapy/_core/descriptors.py 91 92 93 94 95 96 97 98 99 100 def instance_strategy ( iterable : Iterable [ Instance ]) -> np . ndarray : \"\"\"It returns the instance information as its descriptor Args: iterable (Iterable[Instance]): Instances to describe Returns: np.ndarray: Array of descriptor instance (whole instace data) \"\"\" return np . asarray ([ * iterable ]) performance_strategy ( performances ) It generates the performance descriptor of an instance based on the scores of the solvers in the portfolio over such instance Parameters: iterable ( Iterable [ Instance ] ) \u2013 Instances to describe Returns: ndarray \u2013 np.ndarray: Array of performance descriptors of each instance Source code in digneapy/_core/descriptors.py 77 78 79 80 81 82 83 84 85 86 87 88 def performance_strategy ( performances : np . ndarray ) -> np . ndarray : \"\"\"It generates the performance descriptor of an instance based on the scores of the solvers in the portfolio over such instance Args: iterable (Iterable[Instance]): Instances to describe Returns: np.ndarray: Array of performance descriptors of each instance \"\"\" print ( performances ) return np . mean ( performances , axis = 1 )","title":"Descriptors"},{"location":"reference/_core/descriptors/#_core.descriptors.__property_strategy","text":"Returns a np.ndarray with the information required of the instances Parameters: iterable ( Iterable [ Instance ] ) \u2013 Instances to describe Returns: \u2013 np.ndarray: Array of the feature descriptors of each instance Source code in digneapy/_core/descriptors.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def __property_strategy ( attr : str ): \"\"\"Returns a np.ndarray with the information required of the instances Args: iterable (Iterable[Instance]): Instances to describe Returns: np.ndarray: Array of the feature descriptors of each instance \"\"\" try : if attr not in ( \"features\" , \"transformed\" ): raise AttributeError () except AttributeError : raise ValueError ( f \"Object of class Instance does not have a property named { attr } \" ) def strategy ( iterable : Iterable [ Instance ]) -> np . ndarray : return np . asarray ([ getattr ( i , attr ) for i in iterable ]) return strategy","title":"__property_strategy"},{"location":"reference/_core/descriptors/#_core.descriptors.descriptor","text":"Decorator to create new descriptor strategies Parameters: key ( str ) \u2013 Key to refer the descriptor-function verbose ( bool , default: False ) \u2013 Prints a message when the function is registered. Defaults to False. Source code in digneapy/_core/descriptors.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def descriptor ( key : str , verbose : bool = False ): \"\"\"Decorator to create new descriptor strategies Args: key (str): Key to refer the descriptor-function verbose (bool, optional): Prints a message when the function is registered. Defaults to False. \"\"\" def decorate ( func : DescStrategy ): if verbose : print ( f \"Registering descriptor function: { func . __name__ } with key: { key } \" ) DESCRIPTORS [ key ] = func return func return decorate","title":"descriptor"},{"location":"reference/_core/descriptors/#_core.descriptors.instance_strategy","text":"It returns the instance information as its descriptor Parameters: iterable ( Iterable [ Instance ] ) \u2013 Instances to describe Returns: ndarray \u2013 np.ndarray: Array of descriptor instance (whole instace data) Source code in digneapy/_core/descriptors.py 91 92 93 94 95 96 97 98 99 100 def instance_strategy ( iterable : Iterable [ Instance ]) -> np . ndarray : \"\"\"It returns the instance information as its descriptor Args: iterable (Iterable[Instance]): Instances to describe Returns: np.ndarray: Array of descriptor instance (whole instace data) \"\"\" return np . asarray ([ * iterable ])","title":"instance_strategy"},{"location":"reference/_core/descriptors/#_core.descriptors.performance_strategy","text":"It generates the performance descriptor of an instance based on the scores of the solvers in the portfolio over such instance Parameters: iterable ( Iterable [ Instance ] ) \u2013 Instances to describe Returns: ndarray \u2013 np.ndarray: Array of performance descriptors of each instance Source code in digneapy/_core/descriptors.py 77 78 79 80 81 82 83 84 85 86 87 88 def performance_strategy ( performances : np . ndarray ) -> np . ndarray : \"\"\"It generates the performance descriptor of an instance based on the scores of the solvers in the portfolio over such instance Args: iterable (Iterable[Instance]): Instances to describe Returns: np.ndarray: Array of performance descriptors of each instance \"\"\" print ( performances ) return np . mean ( performances , axis = 1 )","title":"performance_strategy"},{"location":"reference/_core/scores/","text":"@File : scores.py @Time : 2024/09/18 10:43:17 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2024, Alejandro Marrero @Desc : None max_gap_target ( scores ) Maximum gap to target. It tries to maximise the gap between the target solver and the other solvers in the portfolio. Use this metric to generate instances that are EASY to solve by the target algorithm Parameters: scores ( ndarray [ float ] ) \u2013 Scores of each solver over every instances. It is expected Returns: ndarray \u2013 np.ndarray: Performance biases for every instance. Instance.p attribute. Source code in digneapy/_core/scores.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def max_gap_target ( scores : np . ndarray ) -> np . ndarray : \"\"\"Maximum gap to target. It tries to maximise the gap between the target solver and the other solvers in the portfolio. Use this metric to generate instances that are EASY to solve by the target algorithm Args: scores (np.ndarray[float]): Scores of each solver over every instances. It is expected that the first value is the score of the target. Returns: np.ndarray: Performance biases for every instance. Instance.p attribute. \"\"\" if scores . ndim != 2 : raise ValueError ( f \"Expected a 2d numpy array (i, s). Where `i` is the number of instances, `s` the number of solvers in the portfolio. Instead, scores have shape: { scores . shape } \" ) return scores [:, 0 ] - np . max ( scores [:, 1 :], axis = 1 ) runtime_score ( scores ) Runtime based metric. It tries to maximise the gap between the runing time of the target solver and the other solvers in the portfolio. Use this metric with exact solvers which provide the same objective values for an instance. Parameters: scores ( ndarray [ float ] ) \u2013 Scores of each solver over every instances. It is expected Returns: np.ndarray: Performance biases for every instance. Instance.p attribute. Source code in digneapy/_core/scores.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 def runtime_score ( scores : np . ndarray ) -> np . ndarray : \"\"\"Runtime based metric. It tries to maximise the gap between the runing time of the target solver and the other solvers in the portfolio. Use this metric with exact solvers which provide the same objective values for an instance. Args: scores (np.ndarray[float]): Scores of each solver over every instances. It is expected that the first value is the score of the target. Returns: np.ndarray: Performance biases for every instance. Instance.p attribute. \"\"\" return np . min ( scores [:, 1 :], axis = 1 ) - scores [:, 0 ]","title":"Scores"},{"location":"reference/_core/scores/#_core.scores.max_gap_target","text":"Maximum gap to target. It tries to maximise the gap between the target solver and the other solvers in the portfolio. Use this metric to generate instances that are EASY to solve by the target algorithm Parameters: scores ( ndarray [ float ] ) \u2013 Scores of each solver over every instances. It is expected Returns: ndarray \u2013 np.ndarray: Performance biases for every instance. Instance.p attribute. Source code in digneapy/_core/scores.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def max_gap_target ( scores : np . ndarray ) -> np . ndarray : \"\"\"Maximum gap to target. It tries to maximise the gap between the target solver and the other solvers in the portfolio. Use this metric to generate instances that are EASY to solve by the target algorithm Args: scores (np.ndarray[float]): Scores of each solver over every instances. It is expected that the first value is the score of the target. Returns: np.ndarray: Performance biases for every instance. Instance.p attribute. \"\"\" if scores . ndim != 2 : raise ValueError ( f \"Expected a 2d numpy array (i, s). Where `i` is the number of instances, `s` the number of solvers in the portfolio. Instead, scores have shape: { scores . shape } \" ) return scores [:, 0 ] - np . max ( scores [:, 1 :], axis = 1 )","title":"max_gap_target"},{"location":"reference/_core/scores/#_core.scores.runtime_score","text":"Runtime based metric. It tries to maximise the gap between the runing time of the target solver and the other solvers in the portfolio. Use this metric with exact solvers which provide the same objective values for an instance. Parameters: scores ( ndarray [ float ] ) \u2013 Scores of each solver over every instances. It is expected Returns: np.ndarray: Performance biases for every instance. Instance.p attribute. Source code in digneapy/_core/scores.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 def runtime_score ( scores : np . ndarray ) -> np . ndarray : \"\"\"Runtime based metric. It tries to maximise the gap between the runing time of the target solver and the other solvers in the portfolio. Use this metric with exact solvers which provide the same objective values for an instance. Args: scores (np.ndarray[float]): Scores of each solver over every instances. It is expected that the first value is the score of the target. Returns: np.ndarray: Performance biases for every instance. Instance.p attribute. \"\"\" return np . min ( scores [:, 1 :], axis = 1 ) - scores [:, 0 ]","title":"runtime_score"},{"location":"reference/_core/types/","text":"@File : types.py @Time : 2025/04/03 12:05:50 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2025, Alejandro Marrero @Desc : None RNG Bases: Protocol Protocol to type check all operators have _rng of instances types in digneapy Source code in digneapy/_core/types.py 18 19 20 21 22 23 24 25 26 class RNG ( Protocol ): \"\"\"Protocol to type check all operators have _rng of instances types in digneapy\"\"\" _rng : Generator _seed : int | None def initialize_rng ( self , seed : Optional [ int ] = None ): self . _seed = seed self . _rng = np . random . default_rng ()","title":"Types"},{"location":"reference/_core/types/#_core.types.RNG","text":"Bases: Protocol Protocol to type check all operators have _rng of instances types in digneapy Source code in digneapy/_core/types.py 18 19 20 21 22 23 24 25 26 class RNG ( Protocol ): \"\"\"Protocol to type check all operators have _rng of instances types in digneapy\"\"\" _rng : Generator _seed : int | None def initialize_rng ( self , seed : Optional [ int ] = None ): self . _seed = seed self . _rng = np . random . default_rng ()","title":"RNG"},{"location":"reference/archives/","text":"@File : init .py @Time : 2024/06/07 12:16:04 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2024, Alejandro Marrero @Desc : None Archive Class Archive Stores a collection of diverse Instances Source code in digneapy/archives/_base_archive.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 class Archive : \"\"\"Class Archive Stores a collection of diverse Instances \"\"\" def __init__ ( self , threshold : float , instances : Optional [ Sequence [ Instance ]] = None , dtype = np . float64 , ): \"\"\"Creates an instance of a Archive (unstructured) for QD algorithms Args: threshold (float): Minimum value of sparseness to include an Instance into the archive. instances (Iterable[Instance], optional): Instances to initialise the archive. Defaults to None. \"\"\" self . _storage = { \"instances\" : [], \"descriptors\" : []} if instances : self . _storage [ \"instances\" ] . extend ( instances ) self . _storage [ \"descriptors\" ] . extend ( np . asarray ([ instance . descriptor for instance in instances ]) ) self . _threshold = threshold self . _dtype = dtype @property def instances ( self ) -> Sequence [ Instance ]: return self . _storage [ \"instances\" ] @property def descriptors ( self ) -> np . ndarray : return np . asarray ( self . _storage [ \"descriptors\" ]) @property def threshold ( self ): return self . _threshold @threshold . setter def threshold ( self , t : float ): try : t_f = float ( t ) except Exception : msg = f \"The threshold value { t } is not a float in 'threshold' setter of class { self . __class__ . __name__ } \" raise TypeError ( msg ) self . _threshold = t_f def __iter__ ( self ): return iter ( self . _storage [ \"instances\" ]) def __str__ ( self ): return f \"Archive(threshold= { self . _threshold } ,data=(| { len ( self ) } |))\" def __repr__ ( self ): return f \"Archive(threshold= { self . _threshold } ,data=(| { len ( self ) } |))\" def __array__ ( self , dtype = None , copy = None ) -> np . ndarray : \"\"\"Creates a ndarray with the descriptors >>> import numpy as np >>> descriptors = [list(range(d, d + 5)) for d in range(10)] >>> archive = Archive(descriptors) >>> np_archive = np.array(archive) >>> assert len(np_archive) == len(archive) >>> assert type(np_archive) == type(np.zeros(1)) \"\"\" return np . asarray ( self . _storage [ \"instances\" ], dtype = dtype , copy = copy ) def __eq__ ( self , other : Self ): \"\"\"Compares whether to Archives are equal >>> import copy >>> variables = [list(range(d, d + 5)) for d in range(10)] >>> instances = [Instance(variables=v, s=1.0) for v in variables] >>> archive = Archive(threshold=0.0, instances=instances) >>> empty_archive = Archive(threshold=0.0) >>> a1 = copy.copy(archive) >>> assert a1 == archive >>> assert empty_archive != archive \"\"\" return len ( self ) == len ( other ) and all ( np . array_equal ( a , b ) for a , b in zip ( self . _storage [ \"descriptors\" ], other . _storage [ \"descriptors\" ]) ) def __hash__ ( self ): from functools import reduce hashes = ( hash ( i ) for i in self . instances ) return reduce ( lambda a , b : a ^ b , hashes , 0 ) def __bool__ ( self ): \"\"\"Returns True if len(self) > 1 >>> descriptors = [list(range(d, d + 5)) for d in range(10)] >>> archive = Archive(threshold=0.0, instances=descriptors) >>> empty_archive = Archive(threshold=0.0) >>> assert archive >>> assert not empty_archive \"\"\" return len ( self ) != 0 def __len__ ( self ): return len ( self . instances ) def __getitem__ ( self , key ): if isinstance ( key , slice ): cls = type ( self ) # To facilitate subclassing return cls ( self . _threshold , self . instances [ key ]) index = operator . index ( key ) return self . _storage [ \"instances\" ][ index ] def extend ( self , instances : Sequence [ Instance ], novelty_scores : Optional [ np . ndarray ] = None , descriptors : Optional [ np . ndarray ] = None , ): \"\"\"Extends the current archive with all the individuals inside iterable that have a sparseness value greater than the archive threshold. Args: instances (Sequence[Instance]): Sequence of instances to be include in the archive. \"\"\" scores = ( novelty_scores if novelty_scores is not None else np . asarray ([ instance . s for instance in instances ]) ) descriptors = ( descriptors if descriptors is not None else np . asarray ([ instance . descriptor for instance in instances ]) ) to_insert = np . where ( scores >= self . threshold )[ 0 ] self . _storage [ \"instances\" ] . extend (( instances [ i ] for i in to_insert )) self . _storage [ \"descriptors\" ] . extend ( descriptors [ to_insert ]) def __format__ ( self , fmt_spec = \"\" ): variables = self outer_fmt = \"( {} )\" components = ( format ( c , fmt_spec ) for c in variables ) return outer_fmt . format ( \", \" . join ( components )) def asdict ( self ) -> dict : return { \"threshold\" : self . _threshold , \"instances\" : { i : instance . asdict () for i , instance in enumerate ( self . _storage [ \"instances\" ]) }, } def to_json ( self ) -> str : \"\"\"Converts the archive into a JSON object Returns: str: JSON str of the archive content \"\"\" return json . dumps ( self . asdict (), indent = 4 ) __array__ ( dtype = None , copy = None ) Creates a ndarray with the descriptors import numpy as np descriptors = [list(range(d, d + 5)) for d in range(10)] archive = Archive(descriptors) np_archive = np.array(archive) assert len(np_archive) == len(archive) assert type(np_archive) == type(np.zeros(1)) Source code in digneapy/archives/_base_archive.py 81 82 83 84 85 86 87 88 89 90 91 def __array__ ( self , dtype = None , copy = None ) -> np . ndarray : \"\"\"Creates a ndarray with the descriptors >>> import numpy as np >>> descriptors = [list(range(d, d + 5)) for d in range(10)] >>> archive = Archive(descriptors) >>> np_archive = np.array(archive) >>> assert len(np_archive) == len(archive) >>> assert type(np_archive) == type(np.zeros(1)) \"\"\" return np . asarray ( self . _storage [ \"instances\" ], dtype = dtype , copy = copy ) __bool__ () Returns True if len(self) > 1 descriptors = [list(range(d, d + 5)) for d in range(10)] archive = Archive(threshold=0.0, instances=descriptors) empty_archive = Archive(threshold=0.0) assert archive assert not empty_archive Source code in digneapy/archives/_base_archive.py 117 118 119 120 121 122 123 124 125 126 127 def __bool__ ( self ): \"\"\"Returns True if len(self) > 1 >>> descriptors = [list(range(d, d + 5)) for d in range(10)] >>> archive = Archive(threshold=0.0, instances=descriptors) >>> empty_archive = Archive(threshold=0.0) >>> assert archive >>> assert not empty_archive \"\"\" return len ( self ) != 0 __eq__ ( other ) Compares whether to Archives are equal import copy variables = [list(range(d, d + 5)) for d in range(10)] instances = [Instance(variables=v, s=1.0) for v in variables] archive = Archive(threshold=0.0, instances=instances) empty_archive = Archive(threshold=0.0) a1 = copy.copy(archive) assert a1 == archive assert empty_archive != archive Source code in digneapy/archives/_base_archive.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def __eq__ ( self , other : Self ): \"\"\"Compares whether to Archives are equal >>> import copy >>> variables = [list(range(d, d + 5)) for d in range(10)] >>> instances = [Instance(variables=v, s=1.0) for v in variables] >>> archive = Archive(threshold=0.0, instances=instances) >>> empty_archive = Archive(threshold=0.0) >>> a1 = copy.copy(archive) >>> assert a1 == archive >>> assert empty_archive != archive \"\"\" return len ( self ) == len ( other ) and all ( np . array_equal ( a , b ) for a , b in zip ( self . _storage [ \"descriptors\" ], other . _storage [ \"descriptors\" ]) ) __init__ ( threshold , instances = None , dtype = np . float64 ) Creates an instance of a Archive (unstructured) for QD algorithms Parameters: threshold ( float ) \u2013 Minimum value of sparseness to include an Instance into the archive. instances ( Iterable [ Instance ] , default: None ) \u2013 Instances to initialise the archive. Defaults to None. Source code in digneapy/archives/_base_archive.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , threshold : float , instances : Optional [ Sequence [ Instance ]] = None , dtype = np . float64 , ): \"\"\"Creates an instance of a Archive (unstructured) for QD algorithms Args: threshold (float): Minimum value of sparseness to include an Instance into the archive. instances (Iterable[Instance], optional): Instances to initialise the archive. Defaults to None. \"\"\" self . _storage = { \"instances\" : [], \"descriptors\" : []} if instances : self . _storage [ \"instances\" ] . extend ( instances ) self . _storage [ \"descriptors\" ] . extend ( np . asarray ([ instance . descriptor for instance in instances ]) ) self . _threshold = threshold self . _dtype = dtype extend ( instances , novelty_scores = None , descriptors = None ) Extends the current archive with all the individuals inside iterable that have a sparseness value greater than the archive threshold. Parameters: instances ( Sequence [ Instance ] ) \u2013 Sequence of instances to be include in the archive. Source code in digneapy/archives/_base_archive.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def extend ( self , instances : Sequence [ Instance ], novelty_scores : Optional [ np . ndarray ] = None , descriptors : Optional [ np . ndarray ] = None , ): \"\"\"Extends the current archive with all the individuals inside iterable that have a sparseness value greater than the archive threshold. Args: instances (Sequence[Instance]): Sequence of instances to be include in the archive. \"\"\" scores = ( novelty_scores if novelty_scores is not None else np . asarray ([ instance . s for instance in instances ]) ) descriptors = ( descriptors if descriptors is not None else np . asarray ([ instance . descriptor for instance in instances ]) ) to_insert = np . where ( scores >= self . threshold )[ 0 ] self . _storage [ \"instances\" ] . extend (( instances [ i ] for i in to_insert )) self . _storage [ \"descriptors\" ] . extend ( descriptors [ to_insert ]) to_json () Converts the archive into a JSON object Returns: str ( str ) \u2013 JSON str of the archive content Source code in digneapy/archives/_base_archive.py 181 182 183 184 185 186 187 188 def to_json ( self ) -> str : \"\"\"Converts the archive into a JSON object Returns: str: JSON str of the archive content \"\"\" return json . dumps ( self . asdict (), indent = 4 ) CVTArchive Bases: GridArchive , RNG An Archive that divides a high-dimensional measure space into k homogeneous geometric regions. Based on the paper from Vassiliades et al (2018) https://ieeexplore.ieee.org/document/8000667 The computational complexity of the method we provide for constructing the CVT (in Algorithm 1) is O(ndki), where n is the number of d-dimensional samples to be clustered, k is the number of clusters, and i is the number of iterations needed until convergence Source code in digneapy/archives/_cvt_archive.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 class CVTArchive ( GridArchive , RNG ): \"\"\"An Archive that divides a high-dimensional measure space into k homogeneous geometric regions. Based on the paper from Vassiliades et al (2018) <https://ieeexplore.ieee.org/document/8000667> > The computational complexity of the method we provide for constructing the CVT (in Algorithm 1) is O(ndki), > where n is the number of d-dimensional samples to be clustered, k is the number of clusters, > and i is the number of iterations needed until convergence \"\"\" def __init__ ( self , k : int , ranges : Sequence [ Tuple [ float , float ]], n_samples : int , centroids : Optional [ npt . NDArray | str ] = None , samples : Optional [ npt . NDArray | str ] = None , dtype = np . float64 , seed : int = 42 , ): \"\"\"Creates a CVTArchive object Args: k (int): Number of centroids (regions) to create ranges (Sequence[Tuple[float, float]]): Ranges of the measure space. Upper and lower bound of each dimension of the measure space, e.g. ``[(-1, 1), (-2, 2)]`` indicates the first dimension should have bounds :math:`[-1,1]` (inclusive), and the second dimension should have bounds :math:`[-2,2]` (inclusive). The legnth of ``ranges`` indicates the number of dimensions of the measure space. n_samples (int): Number of samples to generate before calculating the centroids. centroids (Optional[npt.NDArray | str], optional): Precalculated centroids for the archive. The options are a np.ndarray with the values of ``k`` centroids or a .txt with the centroids to be loaded by Numpy. Defaults to None. samples (Optional[npt.NDArray | str], optional): Precalculated samples for the archive. The options are a np.ndarray with the values of ``n_samples`` samples or a .txt with the samples to be loaded by Numpy. Defaults to None. Raises: ValueError: If len(ranges) <= 0. ValueError: If the number of samples is less than zero or less than the number of regions (k). ValueError: If the number of regions is less than zero. ValueError: If the samples file cannot be loaded. ValueError: If given a samples np.ndarray the number of samples in the file is different from the number of expected samples (n_samples). ValueError: If the centroids file cannot be loaded. ValueError: If given a centroids np.ndarray the number of centroids in the file is different from the number of regions (k). \"\"\" if k <= 0 : raise ValueError ( f \"The number of regions (k = { k } ) must be >= 1\" ) if len ( ranges ) <= 0 : raise ValueError ( f \"ranges must have length >= 1 and it has length { len ( ranges ) } \" ) if n_samples <= 0 or n_samples < k : raise ValueError ( f \"The number of samples (n_samples = { n_samples } ) must be >= 1 and >= regions (k = { k } )\" ) GridArchive . __init__ ( self , dimensions = ( 1 ,) * len ( ranges ), ranges = ranges , dtype = dtype ) self . _dimensions = len ( ranges ) ranges = list ( zip ( * ranges )) self . _lower_bounds = np . array ( ranges [ 0 ], dtype = self . _dtype ) self . _upper_bounds = np . array ( ranges [ 1 ], dtype = self . _dtype ) self . _interval = self . _upper_bounds - self . _lower_bounds self . _k = k self . _n_samples = n_samples self . _samples = None self . _centroids = None self . initialize_rng ( seed = seed ) self . _kmeans = KMeans ( n_clusters = self . _k , n_init = 1 , random_state = self . _seed ) # Loading samples if given if samples is not None : if isinstance ( samples , str ): try : self . _samples = np . load ( samples ) self . _n_samples = len ( self . _samples ) except Exception as _ : raise ValueError ( f \"Error in CVTArchive.__init__() loading the samples file { samples } .\" ) elif isinstance ( samples , np . ndarray ) and len ( samples ) != n_samples : raise ValueError ( f \"The number of samples { len ( samples ) } must be equal to the number of expected samples (n_samples = { n_samples } )\" ) else : self . _samples = np . asarray ( samples ) if centroids is not None : if isinstance ( centroids , str ): try : self . _centroids = np . load ( centroids ) self . _k = len ( self . _centroids ) except Exception as _ : raise ValueError ( f \"Error in CVTArchive.__init__() loading the centroids file { centroids } .\" ) elif isinstance ( centroids , np . ndarray ) and len ( centroids ) != k : raise ValueError ( f \"The number of centroids { len ( centroids ) } must be equal to the number of regions (k = { self . _k } )\" ) else : self . _centroids = np . asarray ( centroids ) else : # Generate centroids if self . _samples is None : # Generate uniform samples if not given rng = np . random . default_rng ( seed = self . _seed ) self . _samples = rng . uniform ( low = self . _lower_bounds , high = self . _upper_bounds , size = ( self . _n_samples , self . _dimensions ), ) self . _kmeans . fit ( self . _samples ) self . _centroids = self . _kmeans . cluster_centers_ self . _kdtree = KDTree ( self . _centroids , metric = \"euclidean\" ) @property def dimensions ( self ) -> int : \"\"\"Dimensions of the measure space used Returns: int: Dimensions of the measure space used \"\"\" return self . _dimensions @property def samples ( self ) -> np . ndarray : \"\"\"Returns the samples used to generate the centroids Returns: np.ndarray: Samples \"\"\" return self . _samples @property def centroids ( self ) -> np . ndarray : \"\"\"Returns k centroids calculated from the samples Returns: np.ndarray: K d-dimensional centroids \"\"\" return self . _centroids @property def regions ( self ) -> int : \"\"\"Number of regions (k) of centroids in the CVTArchive Returns: int: k \"\"\" return self . _k @property def bounds ( self ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Tuple with the lower and upper bounds of the measure space The first value is the lower bounds and the second value is the upper bounds. Each value is a list with the corresponding lower/upper bound of the ith dimension in the measure space \"\"\" return ( self . _lower_bounds , self . _upper_bounds ) @property def instances ( self ) -> list [ Instance ]: return list ( self . _storage . values ()) def __str__ ( self ): return f \"CVArchive(dim= { self . _dimensions } ,regions= { self . _k } ,centroids= { self . _centroids } )\" def __repr__ ( self ): return f \"CVArchive(dim= { self . _dimensions } ,regions= { self . _k } ,centroids= { self . _centroids } )\" def __iter__ ( self ): \"\"\"Iterates over the dictionary of instances Returns: Iterator: Yields position in the hypercube and instance located in such position \"\"\" return iter ( self . _storage . values ()) def lower_i ( self , i ) -> np . float64 : if i < 0 or i > len ( self . _lower_bounds ): msg = f \"index { i } is out of bounds. Valid values are [0- { len ( self . _lower_bounds ) } ]\" raise ValueError ( msg ) return self . _lower_bounds [ i ] def upper_i ( self , i ) -> np . float64 : if i < 0 or i > len ( self . _upper_bounds ): msg = f \"index { i } is out of bounds. Valid values are [0- { len ( self . _upper_bounds ) } ]\" raise ValueError ( msg ) return self . _upper_bounds [ i ] def index_of ( self , descriptors ) -> np . ndarray : \"\"\"Computes the indeces of a batch of descriptors. Args: descriptors (array-like): (batch_size, dimensions) array of descriptors for each instance Raises: ValueError: ``descriptors`` is not shape (batch_size, dimensions) Returns: np.ndarray: (batch_size, ) array of integer indices representing the flattened grid coordinates. \"\"\" descriptors = np . array ( descriptors ) if len ( descriptors ) == 0 : return np . empty ( 0 ) elif ( descriptors . ndim == 1 and descriptors . shape [ 0 ] != self . _dimensions or descriptors . ndim == 2 and descriptors . shape [ 1 ] != self . _dimensions ): raise ValueError ( f \"Expected descriptors to be an array with shape \" f \"(batch_size, dimensions) (i.e. shape \" f \"(batch_size, { self . _dimensions } )) but it had shape \" f \" { descriptors . shape } \" ) indices = self . _kdtree . query ( descriptors , return_distance = False ) indices = indices [:, 0 ] return indices . astype ( np . int32 ) def to_file ( self , file_pattern : str = \"CVTArchive\" ): \"\"\"Saves the centroids and the samples of the CVTArchive to .npy files Each attribute is saved in its own filename. Therefore, file_pattern is expected not to contain any extension Args: file_pattern (str, optional): Pattern of the expected filenames. Defaults to \"CVTArchive\". \"\"\" np . save ( f \" { file_pattern } _centroids.npy\" , self . _centroids ) np . save ( f \" { file_pattern } _samples.npy\" , self . _samples ) @classmethod def load_from_json ( cls , filename : str ): \"\"\"Creates a CVTArchive object from the content of a previously created JSON file Args: filename (str): Filename of the JSON file with the CVTArchive information Raises: ValueError: If there's any error while loading the file. (IOError) ValueError: If the JSON file does not contain all the expected keys Returns: Self: Returns a CVTArchive object \"\"\" expected_keys = { \"dimensions\" , \"n_samples\" , \"regions\" , \"lbs\" , \"ubs\" , \"centroids\" , \"samples\" , } try : with open ( filename , \"r\" ) as file : json_data = json . load ( file ) if expected_keys != json_data . keys (): raise ValueError ( f \"The JSON file does not contain all the minimum expected keys. Expected keys are { expected_keys } and got { json_data . keys () } \" ) _ranges = [ ( l_i , u_i ) for l_i , u_i in zip ( json_data [ \"lbs\" ], json_data [ \"ubs\" ]) ] new_archive = cls ( k = json_data [ \"regions\" ], ranges = _ranges , n_samples = json_data [ \"n_samples\" ], centroids = json_data [ \"centroids\" ], samples = json_data [ \"samples\" ], ) return new_archive except IOError as io : raise ValueError ( f \"Error opening file { filename } . Reason -> { io . strerror } \" ) def asdict ( self ) -> dict : return { \"dimensions\" : self . _dimensions , \"n_samples\" : self . _n_samples , \"regions\" : self . _k , \"lbs\" : self . _lower_bounds . tolist (), \"ubs\" : self . _upper_bounds . tolist (), \"centroids\" : self . _centroids . tolist (), \"samples\" : self . _samples . tolist (), \"instances\" : { i : instance . asdict () for i , instance in enumerate ( self . _storage . values ()) }, } def to_json ( self , filename : Optional [ str ] = None ) -> str : \"\"\"Returns the content of the CVTArchive in JSON format. Returns: str: String in JSON format with the content of the CVTArchive \"\"\" json_data = json . dumps ( self . asdict (), indent = 4 ) if filename is not None : filename = ( f \" { filename } .json\" if not filename . endswith ( \".json\" ) else filename ) with open ( filename , \"w\" ) as f : f . write ( json_data ) return json_data bounds property Tuple with the lower and upper bounds of the measure space The first value is the lower bounds and the second value is the upper bounds. Each value is a list with the corresponding lower/upper bound of the ith dimension in the measure space centroids property Returns k centroids calculated from the samples Returns: ndarray \u2013 np.ndarray: K d-dimensional centroids dimensions property Dimensions of the measure space used Returns: int ( int ) \u2013 Dimensions of the measure space used regions property Number of regions (k) of centroids in the CVTArchive Returns: int ( int ) \u2013 k samples property Returns the samples used to generate the centroids Returns: ndarray \u2013 np.ndarray: Samples __init__ ( k , ranges , n_samples , centroids = None , samples = None , dtype = np . float64 , seed = 42 ) Creates a CVTArchive object Parameters: k ( int ) \u2013 Number of centroids (regions) to create ranges ( Sequence [ Tuple [ float , float ]] ) \u2013 Ranges of the measure space. Upper and lower bound of each indicates the first dimension should have bounds \u2013 math: [-1,1] \u2013 math: [-2,2] (inclusive). The legnth of ranges indicates the number of dimensions of the measure space. n_samples ( int ) \u2013 Number of samples to generate before calculating the centroids. centroids ( Optional [ NDArray | str ] , default: None ) \u2013 Precalculated centroids for the archive. samples ( Optional [ NDArray | str ] , default: None ) \u2013 Precalculated samples for the archive. Raises: ValueError \u2013 If len(ranges) <= 0. ValueError \u2013 If the number of samples is less than zero or less than the number of regions (k). ValueError \u2013 If the number of regions is less than zero. ValueError \u2013 If the samples file cannot be loaded. ValueError \u2013 If given a samples np.ndarray the number of samples in the file is different from the number of expected samples (n_samples). ValueError \u2013 If the centroids file cannot be loaded. ValueError \u2013 If given a centroids np.ndarray the number of centroids in the file is different from the number of regions (k). Source code in digneapy/archives/_cvt_archive.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def __init__ ( self , k : int , ranges : Sequence [ Tuple [ float , float ]], n_samples : int , centroids : Optional [ npt . NDArray | str ] = None , samples : Optional [ npt . NDArray | str ] = None , dtype = np . float64 , seed : int = 42 , ): \"\"\"Creates a CVTArchive object Args: k (int): Number of centroids (regions) to create ranges (Sequence[Tuple[float, float]]): Ranges of the measure space. Upper and lower bound of each dimension of the measure space, e.g. ``[(-1, 1), (-2, 2)]`` indicates the first dimension should have bounds :math:`[-1,1]` (inclusive), and the second dimension should have bounds :math:`[-2,2]` (inclusive). The legnth of ``ranges`` indicates the number of dimensions of the measure space. n_samples (int): Number of samples to generate before calculating the centroids. centroids (Optional[npt.NDArray | str], optional): Precalculated centroids for the archive. The options are a np.ndarray with the values of ``k`` centroids or a .txt with the centroids to be loaded by Numpy. Defaults to None. samples (Optional[npt.NDArray | str], optional): Precalculated samples for the archive. The options are a np.ndarray with the values of ``n_samples`` samples or a .txt with the samples to be loaded by Numpy. Defaults to None. Raises: ValueError: If len(ranges) <= 0. ValueError: If the number of samples is less than zero or less than the number of regions (k). ValueError: If the number of regions is less than zero. ValueError: If the samples file cannot be loaded. ValueError: If given a samples np.ndarray the number of samples in the file is different from the number of expected samples (n_samples). ValueError: If the centroids file cannot be loaded. ValueError: If given a centroids np.ndarray the number of centroids in the file is different from the number of regions (k). \"\"\" if k <= 0 : raise ValueError ( f \"The number of regions (k = { k } ) must be >= 1\" ) if len ( ranges ) <= 0 : raise ValueError ( f \"ranges must have length >= 1 and it has length { len ( ranges ) } \" ) if n_samples <= 0 or n_samples < k : raise ValueError ( f \"The number of samples (n_samples = { n_samples } ) must be >= 1 and >= regions (k = { k } )\" ) GridArchive . __init__ ( self , dimensions = ( 1 ,) * len ( ranges ), ranges = ranges , dtype = dtype ) self . _dimensions = len ( ranges ) ranges = list ( zip ( * ranges )) self . _lower_bounds = np . array ( ranges [ 0 ], dtype = self . _dtype ) self . _upper_bounds = np . array ( ranges [ 1 ], dtype = self . _dtype ) self . _interval = self . _upper_bounds - self . _lower_bounds self . _k = k self . _n_samples = n_samples self . _samples = None self . _centroids = None self . initialize_rng ( seed = seed ) self . _kmeans = KMeans ( n_clusters = self . _k , n_init = 1 , random_state = self . _seed ) # Loading samples if given if samples is not None : if isinstance ( samples , str ): try : self . _samples = np . load ( samples ) self . _n_samples = len ( self . _samples ) except Exception as _ : raise ValueError ( f \"Error in CVTArchive.__init__() loading the samples file { samples } .\" ) elif isinstance ( samples , np . ndarray ) and len ( samples ) != n_samples : raise ValueError ( f \"The number of samples { len ( samples ) } must be equal to the number of expected samples (n_samples = { n_samples } )\" ) else : self . _samples = np . asarray ( samples ) if centroids is not None : if isinstance ( centroids , str ): try : self . _centroids = np . load ( centroids ) self . _k = len ( self . _centroids ) except Exception as _ : raise ValueError ( f \"Error in CVTArchive.__init__() loading the centroids file { centroids } .\" ) elif isinstance ( centroids , np . ndarray ) and len ( centroids ) != k : raise ValueError ( f \"The number of centroids { len ( centroids ) } must be equal to the number of regions (k = { self . _k } )\" ) else : self . _centroids = np . asarray ( centroids ) else : # Generate centroids if self . _samples is None : # Generate uniform samples if not given rng = np . random . default_rng ( seed = self . _seed ) self . _samples = rng . uniform ( low = self . _lower_bounds , high = self . _upper_bounds , size = ( self . _n_samples , self . _dimensions ), ) self . _kmeans . fit ( self . _samples ) self . _centroids = self . _kmeans . cluster_centers_ self . _kdtree = KDTree ( self . _centroids , metric = \"euclidean\" ) __iter__ () Iterates over the dictionary of instances Returns: Iterator \u2013 Yields position in the hypercube and instance located in such position Source code in digneapy/archives/_cvt_archive.py 201 202 203 204 205 206 207 def __iter__ ( self ): \"\"\"Iterates over the dictionary of instances Returns: Iterator: Yields position in the hypercube and instance located in such position \"\"\" return iter ( self . _storage . values ()) index_of ( descriptors ) Computes the indeces of a batch of descriptors. Parameters: descriptors ( array - like ) \u2013 (batch_size, dimensions) array of descriptors for each instance Raises: ValueError \u2013 descriptors is not shape (batch_size, dimensions) Returns: ndarray \u2013 np.ndarray: (batch_size, ) array of integer indices representing the flattened grid coordinates. Source code in digneapy/archives/_cvt_archive.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 def index_of ( self , descriptors ) -> np . ndarray : \"\"\"Computes the indeces of a batch of descriptors. Args: descriptors (array-like): (batch_size, dimensions) array of descriptors for each instance Raises: ValueError: ``descriptors`` is not shape (batch_size, dimensions) Returns: np.ndarray: (batch_size, ) array of integer indices representing the flattened grid coordinates. \"\"\" descriptors = np . array ( descriptors ) if len ( descriptors ) == 0 : return np . empty ( 0 ) elif ( descriptors . ndim == 1 and descriptors . shape [ 0 ] != self . _dimensions or descriptors . ndim == 2 and descriptors . shape [ 1 ] != self . _dimensions ): raise ValueError ( f \"Expected descriptors to be an array with shape \" f \"(batch_size, dimensions) (i.e. shape \" f \"(batch_size, { self . _dimensions } )) but it had shape \" f \" { descriptors . shape } \" ) indices = self . _kdtree . query ( descriptors , return_distance = False ) indices = indices [:, 0 ] return indices . astype ( np . int32 ) load_from_json ( filename ) classmethod Creates a CVTArchive object from the content of a previously created JSON file Parameters: filename ( str ) \u2013 Filename of the JSON file with the CVTArchive information Raises: ValueError \u2013 If there's any error while loading the file. (IOError) ValueError \u2013 If the JSON file does not contain all the expected keys Returns: Self \u2013 Returns a CVTArchive object Source code in digneapy/archives/_cvt_archive.py 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 @classmethod def load_from_json ( cls , filename : str ): \"\"\"Creates a CVTArchive object from the content of a previously created JSON file Args: filename (str): Filename of the JSON file with the CVTArchive information Raises: ValueError: If there's any error while loading the file. (IOError) ValueError: If the JSON file does not contain all the expected keys Returns: Self: Returns a CVTArchive object \"\"\" expected_keys = { \"dimensions\" , \"n_samples\" , \"regions\" , \"lbs\" , \"ubs\" , \"centroids\" , \"samples\" , } try : with open ( filename , \"r\" ) as file : json_data = json . load ( file ) if expected_keys != json_data . keys (): raise ValueError ( f \"The JSON file does not contain all the minimum expected keys. Expected keys are { expected_keys } and got { json_data . keys () } \" ) _ranges = [ ( l_i , u_i ) for l_i , u_i in zip ( json_data [ \"lbs\" ], json_data [ \"ubs\" ]) ] new_archive = cls ( k = json_data [ \"regions\" ], ranges = _ranges , n_samples = json_data [ \"n_samples\" ], centroids = json_data [ \"centroids\" ], samples = json_data [ \"samples\" ], ) return new_archive except IOError as io : raise ValueError ( f \"Error opening file { filename } . Reason -> { io . strerror } \" ) to_file ( file_pattern = 'CVTArchive' ) Saves the centroids and the samples of the CVTArchive to .npy files Each attribute is saved in its own filename. Therefore, file_pattern is expected not to contain any extension Parameters: file_pattern ( str , default: 'CVTArchive' ) \u2013 Pattern of the expected filenames. Defaults to \"CVTArchive\". Source code in digneapy/archives/_cvt_archive.py 254 255 256 257 258 259 260 261 262 263 def to_file ( self , file_pattern : str = \"CVTArchive\" ): \"\"\"Saves the centroids and the samples of the CVTArchive to .npy files Each attribute is saved in its own filename. Therefore, file_pattern is expected not to contain any extension Args: file_pattern (str, optional): Pattern of the expected filenames. Defaults to \"CVTArchive\". \"\"\" np . save ( f \" { file_pattern } _centroids.npy\" , self . _centroids ) np . save ( f \" { file_pattern } _samples.npy\" , self . _samples ) to_json ( filename = None ) Returns the content of the CVTArchive in JSON format. Returns: str ( str ) \u2013 String in JSON format with the content of the CVTArchive Source code in digneapy/archives/_cvt_archive.py 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 def to_json ( self , filename : Optional [ str ] = None ) -> str : \"\"\"Returns the content of the CVTArchive in JSON format. Returns: str: String in JSON format with the content of the CVTArchive \"\"\" json_data = json . dumps ( self . asdict (), indent = 4 ) if filename is not None : filename = ( f \" { filename } .json\" if not filename . endswith ( \".json\" ) else filename ) with open ( filename , \"w\" ) as f : f . write ( json_data ) return json_data GridArchive Bases: Archive An archive that divides each dimension into a uniformly-sized cells. The source code of this class is inspired by the GridArchive class of pyribs https://github.com/icaros-usc/pyribs/blob/master/ribs/archives/_grid_archive.py This archive is the container described in Mouret 2015 <https://arxiv.org/pdf/1504.04909.pdf> _. It can be visualized as an n-dimensional grid in the measure space that is divided into a certain number of cells in each dimension. Each cell contains an elite, i.e. a solution that maximizes the objective function for the measures in that cell. Source code in digneapy/archives/_grid_archive.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 class GridArchive ( Archive ): \"\"\"An archive that divides each dimension into a uniformly-sized cells. The source code of this class is inspired by the GridArchive class of pyribs <https://github.com/icaros-usc/pyribs/blob/master/ribs/archives/_grid_archive.py> This archive is the container described in `Mouret 2015 <https://arxiv.org/pdf/1504.04909.pdf>`_. It can be visualized as an n-dimensional grid in the measure space that is divided into a certain number of cells in each dimension. Each cell contains an elite, i.e. a solution that `maximizes` the objective function for the measures in that cell. \"\"\" def __init__ ( self , dimensions : Sequence [ int ], ranges : Sequence [ Tuple [ float , float ]], instances : Optional [ Iterable [ Instance ]] = None , eps : float = 1e-6 , dtype = np . float64 , ): \"\"\"Creates a GridArchive instance Args: dimensions (Sequence[int]): (array-like of int): Number of cells in each dimension of the measure space, e.g. ``[20, 30, 40]`` indicates there should be 3 dimensions with 20, 30, and 40 cells. (The number of dimensions is implicitly defined in the length of this argument). ranges (Sequence[Tuple[float]]): (array-like of (float, float)): Upper and lower bound of each dimension of the measure space, e.g. ``[(-1, 1), (-2, 2)]`` indicates the first dimension should have bounds :math:`[-1,1]` (inclusive), and the second dimension should have bounds :math:`[-2,2]` (inclusive). ``ranges`` should be the same length as ``dims``. instances (Optional[Iterable[Instance]], optional): Instances to pre-initialise the archive. Defaults to None. eps (float, optional): Due to floating point precision errors, we add a small epsilon when computing the archive indices in the :meth:`index_of` method -- refer to the implementation `here. Defaults to 1e-6. dtype(str or data-type): Data type of the solutions, objectives, and measures. Raises: ValueError: ``dimensions`` and ``ranges`` are not the same length \"\"\" Archive . __init__ ( self , threshold = np . finfo ( np . float32 ) . max , dtype = dtype ) if len ( ranges ) == 0 or len ( dimensions ) == 0 : raise ValueError ( \"dimensions and ranges must have length >= 1\" ) if len ( ranges ) != len ( dimensions ): raise ValueError ( f \"len(dimensions) = { len ( dimensions ) } != len(ranges) = { len ( ranges ) } in GridArchive.__init__()\" ) self . _dimensions = np . asarray ( dimensions ) ranges = list ( zip ( * ranges )) self . _lower_bounds = np . array ( ranges [ 0 ], dtype = dtype ) self . _upper_bounds = np . array ( ranges [ 1 ], dtype = dtype ) self . _interval = self . _upper_bounds - self . _lower_bounds self . _eps = eps self . _cells = np . prod ( self . _dimensions , dtype = object ) self . _grid : Dict [ int , np . ndarray ] = {} self . _storage : Dict [ int , Instance ] = {} _bounds = [] for dimension , l_b , u_b in zip ( self . _dimensions , self . _lower_bounds , self . _upper_bounds ): _bounds . append ( np . linspace ( l_b , u_b , dimension )) self . _boundaries = np . asarray ( _bounds ) if instances is not None : self . extend ( instances ) @property def dimensions ( self ): return self . _dimensions @property def bounds ( self ): \"\"\"list of numpy.ndarray: The boundaries of the cells in each dimension. Entry ``i`` in this list is an array that contains the boundaries of the cells in dimension ``i``. The array contains ``self.dims[i] + 1`` entries laid out like this:: Archive cells: | 0 | 1 | ... | self.dims[i] | boundaries[i]: 0 1 2 self.dims[i] - 1 self.dims[i] Thus, ``boundaries[i][j]`` and ``boundaries[i][j + 1]`` are the lower and upper bounds of cell ``j`` in dimension ``i``. To access the lower bounds of all the cells in dimension ``i``, use ``boundaries[i][:-1]``, and to access all the upper bounds, use ``boundaries[i][1:]``. \"\"\" return self . _boundaries @property def n_cells ( self ): return self . _cells @property def coverage ( self ): \"\"\"Get the coverage of the hypercube space. The coverage is calculated has the number of cells filled over the total space available. Returns: float: Filled cells over the total available. \"\"\" if len ( self . _grid ) == 0 : return 0.0 return len ( self . _grid ) / self . _cells @property def filled_cells ( self ): return self . _grid . keys () @property def instances ( self ) -> Sequence [ Instance ]: return list ( self . _storage . values ()) def __str__ ( self ): return f \"GridArchive(dim= { self . _dimensions } ,cells= { self . _cells } ,bounds= { self . _boundaries } )\" def __repr__ ( self ): return f \"GridArchive(dim= { self . _dimensions } ,cells= { self . _cells } ,bounds= { self . _boundaries } )\" def __len__ ( self ): return len ( self . _grid ) def __getitem__ ( self , key ): \"\"\"Returns a dictionary with the descriptors as the keys. The values are the instances found. Note that some of the given keys may not be in the archive. Args: key (array-like or descriptor): Descriptors of the instances that want to retrieve. Valid examples are: - archive[[0,11], [0,5]] --> Get the instances with the descriptors (0,11) and (0, 5) - archive[0,11] --> Get the instances at indices 0 and 11 Raises: TypeError: If the key is an slice. Not allowed. ValueError: If the shape of the keys are not valid. Returns: dict: Returns a dict with the found instances. \"\"\" if isinstance ( key , slice ): raise TypeError ( \"Slicing is not available in GridArchive. Use 1D index or descriptor-type indices\" ) descriptors = np . asarray ( key ) if descriptors . ndim == 1 : indices = descriptors elif descriptors . ndim == 2 and descriptors . shape [ 1 ] == len ( self . _dimensions ): indices = self . index_of ( descriptors ) . tolist () else : raise ValueError ( f \"Expected descriptors to be an array with shape \" f \"(batch_size, dimensions) (i.e. shape \" f \"(batch_size, { len ( self . _dimensions ) } )) but it had shape \" f \" { descriptors . shape } \" ) if isinstance ( indices , int ): indices = [ indices ] instances = [ self . _storage [ idx ] for idx in indices ] return instances def __iter__ ( self ): \"\"\"Iterates over the dictionary of instances Returns: Iterator: Yields position in the hypercube and instance located in such position \"\"\" return iter ( self . _storage . values ()) def lower_i ( self , i ): if i < 0 or i > len ( self . _lower_bounds ): msg = f \"index { i } is out of bounds. Valid values are [0- { len ( self . _boundaries ) } ]\" raise ValueError ( msg ) return self . _lower_bounds [ i ] def upper_i ( self , i ): if i < 0 or i > len ( self . _upper_bounds ): msg = f \"index { i } is out of bounds. Valid values are [0- { len ( self . _boundaries ) } ]\" raise ValueError ( msg ) return self . _upper_bounds [ i ] def append ( self , instance : Instance , descriptor : Optional [ np . ndarray ] = None ): \"\"\"Inserts an Instance into the Grid Args: instance (Instance): Instace to be inserted Raises: TypeError: ``instance`` is not a instance of the class Instance. \"\"\" if not isinstance ( instance , Instance ): msg = \"Only objects of type Instance can be inserted into a GridArchive\" raise TypeError ( msg ) descriptor = ( np . asarray ( instance . descriptor ) if descriptor is None else descriptor ) index = self . index_of ([ descriptor ])[ 0 ] if index not in self . _grid or instance > self . _grid [ index ]: self . _grid [ index ] = descriptor self . _storage [ index ] = instance . clone () def extend ( self , instances : Sequence [ Instance ], descriptors : Optional [ np . ndarray ] = None , * args , ** kwargs , ): \"\"\"Includes all the instances in iterable into the Grid Args: iterable (Iterable[Instance]): Iterable of instances \"\"\" if not all ( isinstance ( i , Instance ) for i in instances ): msg = \"Only objects of type Instance can be inserted into a GridArchive\" raise TypeError ( msg ) if descriptors is None : try : descriptors = np . asarray ([ i . descriptor for i in instances ]) except AttributeError as e : print ( \"Instances do not have a descriptor yet and the value descriptor is None\" ) raise ( e ) indices = self . index_of ( descriptors ) for idx , instance , descriptor in zip ( indices , instances , descriptors , strict = True ): if idx not in self . _grid or instance . fitness > self . _storage [ idx ] . fitness : self . _storage [ idx ] = instance . clone () self . _grid [ idx ] = descriptor def remove ( self , descriptors : np . ndarray ): \"\"\"Removes all the instances with the matching descriptors in iterable from the grid\"\"\" indices_to_remove = self . index_of ( descriptors ) for index in indices_to_remove : if index in self . _grid : del self . _grid [ index ] del self . _storage [ index ] def purge_unfeasible ( self , attr : str = \"p\" ): \"\"\"Removes all the unfeasible instances from the grid\"\"\" keys_to_remove = [ i for i in self . _storage . keys () if getattr ( self . _storage [ i ], attr ) < 0 ] for i in keys_to_remove : del self . _grid [ i ] del self . _storage [ i ] def index_of ( self , descriptors ): \"\"\"Computes the indices of a batch of descriptors. Args: descriptors (array-like): (batch_size, dimensions) array of descriptors for each instance Raises: ValueError: ``descriptors`` is not shape (batch_size, dimensions) Returns: np.ndarray: (batch_size, ) array of integer indices representing the flattened grid coordinates. \"\"\" if len ( descriptors ) == 0 : return np . empty ( 0 ) descriptors = np . asarray ( descriptors ) if ( descriptors . ndim == 1 and descriptors . shape [ 0 ] != len ( self . _dimensions ) or descriptors . ndim == 2 and descriptors . shape [ 1 ] != len ( self . _dimensions ) ): raise ValueError ( f \"Expected descriptors to be an array with shape \" f \"(batch_size, dimensions) (i.e. shape \" f \"(batch_size, { len ( self . _dimensions ) } )) but it had shape \" f \" { descriptors . shape } \" ) grid_indices = ( ( self . _dimensions * ( descriptors - self . _lower_bounds ) + self . _eps ) / self . _interval ) . astype ( int ) # Clip the indexes to make sure they are in the expected range for each dimension clipped = np . clip ( grid_indices , 0 , self . _dimensions - 1 ) return self . _grid_to_int_index ( clipped ) def _grid_to_int_index ( self , grid_indices ) -> np . ndarray : grid_indices = np . asarray ( grid_indices ) if len ( self . _dimensions ) > 64 : strides = np . cumprod (( 1 ,) + tuple ( self . _dimensions [:: - 1 ][: - 1 ]))[:: - 1 ] # Reshape strides to (1, num_dimensions) to make it broadcastable with grid_indices strides = strides . reshape ( 1 , - 1 ) flattened_indices = np . sum ( grid_indices * strides , axis = 1 , dtype = object ) else : flattened_indices = np . ravel_multi_index ( grid_indices . T , self . _dimensions ) . astype ( int ) return flattened_indices def int_to_grid_index ( self , int_indices ) -> np . ndarray : int_indices = np . asarray ( int_indices ) if len ( self . _dimensions ) > 64 : # Manually unravel the index for dimensions > 64 unravel_indices = [] remaining_indices = int_indices . astype ( object ) for dim_size in self . _dimensions [:: - 1 ]: unravel_indices . append ( remaining_indices % dim_size ) remaining_indices //= dim_size unravel_indices = np . array ( unravel_indices [:: - 1 ]) . T else : unravel_indices = np . asarray ( np . unravel_index ( int_indices , self . _dimensions , ) ) . T . astype ( int ) return unravel_indices def asdict ( self ) -> dict : return { \"dimensions\" : self . _dimensions . tolist (), \"lbs\" : self . _lower_bounds . tolist (), \"ubs\" : self . _upper_bounds . tolist (), \"n_cells\" : self . _cells , \"instances\" : { i : instance . asdict () for i , instance in enumerate ( self . _storage . values ()) }, } def to_json ( self ) -> str : return json . dumps ( self . asdict (), indent = 4 ) bounds property list of numpy.ndarray: The boundaries of the cells in each dimension. Entry i in this list is an array that contains the boundaries of the cells in dimension i . The array contains self.dims[i] + 1 entries laid out like this:: Archive cells: | 0 | 1 | ... | self.dims[i] | boundaries[i]: 0 1 2 self.dims[i] - 1 self.dims[i] Thus, boundaries[i][j] and boundaries[i][j + 1] are the lower and upper bounds of cell j in dimension i . To access the lower bounds of all the cells in dimension i , use boundaries[i][:-1] , and to access all the upper bounds, use boundaries[i][1:] . coverage property Get the coverage of the hypercube space. The coverage is calculated has the number of cells filled over the total space available. Returns: float \u2013 Filled cells over the total available. __getitem__ ( key ) Returns a dictionary with the descriptors as the keys. The values are the instances found. Note that some of the given keys may not be in the archive. Parameters: key ( array - like or descriptor ) \u2013 Descriptors of the instances that want to retrieve. Valid examples are \u2013 Raises: TypeError \u2013 If the key is an slice. Not allowed. ValueError \u2013 If the shape of the keys are not valid. Returns: dict \u2013 Returns a dict with the found instances. Source code in digneapy/archives/_grid_archive.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def __getitem__ ( self , key ): \"\"\"Returns a dictionary with the descriptors as the keys. The values are the instances found. Note that some of the given keys may not be in the archive. Args: key (array-like or descriptor): Descriptors of the instances that want to retrieve. Valid examples are: - archive[[0,11], [0,5]] --> Get the instances with the descriptors (0,11) and (0, 5) - archive[0,11] --> Get the instances at indices 0 and 11 Raises: TypeError: If the key is an slice. Not allowed. ValueError: If the shape of the keys are not valid. Returns: dict: Returns a dict with the found instances. \"\"\" if isinstance ( key , slice ): raise TypeError ( \"Slicing is not available in GridArchive. Use 1D index or descriptor-type indices\" ) descriptors = np . asarray ( key ) if descriptors . ndim == 1 : indices = descriptors elif descriptors . ndim == 2 and descriptors . shape [ 1 ] == len ( self . _dimensions ): indices = self . index_of ( descriptors ) . tolist () else : raise ValueError ( f \"Expected descriptors to be an array with shape \" f \"(batch_size, dimensions) (i.e. shape \" f \"(batch_size, { len ( self . _dimensions ) } )) but it had shape \" f \" { descriptors . shape } \" ) if isinstance ( indices , int ): indices = [ indices ] instances = [ self . _storage [ idx ] for idx in indices ] return instances __init__ ( dimensions , ranges , instances = None , eps = 1e-06 , dtype = np . float64 ) Creates a GridArchive instance Parameters: dimensions ( Sequence [ int ] ) \u2013 (array-like of int): Number of cells in each dimension of the ranges ( Sequence [ Tuple [ float ]] ) \u2013 (array-like of (float, float)): Upper and lower bound of each indicates the first dimension should have bounds \u2013 math: [-1,1] \u2013 math: [-2,2] (inclusive). ranges should be the same length as instances ( Optional [ Iterable [ Instance ]] , default: None ) \u2013 Instances to pre-initialise the archive. Defaults to None. eps ( float , default: 1e-06 ) \u2013 Due to floating point precision errors, we add a small epsilon when computing the archive indices in the \u2013 meth: index_of dtype ( str or data - type , default: float64 ) \u2013 Data type of the solutions, objectives, Raises: ValueError \u2013 dimensions and ranges are not the same length Source code in digneapy/archives/_grid_archive.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def __init__ ( self , dimensions : Sequence [ int ], ranges : Sequence [ Tuple [ float , float ]], instances : Optional [ Iterable [ Instance ]] = None , eps : float = 1e-6 , dtype = np . float64 , ): \"\"\"Creates a GridArchive instance Args: dimensions (Sequence[int]): (array-like of int): Number of cells in each dimension of the measure space, e.g. ``[20, 30, 40]`` indicates there should be 3 dimensions with 20, 30, and 40 cells. (The number of dimensions is implicitly defined in the length of this argument). ranges (Sequence[Tuple[float]]): (array-like of (float, float)): Upper and lower bound of each dimension of the measure space, e.g. ``[(-1, 1), (-2, 2)]`` indicates the first dimension should have bounds :math:`[-1,1]` (inclusive), and the second dimension should have bounds :math:`[-2,2]` (inclusive). ``ranges`` should be the same length as ``dims``. instances (Optional[Iterable[Instance]], optional): Instances to pre-initialise the archive. Defaults to None. eps (float, optional): Due to floating point precision errors, we add a small epsilon when computing the archive indices in the :meth:`index_of` method -- refer to the implementation `here. Defaults to 1e-6. dtype(str or data-type): Data type of the solutions, objectives, and measures. Raises: ValueError: ``dimensions`` and ``ranges`` are not the same length \"\"\" Archive . __init__ ( self , threshold = np . finfo ( np . float32 ) . max , dtype = dtype ) if len ( ranges ) == 0 or len ( dimensions ) == 0 : raise ValueError ( \"dimensions and ranges must have length >= 1\" ) if len ( ranges ) != len ( dimensions ): raise ValueError ( f \"len(dimensions) = { len ( dimensions ) } != len(ranges) = { len ( ranges ) } in GridArchive.__init__()\" ) self . _dimensions = np . asarray ( dimensions ) ranges = list ( zip ( * ranges )) self . _lower_bounds = np . array ( ranges [ 0 ], dtype = dtype ) self . _upper_bounds = np . array ( ranges [ 1 ], dtype = dtype ) self . _interval = self . _upper_bounds - self . _lower_bounds self . _eps = eps self . _cells = np . prod ( self . _dimensions , dtype = object ) self . _grid : Dict [ int , np . ndarray ] = {} self . _storage : Dict [ int , Instance ] = {} _bounds = [] for dimension , l_b , u_b in zip ( self . _dimensions , self . _lower_bounds , self . _upper_bounds ): _bounds . append ( np . linspace ( l_b , u_b , dimension )) self . _boundaries = np . asarray ( _bounds ) if instances is not None : self . extend ( instances ) __iter__ () Iterates over the dictionary of instances Returns: Iterator \u2013 Yields position in the hypercube and instance located in such position Source code in digneapy/archives/_grid_archive.py 191 192 193 194 195 196 197 def __iter__ ( self ): \"\"\"Iterates over the dictionary of instances Returns: Iterator: Yields position in the hypercube and instance located in such position \"\"\" return iter ( self . _storage . values ()) append ( instance , descriptor = None ) Inserts an Instance into the Grid Parameters: instance ( Instance ) \u2013 Instace to be inserted Raises: TypeError \u2013 instance is not a instance of the class Instance. Source code in digneapy/archives/_grid_archive.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def append ( self , instance : Instance , descriptor : Optional [ np . ndarray ] = None ): \"\"\"Inserts an Instance into the Grid Args: instance (Instance): Instace to be inserted Raises: TypeError: ``instance`` is not a instance of the class Instance. \"\"\" if not isinstance ( instance , Instance ): msg = \"Only objects of type Instance can be inserted into a GridArchive\" raise TypeError ( msg ) descriptor = ( np . asarray ( instance . descriptor ) if descriptor is None else descriptor ) index = self . index_of ([ descriptor ])[ 0 ] if index not in self . _grid or instance > self . _grid [ index ]: self . _grid [ index ] = descriptor self . _storage [ index ] = instance . clone () extend ( instances , descriptors = None , * args , ** kwargs ) Includes all the instances in iterable into the Grid Parameters: iterable ( Iterable [ Instance ] ) \u2013 Iterable of instances Source code in digneapy/archives/_grid_archive.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 def extend ( self , instances : Sequence [ Instance ], descriptors : Optional [ np . ndarray ] = None , * args , ** kwargs , ): \"\"\"Includes all the instances in iterable into the Grid Args: iterable (Iterable[Instance]): Iterable of instances \"\"\" if not all ( isinstance ( i , Instance ) for i in instances ): msg = \"Only objects of type Instance can be inserted into a GridArchive\" raise TypeError ( msg ) if descriptors is None : try : descriptors = np . asarray ([ i . descriptor for i in instances ]) except AttributeError as e : print ( \"Instances do not have a descriptor yet and the value descriptor is None\" ) raise ( e ) indices = self . index_of ( descriptors ) for idx , instance , descriptor in zip ( indices , instances , descriptors , strict = True ): if idx not in self . _grid or instance . fitness > self . _storage [ idx ] . fitness : self . _storage [ idx ] = instance . clone () self . _grid [ idx ] = descriptor index_of ( descriptors ) Computes the indices of a batch of descriptors. Parameters: descriptors ( array - like ) \u2013 (batch_size, dimensions) array of descriptors for each instance Raises: ValueError \u2013 descriptors is not shape (batch_size, dimensions) Returns: \u2013 np.ndarray: (batch_size, ) array of integer indices representing the flattened grid coordinates. Source code in digneapy/archives/_grid_archive.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def index_of ( self , descriptors ): \"\"\"Computes the indices of a batch of descriptors. Args: descriptors (array-like): (batch_size, dimensions) array of descriptors for each instance Raises: ValueError: ``descriptors`` is not shape (batch_size, dimensions) Returns: np.ndarray: (batch_size, ) array of integer indices representing the flattened grid coordinates. \"\"\" if len ( descriptors ) == 0 : return np . empty ( 0 ) descriptors = np . asarray ( descriptors ) if ( descriptors . ndim == 1 and descriptors . shape [ 0 ] != len ( self . _dimensions ) or descriptors . ndim == 2 and descriptors . shape [ 1 ] != len ( self . _dimensions ) ): raise ValueError ( f \"Expected descriptors to be an array with shape \" f \"(batch_size, dimensions) (i.e. shape \" f \"(batch_size, { len ( self . _dimensions ) } )) but it had shape \" f \" { descriptors . shape } \" ) grid_indices = ( ( self . _dimensions * ( descriptors - self . _lower_bounds ) + self . _eps ) / self . _interval ) . astype ( int ) # Clip the indexes to make sure they are in the expected range for each dimension clipped = np . clip ( grid_indices , 0 , self . _dimensions - 1 ) return self . _grid_to_int_index ( clipped ) purge_unfeasible ( attr = 'p' ) Removes all the unfeasible instances from the grid Source code in digneapy/archives/_grid_archive.py 272 273 274 275 276 277 278 279 def purge_unfeasible ( self , attr : str = \"p\" ): \"\"\"Removes all the unfeasible instances from the grid\"\"\" keys_to_remove = [ i for i in self . _storage . keys () if getattr ( self . _storage [ i ], attr ) < 0 ] for i in keys_to_remove : del self . _grid [ i ] del self . _storage [ i ] remove ( descriptors ) Removes all the instances with the matching descriptors in iterable from the grid Source code in digneapy/archives/_grid_archive.py 263 264 265 266 267 268 269 270 def remove ( self , descriptors : np . ndarray ): \"\"\"Removes all the instances with the matching descriptors in iterable from the grid\"\"\" indices_to_remove = self . index_of ( descriptors ) for index in indices_to_remove : if index in self . _grid : del self . _grid [ index ] del self . _storage [ index ]","title":"Index"},{"location":"reference/archives/#archives.Archive","text":"Class Archive Stores a collection of diverse Instances Source code in digneapy/archives/_base_archive.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 class Archive : \"\"\"Class Archive Stores a collection of diverse Instances \"\"\" def __init__ ( self , threshold : float , instances : Optional [ Sequence [ Instance ]] = None , dtype = np . float64 , ): \"\"\"Creates an instance of a Archive (unstructured) for QD algorithms Args: threshold (float): Minimum value of sparseness to include an Instance into the archive. instances (Iterable[Instance], optional): Instances to initialise the archive. Defaults to None. \"\"\" self . _storage = { \"instances\" : [], \"descriptors\" : []} if instances : self . _storage [ \"instances\" ] . extend ( instances ) self . _storage [ \"descriptors\" ] . extend ( np . asarray ([ instance . descriptor for instance in instances ]) ) self . _threshold = threshold self . _dtype = dtype @property def instances ( self ) -> Sequence [ Instance ]: return self . _storage [ \"instances\" ] @property def descriptors ( self ) -> np . ndarray : return np . asarray ( self . _storage [ \"descriptors\" ]) @property def threshold ( self ): return self . _threshold @threshold . setter def threshold ( self , t : float ): try : t_f = float ( t ) except Exception : msg = f \"The threshold value { t } is not a float in 'threshold' setter of class { self . __class__ . __name__ } \" raise TypeError ( msg ) self . _threshold = t_f def __iter__ ( self ): return iter ( self . _storage [ \"instances\" ]) def __str__ ( self ): return f \"Archive(threshold= { self . _threshold } ,data=(| { len ( self ) } |))\" def __repr__ ( self ): return f \"Archive(threshold= { self . _threshold } ,data=(| { len ( self ) } |))\" def __array__ ( self , dtype = None , copy = None ) -> np . ndarray : \"\"\"Creates a ndarray with the descriptors >>> import numpy as np >>> descriptors = [list(range(d, d + 5)) for d in range(10)] >>> archive = Archive(descriptors) >>> np_archive = np.array(archive) >>> assert len(np_archive) == len(archive) >>> assert type(np_archive) == type(np.zeros(1)) \"\"\" return np . asarray ( self . _storage [ \"instances\" ], dtype = dtype , copy = copy ) def __eq__ ( self , other : Self ): \"\"\"Compares whether to Archives are equal >>> import copy >>> variables = [list(range(d, d + 5)) for d in range(10)] >>> instances = [Instance(variables=v, s=1.0) for v in variables] >>> archive = Archive(threshold=0.0, instances=instances) >>> empty_archive = Archive(threshold=0.0) >>> a1 = copy.copy(archive) >>> assert a1 == archive >>> assert empty_archive != archive \"\"\" return len ( self ) == len ( other ) and all ( np . array_equal ( a , b ) for a , b in zip ( self . _storage [ \"descriptors\" ], other . _storage [ \"descriptors\" ]) ) def __hash__ ( self ): from functools import reduce hashes = ( hash ( i ) for i in self . instances ) return reduce ( lambda a , b : a ^ b , hashes , 0 ) def __bool__ ( self ): \"\"\"Returns True if len(self) > 1 >>> descriptors = [list(range(d, d + 5)) for d in range(10)] >>> archive = Archive(threshold=0.0, instances=descriptors) >>> empty_archive = Archive(threshold=0.0) >>> assert archive >>> assert not empty_archive \"\"\" return len ( self ) != 0 def __len__ ( self ): return len ( self . instances ) def __getitem__ ( self , key ): if isinstance ( key , slice ): cls = type ( self ) # To facilitate subclassing return cls ( self . _threshold , self . instances [ key ]) index = operator . index ( key ) return self . _storage [ \"instances\" ][ index ] def extend ( self , instances : Sequence [ Instance ], novelty_scores : Optional [ np . ndarray ] = None , descriptors : Optional [ np . ndarray ] = None , ): \"\"\"Extends the current archive with all the individuals inside iterable that have a sparseness value greater than the archive threshold. Args: instances (Sequence[Instance]): Sequence of instances to be include in the archive. \"\"\" scores = ( novelty_scores if novelty_scores is not None else np . asarray ([ instance . s for instance in instances ]) ) descriptors = ( descriptors if descriptors is not None else np . asarray ([ instance . descriptor for instance in instances ]) ) to_insert = np . where ( scores >= self . threshold )[ 0 ] self . _storage [ \"instances\" ] . extend (( instances [ i ] for i in to_insert )) self . _storage [ \"descriptors\" ] . extend ( descriptors [ to_insert ]) def __format__ ( self , fmt_spec = \"\" ): variables = self outer_fmt = \"( {} )\" components = ( format ( c , fmt_spec ) for c in variables ) return outer_fmt . format ( \", \" . join ( components )) def asdict ( self ) -> dict : return { \"threshold\" : self . _threshold , \"instances\" : { i : instance . asdict () for i , instance in enumerate ( self . _storage [ \"instances\" ]) }, } def to_json ( self ) -> str : \"\"\"Converts the archive into a JSON object Returns: str: JSON str of the archive content \"\"\" return json . dumps ( self . asdict (), indent = 4 )","title":"Archive"},{"location":"reference/archives/#archives.Archive.__array__","text":"Creates a ndarray with the descriptors import numpy as np descriptors = [list(range(d, d + 5)) for d in range(10)] archive = Archive(descriptors) np_archive = np.array(archive) assert len(np_archive) == len(archive) assert type(np_archive) == type(np.zeros(1)) Source code in digneapy/archives/_base_archive.py 81 82 83 84 85 86 87 88 89 90 91 def __array__ ( self , dtype = None , copy = None ) -> np . ndarray : \"\"\"Creates a ndarray with the descriptors >>> import numpy as np >>> descriptors = [list(range(d, d + 5)) for d in range(10)] >>> archive = Archive(descriptors) >>> np_archive = np.array(archive) >>> assert len(np_archive) == len(archive) >>> assert type(np_archive) == type(np.zeros(1)) \"\"\" return np . asarray ( self . _storage [ \"instances\" ], dtype = dtype , copy = copy )","title":"__array__"},{"location":"reference/archives/#archives.Archive.__bool__","text":"Returns True if len(self) > 1 descriptors = [list(range(d, d + 5)) for d in range(10)] archive = Archive(threshold=0.0, instances=descriptors) empty_archive = Archive(threshold=0.0) assert archive assert not empty_archive Source code in digneapy/archives/_base_archive.py 117 118 119 120 121 122 123 124 125 126 127 def __bool__ ( self ): \"\"\"Returns True if len(self) > 1 >>> descriptors = [list(range(d, d + 5)) for d in range(10)] >>> archive = Archive(threshold=0.0, instances=descriptors) >>> empty_archive = Archive(threshold=0.0) >>> assert archive >>> assert not empty_archive \"\"\" return len ( self ) != 0","title":"__bool__"},{"location":"reference/archives/#archives.Archive.__eq__","text":"Compares whether to Archives are equal import copy variables = [list(range(d, d + 5)) for d in range(10)] instances = [Instance(variables=v, s=1.0) for v in variables] archive = Archive(threshold=0.0, instances=instances) empty_archive = Archive(threshold=0.0) a1 = copy.copy(archive) assert a1 == archive assert empty_archive != archive Source code in digneapy/archives/_base_archive.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def __eq__ ( self , other : Self ): \"\"\"Compares whether to Archives are equal >>> import copy >>> variables = [list(range(d, d + 5)) for d in range(10)] >>> instances = [Instance(variables=v, s=1.0) for v in variables] >>> archive = Archive(threshold=0.0, instances=instances) >>> empty_archive = Archive(threshold=0.0) >>> a1 = copy.copy(archive) >>> assert a1 == archive >>> assert empty_archive != archive \"\"\" return len ( self ) == len ( other ) and all ( np . array_equal ( a , b ) for a , b in zip ( self . _storage [ \"descriptors\" ], other . _storage [ \"descriptors\" ]) )","title":"__eq__"},{"location":"reference/archives/#archives.Archive.__init__","text":"Creates an instance of a Archive (unstructured) for QD algorithms Parameters: threshold ( float ) \u2013 Minimum value of sparseness to include an Instance into the archive. instances ( Iterable [ Instance ] , default: None ) \u2013 Instances to initialise the archive. Defaults to None. Source code in digneapy/archives/_base_archive.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , threshold : float , instances : Optional [ Sequence [ Instance ]] = None , dtype = np . float64 , ): \"\"\"Creates an instance of a Archive (unstructured) for QD algorithms Args: threshold (float): Minimum value of sparseness to include an Instance into the archive. instances (Iterable[Instance], optional): Instances to initialise the archive. Defaults to None. \"\"\" self . _storage = { \"instances\" : [], \"descriptors\" : []} if instances : self . _storage [ \"instances\" ] . extend ( instances ) self . _storage [ \"descriptors\" ] . extend ( np . asarray ([ instance . descriptor for instance in instances ]) ) self . _threshold = threshold self . _dtype = dtype","title":"__init__"},{"location":"reference/archives/#archives.Archive.extend","text":"Extends the current archive with all the individuals inside iterable that have a sparseness value greater than the archive threshold. Parameters: instances ( Sequence [ Instance ] ) \u2013 Sequence of instances to be include in the archive. Source code in digneapy/archives/_base_archive.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def extend ( self , instances : Sequence [ Instance ], novelty_scores : Optional [ np . ndarray ] = None , descriptors : Optional [ np . ndarray ] = None , ): \"\"\"Extends the current archive with all the individuals inside iterable that have a sparseness value greater than the archive threshold. Args: instances (Sequence[Instance]): Sequence of instances to be include in the archive. \"\"\" scores = ( novelty_scores if novelty_scores is not None else np . asarray ([ instance . s for instance in instances ]) ) descriptors = ( descriptors if descriptors is not None else np . asarray ([ instance . descriptor for instance in instances ]) ) to_insert = np . where ( scores >= self . threshold )[ 0 ] self . _storage [ \"instances\" ] . extend (( instances [ i ] for i in to_insert )) self . _storage [ \"descriptors\" ] . extend ( descriptors [ to_insert ])","title":"extend"},{"location":"reference/archives/#archives.Archive.to_json","text":"Converts the archive into a JSON object Returns: str ( str ) \u2013 JSON str of the archive content Source code in digneapy/archives/_base_archive.py 181 182 183 184 185 186 187 188 def to_json ( self ) -> str : \"\"\"Converts the archive into a JSON object Returns: str: JSON str of the archive content \"\"\" return json . dumps ( self . asdict (), indent = 4 )","title":"to_json"},{"location":"reference/archives/#archives.CVTArchive","text":"Bases: GridArchive , RNG An Archive that divides a high-dimensional measure space into k homogeneous geometric regions. Based on the paper from Vassiliades et al (2018) https://ieeexplore.ieee.org/document/8000667 The computational complexity of the method we provide for constructing the CVT (in Algorithm 1) is O(ndki), where n is the number of d-dimensional samples to be clustered, k is the number of clusters, and i is the number of iterations needed until convergence Source code in digneapy/archives/_cvt_archive.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 class CVTArchive ( GridArchive , RNG ): \"\"\"An Archive that divides a high-dimensional measure space into k homogeneous geometric regions. Based on the paper from Vassiliades et al (2018) <https://ieeexplore.ieee.org/document/8000667> > The computational complexity of the method we provide for constructing the CVT (in Algorithm 1) is O(ndki), > where n is the number of d-dimensional samples to be clustered, k is the number of clusters, > and i is the number of iterations needed until convergence \"\"\" def __init__ ( self , k : int , ranges : Sequence [ Tuple [ float , float ]], n_samples : int , centroids : Optional [ npt . NDArray | str ] = None , samples : Optional [ npt . NDArray | str ] = None , dtype = np . float64 , seed : int = 42 , ): \"\"\"Creates a CVTArchive object Args: k (int): Number of centroids (regions) to create ranges (Sequence[Tuple[float, float]]): Ranges of the measure space. Upper and lower bound of each dimension of the measure space, e.g. ``[(-1, 1), (-2, 2)]`` indicates the first dimension should have bounds :math:`[-1,1]` (inclusive), and the second dimension should have bounds :math:`[-2,2]` (inclusive). The legnth of ``ranges`` indicates the number of dimensions of the measure space. n_samples (int): Number of samples to generate before calculating the centroids. centroids (Optional[npt.NDArray | str], optional): Precalculated centroids for the archive. The options are a np.ndarray with the values of ``k`` centroids or a .txt with the centroids to be loaded by Numpy. Defaults to None. samples (Optional[npt.NDArray | str], optional): Precalculated samples for the archive. The options are a np.ndarray with the values of ``n_samples`` samples or a .txt with the samples to be loaded by Numpy. Defaults to None. Raises: ValueError: If len(ranges) <= 0. ValueError: If the number of samples is less than zero or less than the number of regions (k). ValueError: If the number of regions is less than zero. ValueError: If the samples file cannot be loaded. ValueError: If given a samples np.ndarray the number of samples in the file is different from the number of expected samples (n_samples). ValueError: If the centroids file cannot be loaded. ValueError: If given a centroids np.ndarray the number of centroids in the file is different from the number of regions (k). \"\"\" if k <= 0 : raise ValueError ( f \"The number of regions (k = { k } ) must be >= 1\" ) if len ( ranges ) <= 0 : raise ValueError ( f \"ranges must have length >= 1 and it has length { len ( ranges ) } \" ) if n_samples <= 0 or n_samples < k : raise ValueError ( f \"The number of samples (n_samples = { n_samples } ) must be >= 1 and >= regions (k = { k } )\" ) GridArchive . __init__ ( self , dimensions = ( 1 ,) * len ( ranges ), ranges = ranges , dtype = dtype ) self . _dimensions = len ( ranges ) ranges = list ( zip ( * ranges )) self . _lower_bounds = np . array ( ranges [ 0 ], dtype = self . _dtype ) self . _upper_bounds = np . array ( ranges [ 1 ], dtype = self . _dtype ) self . _interval = self . _upper_bounds - self . _lower_bounds self . _k = k self . _n_samples = n_samples self . _samples = None self . _centroids = None self . initialize_rng ( seed = seed ) self . _kmeans = KMeans ( n_clusters = self . _k , n_init = 1 , random_state = self . _seed ) # Loading samples if given if samples is not None : if isinstance ( samples , str ): try : self . _samples = np . load ( samples ) self . _n_samples = len ( self . _samples ) except Exception as _ : raise ValueError ( f \"Error in CVTArchive.__init__() loading the samples file { samples } .\" ) elif isinstance ( samples , np . ndarray ) and len ( samples ) != n_samples : raise ValueError ( f \"The number of samples { len ( samples ) } must be equal to the number of expected samples (n_samples = { n_samples } )\" ) else : self . _samples = np . asarray ( samples ) if centroids is not None : if isinstance ( centroids , str ): try : self . _centroids = np . load ( centroids ) self . _k = len ( self . _centroids ) except Exception as _ : raise ValueError ( f \"Error in CVTArchive.__init__() loading the centroids file { centroids } .\" ) elif isinstance ( centroids , np . ndarray ) and len ( centroids ) != k : raise ValueError ( f \"The number of centroids { len ( centroids ) } must be equal to the number of regions (k = { self . _k } )\" ) else : self . _centroids = np . asarray ( centroids ) else : # Generate centroids if self . _samples is None : # Generate uniform samples if not given rng = np . random . default_rng ( seed = self . _seed ) self . _samples = rng . uniform ( low = self . _lower_bounds , high = self . _upper_bounds , size = ( self . _n_samples , self . _dimensions ), ) self . _kmeans . fit ( self . _samples ) self . _centroids = self . _kmeans . cluster_centers_ self . _kdtree = KDTree ( self . _centroids , metric = \"euclidean\" ) @property def dimensions ( self ) -> int : \"\"\"Dimensions of the measure space used Returns: int: Dimensions of the measure space used \"\"\" return self . _dimensions @property def samples ( self ) -> np . ndarray : \"\"\"Returns the samples used to generate the centroids Returns: np.ndarray: Samples \"\"\" return self . _samples @property def centroids ( self ) -> np . ndarray : \"\"\"Returns k centroids calculated from the samples Returns: np.ndarray: K d-dimensional centroids \"\"\" return self . _centroids @property def regions ( self ) -> int : \"\"\"Number of regions (k) of centroids in the CVTArchive Returns: int: k \"\"\" return self . _k @property def bounds ( self ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Tuple with the lower and upper bounds of the measure space The first value is the lower bounds and the second value is the upper bounds. Each value is a list with the corresponding lower/upper bound of the ith dimension in the measure space \"\"\" return ( self . _lower_bounds , self . _upper_bounds ) @property def instances ( self ) -> list [ Instance ]: return list ( self . _storage . values ()) def __str__ ( self ): return f \"CVArchive(dim= { self . _dimensions } ,regions= { self . _k } ,centroids= { self . _centroids } )\" def __repr__ ( self ): return f \"CVArchive(dim= { self . _dimensions } ,regions= { self . _k } ,centroids= { self . _centroids } )\" def __iter__ ( self ): \"\"\"Iterates over the dictionary of instances Returns: Iterator: Yields position in the hypercube and instance located in such position \"\"\" return iter ( self . _storage . values ()) def lower_i ( self , i ) -> np . float64 : if i < 0 or i > len ( self . _lower_bounds ): msg = f \"index { i } is out of bounds. Valid values are [0- { len ( self . _lower_bounds ) } ]\" raise ValueError ( msg ) return self . _lower_bounds [ i ] def upper_i ( self , i ) -> np . float64 : if i < 0 or i > len ( self . _upper_bounds ): msg = f \"index { i } is out of bounds. Valid values are [0- { len ( self . _upper_bounds ) } ]\" raise ValueError ( msg ) return self . _upper_bounds [ i ] def index_of ( self , descriptors ) -> np . ndarray : \"\"\"Computes the indeces of a batch of descriptors. Args: descriptors (array-like): (batch_size, dimensions) array of descriptors for each instance Raises: ValueError: ``descriptors`` is not shape (batch_size, dimensions) Returns: np.ndarray: (batch_size, ) array of integer indices representing the flattened grid coordinates. \"\"\" descriptors = np . array ( descriptors ) if len ( descriptors ) == 0 : return np . empty ( 0 ) elif ( descriptors . ndim == 1 and descriptors . shape [ 0 ] != self . _dimensions or descriptors . ndim == 2 and descriptors . shape [ 1 ] != self . _dimensions ): raise ValueError ( f \"Expected descriptors to be an array with shape \" f \"(batch_size, dimensions) (i.e. shape \" f \"(batch_size, { self . _dimensions } )) but it had shape \" f \" { descriptors . shape } \" ) indices = self . _kdtree . query ( descriptors , return_distance = False ) indices = indices [:, 0 ] return indices . astype ( np . int32 ) def to_file ( self , file_pattern : str = \"CVTArchive\" ): \"\"\"Saves the centroids and the samples of the CVTArchive to .npy files Each attribute is saved in its own filename. Therefore, file_pattern is expected not to contain any extension Args: file_pattern (str, optional): Pattern of the expected filenames. Defaults to \"CVTArchive\". \"\"\" np . save ( f \" { file_pattern } _centroids.npy\" , self . _centroids ) np . save ( f \" { file_pattern } _samples.npy\" , self . _samples ) @classmethod def load_from_json ( cls , filename : str ): \"\"\"Creates a CVTArchive object from the content of a previously created JSON file Args: filename (str): Filename of the JSON file with the CVTArchive information Raises: ValueError: If there's any error while loading the file. (IOError) ValueError: If the JSON file does not contain all the expected keys Returns: Self: Returns a CVTArchive object \"\"\" expected_keys = { \"dimensions\" , \"n_samples\" , \"regions\" , \"lbs\" , \"ubs\" , \"centroids\" , \"samples\" , } try : with open ( filename , \"r\" ) as file : json_data = json . load ( file ) if expected_keys != json_data . keys (): raise ValueError ( f \"The JSON file does not contain all the minimum expected keys. Expected keys are { expected_keys } and got { json_data . keys () } \" ) _ranges = [ ( l_i , u_i ) for l_i , u_i in zip ( json_data [ \"lbs\" ], json_data [ \"ubs\" ]) ] new_archive = cls ( k = json_data [ \"regions\" ], ranges = _ranges , n_samples = json_data [ \"n_samples\" ], centroids = json_data [ \"centroids\" ], samples = json_data [ \"samples\" ], ) return new_archive except IOError as io : raise ValueError ( f \"Error opening file { filename } . Reason -> { io . strerror } \" ) def asdict ( self ) -> dict : return { \"dimensions\" : self . _dimensions , \"n_samples\" : self . _n_samples , \"regions\" : self . _k , \"lbs\" : self . _lower_bounds . tolist (), \"ubs\" : self . _upper_bounds . tolist (), \"centroids\" : self . _centroids . tolist (), \"samples\" : self . _samples . tolist (), \"instances\" : { i : instance . asdict () for i , instance in enumerate ( self . _storage . values ()) }, } def to_json ( self , filename : Optional [ str ] = None ) -> str : \"\"\"Returns the content of the CVTArchive in JSON format. Returns: str: String in JSON format with the content of the CVTArchive \"\"\" json_data = json . dumps ( self . asdict (), indent = 4 ) if filename is not None : filename = ( f \" { filename } .json\" if not filename . endswith ( \".json\" ) else filename ) with open ( filename , \"w\" ) as f : f . write ( json_data ) return json_data","title":"CVTArchive"},{"location":"reference/archives/#archives.CVTArchive.bounds","text":"Tuple with the lower and upper bounds of the measure space The first value is the lower bounds and the second value is the upper bounds. Each value is a list with the corresponding lower/upper bound of the ith dimension in the measure space","title":"bounds"},{"location":"reference/archives/#archives.CVTArchive.centroids","text":"Returns k centroids calculated from the samples Returns: ndarray \u2013 np.ndarray: K d-dimensional centroids","title":"centroids"},{"location":"reference/archives/#archives.CVTArchive.dimensions","text":"Dimensions of the measure space used Returns: int ( int ) \u2013 Dimensions of the measure space used","title":"dimensions"},{"location":"reference/archives/#archives.CVTArchive.regions","text":"Number of regions (k) of centroids in the CVTArchive Returns: int ( int ) \u2013 k","title":"regions"},{"location":"reference/archives/#archives.CVTArchive.samples","text":"Returns the samples used to generate the centroids Returns: ndarray \u2013 np.ndarray: Samples","title":"samples"},{"location":"reference/archives/#archives.CVTArchive.__init__","text":"Creates a CVTArchive object Parameters: k ( int ) \u2013 Number of centroids (regions) to create ranges ( Sequence [ Tuple [ float , float ]] ) \u2013 Ranges of the measure space. Upper and lower bound of each indicates the first dimension should have bounds \u2013 math: [-1,1] \u2013 math: [-2,2] (inclusive). The legnth of ranges indicates the number of dimensions of the measure space. n_samples ( int ) \u2013 Number of samples to generate before calculating the centroids. centroids ( Optional [ NDArray | str ] , default: None ) \u2013 Precalculated centroids for the archive. samples ( Optional [ NDArray | str ] , default: None ) \u2013 Precalculated samples for the archive. Raises: ValueError \u2013 If len(ranges) <= 0. ValueError \u2013 If the number of samples is less than zero or less than the number of regions (k). ValueError \u2013 If the number of regions is less than zero. ValueError \u2013 If the samples file cannot be loaded. ValueError \u2013 If given a samples np.ndarray the number of samples in the file is different from the number of expected samples (n_samples). ValueError \u2013 If the centroids file cannot be loaded. ValueError \u2013 If given a centroids np.ndarray the number of centroids in the file is different from the number of regions (k). Source code in digneapy/archives/_cvt_archive.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def __init__ ( self , k : int , ranges : Sequence [ Tuple [ float , float ]], n_samples : int , centroids : Optional [ npt . NDArray | str ] = None , samples : Optional [ npt . NDArray | str ] = None , dtype = np . float64 , seed : int = 42 , ): \"\"\"Creates a CVTArchive object Args: k (int): Number of centroids (regions) to create ranges (Sequence[Tuple[float, float]]): Ranges of the measure space. Upper and lower bound of each dimension of the measure space, e.g. ``[(-1, 1), (-2, 2)]`` indicates the first dimension should have bounds :math:`[-1,1]` (inclusive), and the second dimension should have bounds :math:`[-2,2]` (inclusive). The legnth of ``ranges`` indicates the number of dimensions of the measure space. n_samples (int): Number of samples to generate before calculating the centroids. centroids (Optional[npt.NDArray | str], optional): Precalculated centroids for the archive. The options are a np.ndarray with the values of ``k`` centroids or a .txt with the centroids to be loaded by Numpy. Defaults to None. samples (Optional[npt.NDArray | str], optional): Precalculated samples for the archive. The options are a np.ndarray with the values of ``n_samples`` samples or a .txt with the samples to be loaded by Numpy. Defaults to None. Raises: ValueError: If len(ranges) <= 0. ValueError: If the number of samples is less than zero or less than the number of regions (k). ValueError: If the number of regions is less than zero. ValueError: If the samples file cannot be loaded. ValueError: If given a samples np.ndarray the number of samples in the file is different from the number of expected samples (n_samples). ValueError: If the centroids file cannot be loaded. ValueError: If given a centroids np.ndarray the number of centroids in the file is different from the number of regions (k). \"\"\" if k <= 0 : raise ValueError ( f \"The number of regions (k = { k } ) must be >= 1\" ) if len ( ranges ) <= 0 : raise ValueError ( f \"ranges must have length >= 1 and it has length { len ( ranges ) } \" ) if n_samples <= 0 or n_samples < k : raise ValueError ( f \"The number of samples (n_samples = { n_samples } ) must be >= 1 and >= regions (k = { k } )\" ) GridArchive . __init__ ( self , dimensions = ( 1 ,) * len ( ranges ), ranges = ranges , dtype = dtype ) self . _dimensions = len ( ranges ) ranges = list ( zip ( * ranges )) self . _lower_bounds = np . array ( ranges [ 0 ], dtype = self . _dtype ) self . _upper_bounds = np . array ( ranges [ 1 ], dtype = self . _dtype ) self . _interval = self . _upper_bounds - self . _lower_bounds self . _k = k self . _n_samples = n_samples self . _samples = None self . _centroids = None self . initialize_rng ( seed = seed ) self . _kmeans = KMeans ( n_clusters = self . _k , n_init = 1 , random_state = self . _seed ) # Loading samples if given if samples is not None : if isinstance ( samples , str ): try : self . _samples = np . load ( samples ) self . _n_samples = len ( self . _samples ) except Exception as _ : raise ValueError ( f \"Error in CVTArchive.__init__() loading the samples file { samples } .\" ) elif isinstance ( samples , np . ndarray ) and len ( samples ) != n_samples : raise ValueError ( f \"The number of samples { len ( samples ) } must be equal to the number of expected samples (n_samples = { n_samples } )\" ) else : self . _samples = np . asarray ( samples ) if centroids is not None : if isinstance ( centroids , str ): try : self . _centroids = np . load ( centroids ) self . _k = len ( self . _centroids ) except Exception as _ : raise ValueError ( f \"Error in CVTArchive.__init__() loading the centroids file { centroids } .\" ) elif isinstance ( centroids , np . ndarray ) and len ( centroids ) != k : raise ValueError ( f \"The number of centroids { len ( centroids ) } must be equal to the number of regions (k = { self . _k } )\" ) else : self . _centroids = np . asarray ( centroids ) else : # Generate centroids if self . _samples is None : # Generate uniform samples if not given rng = np . random . default_rng ( seed = self . _seed ) self . _samples = rng . uniform ( low = self . _lower_bounds , high = self . _upper_bounds , size = ( self . _n_samples , self . _dimensions ), ) self . _kmeans . fit ( self . _samples ) self . _centroids = self . _kmeans . cluster_centers_ self . _kdtree = KDTree ( self . _centroids , metric = \"euclidean\" )","title":"__init__"},{"location":"reference/archives/#archives.CVTArchive.__iter__","text":"Iterates over the dictionary of instances Returns: Iterator \u2013 Yields position in the hypercube and instance located in such position Source code in digneapy/archives/_cvt_archive.py 201 202 203 204 205 206 207 def __iter__ ( self ): \"\"\"Iterates over the dictionary of instances Returns: Iterator: Yields position in the hypercube and instance located in such position \"\"\" return iter ( self . _storage . values ())","title":"__iter__"},{"location":"reference/archives/#archives.CVTArchive.index_of","text":"Computes the indeces of a batch of descriptors. Parameters: descriptors ( array - like ) \u2013 (batch_size, dimensions) array of descriptors for each instance Raises: ValueError \u2013 descriptors is not shape (batch_size, dimensions) Returns: ndarray \u2013 np.ndarray: (batch_size, ) array of integer indices representing the flattened grid coordinates. Source code in digneapy/archives/_cvt_archive.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 def index_of ( self , descriptors ) -> np . ndarray : \"\"\"Computes the indeces of a batch of descriptors. Args: descriptors (array-like): (batch_size, dimensions) array of descriptors for each instance Raises: ValueError: ``descriptors`` is not shape (batch_size, dimensions) Returns: np.ndarray: (batch_size, ) array of integer indices representing the flattened grid coordinates. \"\"\" descriptors = np . array ( descriptors ) if len ( descriptors ) == 0 : return np . empty ( 0 ) elif ( descriptors . ndim == 1 and descriptors . shape [ 0 ] != self . _dimensions or descriptors . ndim == 2 and descriptors . shape [ 1 ] != self . _dimensions ): raise ValueError ( f \"Expected descriptors to be an array with shape \" f \"(batch_size, dimensions) (i.e. shape \" f \"(batch_size, { self . _dimensions } )) but it had shape \" f \" { descriptors . shape } \" ) indices = self . _kdtree . query ( descriptors , return_distance = False ) indices = indices [:, 0 ] return indices . astype ( np . int32 )","title":"index_of"},{"location":"reference/archives/#archives.CVTArchive.load_from_json","text":"Creates a CVTArchive object from the content of a previously created JSON file Parameters: filename ( str ) \u2013 Filename of the JSON file with the CVTArchive information Raises: ValueError \u2013 If there's any error while loading the file. (IOError) ValueError \u2013 If the JSON file does not contain all the expected keys Returns: Self \u2013 Returns a CVTArchive object Source code in digneapy/archives/_cvt_archive.py 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 @classmethod def load_from_json ( cls , filename : str ): \"\"\"Creates a CVTArchive object from the content of a previously created JSON file Args: filename (str): Filename of the JSON file with the CVTArchive information Raises: ValueError: If there's any error while loading the file. (IOError) ValueError: If the JSON file does not contain all the expected keys Returns: Self: Returns a CVTArchive object \"\"\" expected_keys = { \"dimensions\" , \"n_samples\" , \"regions\" , \"lbs\" , \"ubs\" , \"centroids\" , \"samples\" , } try : with open ( filename , \"r\" ) as file : json_data = json . load ( file ) if expected_keys != json_data . keys (): raise ValueError ( f \"The JSON file does not contain all the minimum expected keys. Expected keys are { expected_keys } and got { json_data . keys () } \" ) _ranges = [ ( l_i , u_i ) for l_i , u_i in zip ( json_data [ \"lbs\" ], json_data [ \"ubs\" ]) ] new_archive = cls ( k = json_data [ \"regions\" ], ranges = _ranges , n_samples = json_data [ \"n_samples\" ], centroids = json_data [ \"centroids\" ], samples = json_data [ \"samples\" ], ) return new_archive except IOError as io : raise ValueError ( f \"Error opening file { filename } . Reason -> { io . strerror } \" )","title":"load_from_json"},{"location":"reference/archives/#archives.CVTArchive.to_file","text":"Saves the centroids and the samples of the CVTArchive to .npy files Each attribute is saved in its own filename. Therefore, file_pattern is expected not to contain any extension Parameters: file_pattern ( str , default: 'CVTArchive' ) \u2013 Pattern of the expected filenames. Defaults to \"CVTArchive\". Source code in digneapy/archives/_cvt_archive.py 254 255 256 257 258 259 260 261 262 263 def to_file ( self , file_pattern : str = \"CVTArchive\" ): \"\"\"Saves the centroids and the samples of the CVTArchive to .npy files Each attribute is saved in its own filename. Therefore, file_pattern is expected not to contain any extension Args: file_pattern (str, optional): Pattern of the expected filenames. Defaults to \"CVTArchive\". \"\"\" np . save ( f \" { file_pattern } _centroids.npy\" , self . _centroids ) np . save ( f \" { file_pattern } _samples.npy\" , self . _samples )","title":"to_file"},{"location":"reference/archives/#archives.CVTArchive.to_json","text":"Returns the content of the CVTArchive in JSON format. Returns: str ( str ) \u2013 String in JSON format with the content of the CVTArchive Source code in digneapy/archives/_cvt_archive.py 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 def to_json ( self , filename : Optional [ str ] = None ) -> str : \"\"\"Returns the content of the CVTArchive in JSON format. Returns: str: String in JSON format with the content of the CVTArchive \"\"\" json_data = json . dumps ( self . asdict (), indent = 4 ) if filename is not None : filename = ( f \" { filename } .json\" if not filename . endswith ( \".json\" ) else filename ) with open ( filename , \"w\" ) as f : f . write ( json_data ) return json_data","title":"to_json"},{"location":"reference/archives/#archives.GridArchive","text":"Bases: Archive An archive that divides each dimension into a uniformly-sized cells. The source code of this class is inspired by the GridArchive class of pyribs https://github.com/icaros-usc/pyribs/blob/master/ribs/archives/_grid_archive.py This archive is the container described in Mouret 2015 <https://arxiv.org/pdf/1504.04909.pdf> _. It can be visualized as an n-dimensional grid in the measure space that is divided into a certain number of cells in each dimension. Each cell contains an elite, i.e. a solution that maximizes the objective function for the measures in that cell. Source code in digneapy/archives/_grid_archive.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 class GridArchive ( Archive ): \"\"\"An archive that divides each dimension into a uniformly-sized cells. The source code of this class is inspired by the GridArchive class of pyribs <https://github.com/icaros-usc/pyribs/blob/master/ribs/archives/_grid_archive.py> This archive is the container described in `Mouret 2015 <https://arxiv.org/pdf/1504.04909.pdf>`_. It can be visualized as an n-dimensional grid in the measure space that is divided into a certain number of cells in each dimension. Each cell contains an elite, i.e. a solution that `maximizes` the objective function for the measures in that cell. \"\"\" def __init__ ( self , dimensions : Sequence [ int ], ranges : Sequence [ Tuple [ float , float ]], instances : Optional [ Iterable [ Instance ]] = None , eps : float = 1e-6 , dtype = np . float64 , ): \"\"\"Creates a GridArchive instance Args: dimensions (Sequence[int]): (array-like of int): Number of cells in each dimension of the measure space, e.g. ``[20, 30, 40]`` indicates there should be 3 dimensions with 20, 30, and 40 cells. (The number of dimensions is implicitly defined in the length of this argument). ranges (Sequence[Tuple[float]]): (array-like of (float, float)): Upper and lower bound of each dimension of the measure space, e.g. ``[(-1, 1), (-2, 2)]`` indicates the first dimension should have bounds :math:`[-1,1]` (inclusive), and the second dimension should have bounds :math:`[-2,2]` (inclusive). ``ranges`` should be the same length as ``dims``. instances (Optional[Iterable[Instance]], optional): Instances to pre-initialise the archive. Defaults to None. eps (float, optional): Due to floating point precision errors, we add a small epsilon when computing the archive indices in the :meth:`index_of` method -- refer to the implementation `here. Defaults to 1e-6. dtype(str or data-type): Data type of the solutions, objectives, and measures. Raises: ValueError: ``dimensions`` and ``ranges`` are not the same length \"\"\" Archive . __init__ ( self , threshold = np . finfo ( np . float32 ) . max , dtype = dtype ) if len ( ranges ) == 0 or len ( dimensions ) == 0 : raise ValueError ( \"dimensions and ranges must have length >= 1\" ) if len ( ranges ) != len ( dimensions ): raise ValueError ( f \"len(dimensions) = { len ( dimensions ) } != len(ranges) = { len ( ranges ) } in GridArchive.__init__()\" ) self . _dimensions = np . asarray ( dimensions ) ranges = list ( zip ( * ranges )) self . _lower_bounds = np . array ( ranges [ 0 ], dtype = dtype ) self . _upper_bounds = np . array ( ranges [ 1 ], dtype = dtype ) self . _interval = self . _upper_bounds - self . _lower_bounds self . _eps = eps self . _cells = np . prod ( self . _dimensions , dtype = object ) self . _grid : Dict [ int , np . ndarray ] = {} self . _storage : Dict [ int , Instance ] = {} _bounds = [] for dimension , l_b , u_b in zip ( self . _dimensions , self . _lower_bounds , self . _upper_bounds ): _bounds . append ( np . linspace ( l_b , u_b , dimension )) self . _boundaries = np . asarray ( _bounds ) if instances is not None : self . extend ( instances ) @property def dimensions ( self ): return self . _dimensions @property def bounds ( self ): \"\"\"list of numpy.ndarray: The boundaries of the cells in each dimension. Entry ``i`` in this list is an array that contains the boundaries of the cells in dimension ``i``. The array contains ``self.dims[i] + 1`` entries laid out like this:: Archive cells: | 0 | 1 | ... | self.dims[i] | boundaries[i]: 0 1 2 self.dims[i] - 1 self.dims[i] Thus, ``boundaries[i][j]`` and ``boundaries[i][j + 1]`` are the lower and upper bounds of cell ``j`` in dimension ``i``. To access the lower bounds of all the cells in dimension ``i``, use ``boundaries[i][:-1]``, and to access all the upper bounds, use ``boundaries[i][1:]``. \"\"\" return self . _boundaries @property def n_cells ( self ): return self . _cells @property def coverage ( self ): \"\"\"Get the coverage of the hypercube space. The coverage is calculated has the number of cells filled over the total space available. Returns: float: Filled cells over the total available. \"\"\" if len ( self . _grid ) == 0 : return 0.0 return len ( self . _grid ) / self . _cells @property def filled_cells ( self ): return self . _grid . keys () @property def instances ( self ) -> Sequence [ Instance ]: return list ( self . _storage . values ()) def __str__ ( self ): return f \"GridArchive(dim= { self . _dimensions } ,cells= { self . _cells } ,bounds= { self . _boundaries } )\" def __repr__ ( self ): return f \"GridArchive(dim= { self . _dimensions } ,cells= { self . _cells } ,bounds= { self . _boundaries } )\" def __len__ ( self ): return len ( self . _grid ) def __getitem__ ( self , key ): \"\"\"Returns a dictionary with the descriptors as the keys. The values are the instances found. Note that some of the given keys may not be in the archive. Args: key (array-like or descriptor): Descriptors of the instances that want to retrieve. Valid examples are: - archive[[0,11], [0,5]] --> Get the instances with the descriptors (0,11) and (0, 5) - archive[0,11] --> Get the instances at indices 0 and 11 Raises: TypeError: If the key is an slice. Not allowed. ValueError: If the shape of the keys are not valid. Returns: dict: Returns a dict with the found instances. \"\"\" if isinstance ( key , slice ): raise TypeError ( \"Slicing is not available in GridArchive. Use 1D index or descriptor-type indices\" ) descriptors = np . asarray ( key ) if descriptors . ndim == 1 : indices = descriptors elif descriptors . ndim == 2 and descriptors . shape [ 1 ] == len ( self . _dimensions ): indices = self . index_of ( descriptors ) . tolist () else : raise ValueError ( f \"Expected descriptors to be an array with shape \" f \"(batch_size, dimensions) (i.e. shape \" f \"(batch_size, { len ( self . _dimensions ) } )) but it had shape \" f \" { descriptors . shape } \" ) if isinstance ( indices , int ): indices = [ indices ] instances = [ self . _storage [ idx ] for idx in indices ] return instances def __iter__ ( self ): \"\"\"Iterates over the dictionary of instances Returns: Iterator: Yields position in the hypercube and instance located in such position \"\"\" return iter ( self . _storage . values ()) def lower_i ( self , i ): if i < 0 or i > len ( self . _lower_bounds ): msg = f \"index { i } is out of bounds. Valid values are [0- { len ( self . _boundaries ) } ]\" raise ValueError ( msg ) return self . _lower_bounds [ i ] def upper_i ( self , i ): if i < 0 or i > len ( self . _upper_bounds ): msg = f \"index { i } is out of bounds. Valid values are [0- { len ( self . _boundaries ) } ]\" raise ValueError ( msg ) return self . _upper_bounds [ i ] def append ( self , instance : Instance , descriptor : Optional [ np . ndarray ] = None ): \"\"\"Inserts an Instance into the Grid Args: instance (Instance): Instace to be inserted Raises: TypeError: ``instance`` is not a instance of the class Instance. \"\"\" if not isinstance ( instance , Instance ): msg = \"Only objects of type Instance can be inserted into a GridArchive\" raise TypeError ( msg ) descriptor = ( np . asarray ( instance . descriptor ) if descriptor is None else descriptor ) index = self . index_of ([ descriptor ])[ 0 ] if index not in self . _grid or instance > self . _grid [ index ]: self . _grid [ index ] = descriptor self . _storage [ index ] = instance . clone () def extend ( self , instances : Sequence [ Instance ], descriptors : Optional [ np . ndarray ] = None , * args , ** kwargs , ): \"\"\"Includes all the instances in iterable into the Grid Args: iterable (Iterable[Instance]): Iterable of instances \"\"\" if not all ( isinstance ( i , Instance ) for i in instances ): msg = \"Only objects of type Instance can be inserted into a GridArchive\" raise TypeError ( msg ) if descriptors is None : try : descriptors = np . asarray ([ i . descriptor for i in instances ]) except AttributeError as e : print ( \"Instances do not have a descriptor yet and the value descriptor is None\" ) raise ( e ) indices = self . index_of ( descriptors ) for idx , instance , descriptor in zip ( indices , instances , descriptors , strict = True ): if idx not in self . _grid or instance . fitness > self . _storage [ idx ] . fitness : self . _storage [ idx ] = instance . clone () self . _grid [ idx ] = descriptor def remove ( self , descriptors : np . ndarray ): \"\"\"Removes all the instances with the matching descriptors in iterable from the grid\"\"\" indices_to_remove = self . index_of ( descriptors ) for index in indices_to_remove : if index in self . _grid : del self . _grid [ index ] del self . _storage [ index ] def purge_unfeasible ( self , attr : str = \"p\" ): \"\"\"Removes all the unfeasible instances from the grid\"\"\" keys_to_remove = [ i for i in self . _storage . keys () if getattr ( self . _storage [ i ], attr ) < 0 ] for i in keys_to_remove : del self . _grid [ i ] del self . _storage [ i ] def index_of ( self , descriptors ): \"\"\"Computes the indices of a batch of descriptors. Args: descriptors (array-like): (batch_size, dimensions) array of descriptors for each instance Raises: ValueError: ``descriptors`` is not shape (batch_size, dimensions) Returns: np.ndarray: (batch_size, ) array of integer indices representing the flattened grid coordinates. \"\"\" if len ( descriptors ) == 0 : return np . empty ( 0 ) descriptors = np . asarray ( descriptors ) if ( descriptors . ndim == 1 and descriptors . shape [ 0 ] != len ( self . _dimensions ) or descriptors . ndim == 2 and descriptors . shape [ 1 ] != len ( self . _dimensions ) ): raise ValueError ( f \"Expected descriptors to be an array with shape \" f \"(batch_size, dimensions) (i.e. shape \" f \"(batch_size, { len ( self . _dimensions ) } )) but it had shape \" f \" { descriptors . shape } \" ) grid_indices = ( ( self . _dimensions * ( descriptors - self . _lower_bounds ) + self . _eps ) / self . _interval ) . astype ( int ) # Clip the indexes to make sure they are in the expected range for each dimension clipped = np . clip ( grid_indices , 0 , self . _dimensions - 1 ) return self . _grid_to_int_index ( clipped ) def _grid_to_int_index ( self , grid_indices ) -> np . ndarray : grid_indices = np . asarray ( grid_indices ) if len ( self . _dimensions ) > 64 : strides = np . cumprod (( 1 ,) + tuple ( self . _dimensions [:: - 1 ][: - 1 ]))[:: - 1 ] # Reshape strides to (1, num_dimensions) to make it broadcastable with grid_indices strides = strides . reshape ( 1 , - 1 ) flattened_indices = np . sum ( grid_indices * strides , axis = 1 , dtype = object ) else : flattened_indices = np . ravel_multi_index ( grid_indices . T , self . _dimensions ) . astype ( int ) return flattened_indices def int_to_grid_index ( self , int_indices ) -> np . ndarray : int_indices = np . asarray ( int_indices ) if len ( self . _dimensions ) > 64 : # Manually unravel the index for dimensions > 64 unravel_indices = [] remaining_indices = int_indices . astype ( object ) for dim_size in self . _dimensions [:: - 1 ]: unravel_indices . append ( remaining_indices % dim_size ) remaining_indices //= dim_size unravel_indices = np . array ( unravel_indices [:: - 1 ]) . T else : unravel_indices = np . asarray ( np . unravel_index ( int_indices , self . _dimensions , ) ) . T . astype ( int ) return unravel_indices def asdict ( self ) -> dict : return { \"dimensions\" : self . _dimensions . tolist (), \"lbs\" : self . _lower_bounds . tolist (), \"ubs\" : self . _upper_bounds . tolist (), \"n_cells\" : self . _cells , \"instances\" : { i : instance . asdict () for i , instance in enumerate ( self . _storage . values ()) }, } def to_json ( self ) -> str : return json . dumps ( self . asdict (), indent = 4 )","title":"GridArchive"},{"location":"reference/archives/#archives.GridArchive.bounds","text":"list of numpy.ndarray: The boundaries of the cells in each dimension. Entry i in this list is an array that contains the boundaries of the cells in dimension i . The array contains self.dims[i] + 1 entries laid out like this:: Archive cells: | 0 | 1 | ... | self.dims[i] | boundaries[i]: 0 1 2 self.dims[i] - 1 self.dims[i] Thus, boundaries[i][j] and boundaries[i][j + 1] are the lower and upper bounds of cell j in dimension i . To access the lower bounds of all the cells in dimension i , use boundaries[i][:-1] , and to access all the upper bounds, use boundaries[i][1:] .","title":"bounds"},{"location":"reference/archives/#archives.GridArchive.coverage","text":"Get the coverage of the hypercube space. The coverage is calculated has the number of cells filled over the total space available. Returns: float \u2013 Filled cells over the total available.","title":"coverage"},{"location":"reference/archives/#archives.GridArchive.__getitem__","text":"Returns a dictionary with the descriptors as the keys. The values are the instances found. Note that some of the given keys may not be in the archive. Parameters: key ( array - like or descriptor ) \u2013 Descriptors of the instances that want to retrieve. Valid examples are \u2013 Raises: TypeError \u2013 If the key is an slice. Not allowed. ValueError \u2013 If the shape of the keys are not valid. Returns: dict \u2013 Returns a dict with the found instances. Source code in digneapy/archives/_grid_archive.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def __getitem__ ( self , key ): \"\"\"Returns a dictionary with the descriptors as the keys. The values are the instances found. Note that some of the given keys may not be in the archive. Args: key (array-like or descriptor): Descriptors of the instances that want to retrieve. Valid examples are: - archive[[0,11], [0,5]] --> Get the instances with the descriptors (0,11) and (0, 5) - archive[0,11] --> Get the instances at indices 0 and 11 Raises: TypeError: If the key is an slice. Not allowed. ValueError: If the shape of the keys are not valid. Returns: dict: Returns a dict with the found instances. \"\"\" if isinstance ( key , slice ): raise TypeError ( \"Slicing is not available in GridArchive. Use 1D index or descriptor-type indices\" ) descriptors = np . asarray ( key ) if descriptors . ndim == 1 : indices = descriptors elif descriptors . ndim == 2 and descriptors . shape [ 1 ] == len ( self . _dimensions ): indices = self . index_of ( descriptors ) . tolist () else : raise ValueError ( f \"Expected descriptors to be an array with shape \" f \"(batch_size, dimensions) (i.e. shape \" f \"(batch_size, { len ( self . _dimensions ) } )) but it had shape \" f \" { descriptors . shape } \" ) if isinstance ( indices , int ): indices = [ indices ] instances = [ self . _storage [ idx ] for idx in indices ] return instances","title":"__getitem__"},{"location":"reference/archives/#archives.GridArchive.__init__","text":"Creates a GridArchive instance Parameters: dimensions ( Sequence [ int ] ) \u2013 (array-like of int): Number of cells in each dimension of the ranges ( Sequence [ Tuple [ float ]] ) \u2013 (array-like of (float, float)): Upper and lower bound of each indicates the first dimension should have bounds \u2013 math: [-1,1] \u2013 math: [-2,2] (inclusive). ranges should be the same length as instances ( Optional [ Iterable [ Instance ]] , default: None ) \u2013 Instances to pre-initialise the archive. Defaults to None. eps ( float , default: 1e-06 ) \u2013 Due to floating point precision errors, we add a small epsilon when computing the archive indices in the \u2013 meth: index_of dtype ( str or data - type , default: float64 ) \u2013 Data type of the solutions, objectives, Raises: ValueError \u2013 dimensions and ranges are not the same length Source code in digneapy/archives/_grid_archive.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def __init__ ( self , dimensions : Sequence [ int ], ranges : Sequence [ Tuple [ float , float ]], instances : Optional [ Iterable [ Instance ]] = None , eps : float = 1e-6 , dtype = np . float64 , ): \"\"\"Creates a GridArchive instance Args: dimensions (Sequence[int]): (array-like of int): Number of cells in each dimension of the measure space, e.g. ``[20, 30, 40]`` indicates there should be 3 dimensions with 20, 30, and 40 cells. (The number of dimensions is implicitly defined in the length of this argument). ranges (Sequence[Tuple[float]]): (array-like of (float, float)): Upper and lower bound of each dimension of the measure space, e.g. ``[(-1, 1), (-2, 2)]`` indicates the first dimension should have bounds :math:`[-1,1]` (inclusive), and the second dimension should have bounds :math:`[-2,2]` (inclusive). ``ranges`` should be the same length as ``dims``. instances (Optional[Iterable[Instance]], optional): Instances to pre-initialise the archive. Defaults to None. eps (float, optional): Due to floating point precision errors, we add a small epsilon when computing the archive indices in the :meth:`index_of` method -- refer to the implementation `here. Defaults to 1e-6. dtype(str or data-type): Data type of the solutions, objectives, and measures. Raises: ValueError: ``dimensions`` and ``ranges`` are not the same length \"\"\" Archive . __init__ ( self , threshold = np . finfo ( np . float32 ) . max , dtype = dtype ) if len ( ranges ) == 0 or len ( dimensions ) == 0 : raise ValueError ( \"dimensions and ranges must have length >= 1\" ) if len ( ranges ) != len ( dimensions ): raise ValueError ( f \"len(dimensions) = { len ( dimensions ) } != len(ranges) = { len ( ranges ) } in GridArchive.__init__()\" ) self . _dimensions = np . asarray ( dimensions ) ranges = list ( zip ( * ranges )) self . _lower_bounds = np . array ( ranges [ 0 ], dtype = dtype ) self . _upper_bounds = np . array ( ranges [ 1 ], dtype = dtype ) self . _interval = self . _upper_bounds - self . _lower_bounds self . _eps = eps self . _cells = np . prod ( self . _dimensions , dtype = object ) self . _grid : Dict [ int , np . ndarray ] = {} self . _storage : Dict [ int , Instance ] = {} _bounds = [] for dimension , l_b , u_b in zip ( self . _dimensions , self . _lower_bounds , self . _upper_bounds ): _bounds . append ( np . linspace ( l_b , u_b , dimension )) self . _boundaries = np . asarray ( _bounds ) if instances is not None : self . extend ( instances )","title":"__init__"},{"location":"reference/archives/#archives.GridArchive.__iter__","text":"Iterates over the dictionary of instances Returns: Iterator \u2013 Yields position in the hypercube and instance located in such position Source code in digneapy/archives/_grid_archive.py 191 192 193 194 195 196 197 def __iter__ ( self ): \"\"\"Iterates over the dictionary of instances Returns: Iterator: Yields position in the hypercube and instance located in such position \"\"\" return iter ( self . _storage . values ())","title":"__iter__"},{"location":"reference/archives/#archives.GridArchive.append","text":"Inserts an Instance into the Grid Parameters: instance ( Instance ) \u2013 Instace to be inserted Raises: TypeError \u2013 instance is not a instance of the class Instance. Source code in digneapy/archives/_grid_archive.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def append ( self , instance : Instance , descriptor : Optional [ np . ndarray ] = None ): \"\"\"Inserts an Instance into the Grid Args: instance (Instance): Instace to be inserted Raises: TypeError: ``instance`` is not a instance of the class Instance. \"\"\" if not isinstance ( instance , Instance ): msg = \"Only objects of type Instance can be inserted into a GridArchive\" raise TypeError ( msg ) descriptor = ( np . asarray ( instance . descriptor ) if descriptor is None else descriptor ) index = self . index_of ([ descriptor ])[ 0 ] if index not in self . _grid or instance > self . _grid [ index ]: self . _grid [ index ] = descriptor self . _storage [ index ] = instance . clone ()","title":"append"},{"location":"reference/archives/#archives.GridArchive.extend","text":"Includes all the instances in iterable into the Grid Parameters: iterable ( Iterable [ Instance ] ) \u2013 Iterable of instances Source code in digneapy/archives/_grid_archive.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 def extend ( self , instances : Sequence [ Instance ], descriptors : Optional [ np . ndarray ] = None , * args , ** kwargs , ): \"\"\"Includes all the instances in iterable into the Grid Args: iterable (Iterable[Instance]): Iterable of instances \"\"\" if not all ( isinstance ( i , Instance ) for i in instances ): msg = \"Only objects of type Instance can be inserted into a GridArchive\" raise TypeError ( msg ) if descriptors is None : try : descriptors = np . asarray ([ i . descriptor for i in instances ]) except AttributeError as e : print ( \"Instances do not have a descriptor yet and the value descriptor is None\" ) raise ( e ) indices = self . index_of ( descriptors ) for idx , instance , descriptor in zip ( indices , instances , descriptors , strict = True ): if idx not in self . _grid or instance . fitness > self . _storage [ idx ] . fitness : self . _storage [ idx ] = instance . clone () self . _grid [ idx ] = descriptor","title":"extend"},{"location":"reference/archives/#archives.GridArchive.index_of","text":"Computes the indices of a batch of descriptors. Parameters: descriptors ( array - like ) \u2013 (batch_size, dimensions) array of descriptors for each instance Raises: ValueError \u2013 descriptors is not shape (batch_size, dimensions) Returns: \u2013 np.ndarray: (batch_size, ) array of integer indices representing the flattened grid coordinates. Source code in digneapy/archives/_grid_archive.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def index_of ( self , descriptors ): \"\"\"Computes the indices of a batch of descriptors. Args: descriptors (array-like): (batch_size, dimensions) array of descriptors for each instance Raises: ValueError: ``descriptors`` is not shape (batch_size, dimensions) Returns: np.ndarray: (batch_size, ) array of integer indices representing the flattened grid coordinates. \"\"\" if len ( descriptors ) == 0 : return np . empty ( 0 ) descriptors = np . asarray ( descriptors ) if ( descriptors . ndim == 1 and descriptors . shape [ 0 ] != len ( self . _dimensions ) or descriptors . ndim == 2 and descriptors . shape [ 1 ] != len ( self . _dimensions ) ): raise ValueError ( f \"Expected descriptors to be an array with shape \" f \"(batch_size, dimensions) (i.e. shape \" f \"(batch_size, { len ( self . _dimensions ) } )) but it had shape \" f \" { descriptors . shape } \" ) grid_indices = ( ( self . _dimensions * ( descriptors - self . _lower_bounds ) + self . _eps ) / self . _interval ) . astype ( int ) # Clip the indexes to make sure they are in the expected range for each dimension clipped = np . clip ( grid_indices , 0 , self . _dimensions - 1 ) return self . _grid_to_int_index ( clipped )","title":"index_of"},{"location":"reference/archives/#archives.GridArchive.purge_unfeasible","text":"Removes all the unfeasible instances from the grid Source code in digneapy/archives/_grid_archive.py 272 273 274 275 276 277 278 279 def purge_unfeasible ( self , attr : str = \"p\" ): \"\"\"Removes all the unfeasible instances from the grid\"\"\" keys_to_remove = [ i for i in self . _storage . keys () if getattr ( self . _storage [ i ], attr ) < 0 ] for i in keys_to_remove : del self . _grid [ i ] del self . _storage [ i ]","title":"purge_unfeasible"},{"location":"reference/archives/#archives.GridArchive.remove","text":"Removes all the instances with the matching descriptors in iterable from the grid Source code in digneapy/archives/_grid_archive.py 263 264 265 266 267 268 269 270 def remove ( self , descriptors : np . ndarray ): \"\"\"Removes all the instances with the matching descriptors in iterable from the grid\"\"\" indices_to_remove = self . index_of ( descriptors ) for index in indices_to_remove : if index in self . _grid : del self . _grid [ index ] del self . _storage [ index ]","title":"remove"},{"location":"reference/archives/_base_archive/","text":"@File : _base_archive.py @Time : 2024/06/07 12:17:34 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2024, Alejandro Marrero @Desc : None Archive Class Archive Stores a collection of diverse Instances Source code in digneapy/archives/_base_archive.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 class Archive : \"\"\"Class Archive Stores a collection of diverse Instances \"\"\" def __init__ ( self , threshold : float , instances : Optional [ Sequence [ Instance ]] = None , dtype = np . float64 , ): \"\"\"Creates an instance of a Archive (unstructured) for QD algorithms Args: threshold (float): Minimum value of sparseness to include an Instance into the archive. instances (Iterable[Instance], optional): Instances to initialise the archive. Defaults to None. \"\"\" self . _storage = { \"instances\" : [], \"descriptors\" : []} if instances : self . _storage [ \"instances\" ] . extend ( instances ) self . _storage [ \"descriptors\" ] . extend ( np . asarray ([ instance . descriptor for instance in instances ]) ) self . _threshold = threshold self . _dtype = dtype @property def instances ( self ) -> Sequence [ Instance ]: return self . _storage [ \"instances\" ] @property def descriptors ( self ) -> np . ndarray : return np . asarray ( self . _storage [ \"descriptors\" ]) @property def threshold ( self ): return self . _threshold @threshold . setter def threshold ( self , t : float ): try : t_f = float ( t ) except Exception : msg = f \"The threshold value { t } is not a float in 'threshold' setter of class { self . __class__ . __name__ } \" raise TypeError ( msg ) self . _threshold = t_f def __iter__ ( self ): return iter ( self . _storage [ \"instances\" ]) def __str__ ( self ): return f \"Archive(threshold= { self . _threshold } ,data=(| { len ( self ) } |))\" def __repr__ ( self ): return f \"Archive(threshold= { self . _threshold } ,data=(| { len ( self ) } |))\" def __array__ ( self , dtype = None , copy = None ) -> np . ndarray : \"\"\"Creates a ndarray with the descriptors >>> import numpy as np >>> descriptors = [list(range(d, d + 5)) for d in range(10)] >>> archive = Archive(descriptors) >>> np_archive = np.array(archive) >>> assert len(np_archive) == len(archive) >>> assert type(np_archive) == type(np.zeros(1)) \"\"\" return np . asarray ( self . _storage [ \"instances\" ], dtype = dtype , copy = copy ) def __eq__ ( self , other : Self ): \"\"\"Compares whether to Archives are equal >>> import copy >>> variables = [list(range(d, d + 5)) for d in range(10)] >>> instances = [Instance(variables=v, s=1.0) for v in variables] >>> archive = Archive(threshold=0.0, instances=instances) >>> empty_archive = Archive(threshold=0.0) >>> a1 = copy.copy(archive) >>> assert a1 == archive >>> assert empty_archive != archive \"\"\" return len ( self ) == len ( other ) and all ( np . array_equal ( a , b ) for a , b in zip ( self . _storage [ \"descriptors\" ], other . _storage [ \"descriptors\" ]) ) def __hash__ ( self ): from functools import reduce hashes = ( hash ( i ) for i in self . instances ) return reduce ( lambda a , b : a ^ b , hashes , 0 ) def __bool__ ( self ): \"\"\"Returns True if len(self) > 1 >>> descriptors = [list(range(d, d + 5)) for d in range(10)] >>> archive = Archive(threshold=0.0, instances=descriptors) >>> empty_archive = Archive(threshold=0.0) >>> assert archive >>> assert not empty_archive \"\"\" return len ( self ) != 0 def __len__ ( self ): return len ( self . instances ) def __getitem__ ( self , key ): if isinstance ( key , slice ): cls = type ( self ) # To facilitate subclassing return cls ( self . _threshold , self . instances [ key ]) index = operator . index ( key ) return self . _storage [ \"instances\" ][ index ] def extend ( self , instances : Sequence [ Instance ], novelty_scores : Optional [ np . ndarray ] = None , descriptors : Optional [ np . ndarray ] = None , ): \"\"\"Extends the current archive with all the individuals inside iterable that have a sparseness value greater than the archive threshold. Args: instances (Sequence[Instance]): Sequence of instances to be include in the archive. \"\"\" scores = ( novelty_scores if novelty_scores is not None else np . asarray ([ instance . s for instance in instances ]) ) descriptors = ( descriptors if descriptors is not None else np . asarray ([ instance . descriptor for instance in instances ]) ) to_insert = np . where ( scores >= self . threshold )[ 0 ] self . _storage [ \"instances\" ] . extend (( instances [ i ] for i in to_insert )) self . _storage [ \"descriptors\" ] . extend ( descriptors [ to_insert ]) def __format__ ( self , fmt_spec = \"\" ): variables = self outer_fmt = \"( {} )\" components = ( format ( c , fmt_spec ) for c in variables ) return outer_fmt . format ( \", \" . join ( components )) def asdict ( self ) -> dict : return { \"threshold\" : self . _threshold , \"instances\" : { i : instance . asdict () for i , instance in enumerate ( self . _storage [ \"instances\" ]) }, } def to_json ( self ) -> str : \"\"\"Converts the archive into a JSON object Returns: str: JSON str of the archive content \"\"\" return json . dumps ( self . asdict (), indent = 4 ) __array__ ( dtype = None , copy = None ) Creates a ndarray with the descriptors import numpy as np descriptors = [list(range(d, d + 5)) for d in range(10)] archive = Archive(descriptors) np_archive = np.array(archive) assert len(np_archive) == len(archive) assert type(np_archive) == type(np.zeros(1)) Source code in digneapy/archives/_base_archive.py 81 82 83 84 85 86 87 88 89 90 91 def __array__ ( self , dtype = None , copy = None ) -> np . ndarray : \"\"\"Creates a ndarray with the descriptors >>> import numpy as np >>> descriptors = [list(range(d, d + 5)) for d in range(10)] >>> archive = Archive(descriptors) >>> np_archive = np.array(archive) >>> assert len(np_archive) == len(archive) >>> assert type(np_archive) == type(np.zeros(1)) \"\"\" return np . asarray ( self . _storage [ \"instances\" ], dtype = dtype , copy = copy ) __bool__ () Returns True if len(self) > 1 descriptors = [list(range(d, d + 5)) for d in range(10)] archive = Archive(threshold=0.0, instances=descriptors) empty_archive = Archive(threshold=0.0) assert archive assert not empty_archive Source code in digneapy/archives/_base_archive.py 117 118 119 120 121 122 123 124 125 126 127 def __bool__ ( self ): \"\"\"Returns True if len(self) > 1 >>> descriptors = [list(range(d, d + 5)) for d in range(10)] >>> archive = Archive(threshold=0.0, instances=descriptors) >>> empty_archive = Archive(threshold=0.0) >>> assert archive >>> assert not empty_archive \"\"\" return len ( self ) != 0 __eq__ ( other ) Compares whether to Archives are equal import copy variables = [list(range(d, d + 5)) for d in range(10)] instances = [Instance(variables=v, s=1.0) for v in variables] archive = Archive(threshold=0.0, instances=instances) empty_archive = Archive(threshold=0.0) a1 = copy.copy(archive) assert a1 == archive assert empty_archive != archive Source code in digneapy/archives/_base_archive.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def __eq__ ( self , other : Self ): \"\"\"Compares whether to Archives are equal >>> import copy >>> variables = [list(range(d, d + 5)) for d in range(10)] >>> instances = [Instance(variables=v, s=1.0) for v in variables] >>> archive = Archive(threshold=0.0, instances=instances) >>> empty_archive = Archive(threshold=0.0) >>> a1 = copy.copy(archive) >>> assert a1 == archive >>> assert empty_archive != archive \"\"\" return len ( self ) == len ( other ) and all ( np . array_equal ( a , b ) for a , b in zip ( self . _storage [ \"descriptors\" ], other . _storage [ \"descriptors\" ]) ) __init__ ( threshold , instances = None , dtype = np . float64 ) Creates an instance of a Archive (unstructured) for QD algorithms Parameters: threshold ( float ) \u2013 Minimum value of sparseness to include an Instance into the archive. instances ( Iterable [ Instance ] , default: None ) \u2013 Instances to initialise the archive. Defaults to None. Source code in digneapy/archives/_base_archive.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , threshold : float , instances : Optional [ Sequence [ Instance ]] = None , dtype = np . float64 , ): \"\"\"Creates an instance of a Archive (unstructured) for QD algorithms Args: threshold (float): Minimum value of sparseness to include an Instance into the archive. instances (Iterable[Instance], optional): Instances to initialise the archive. Defaults to None. \"\"\" self . _storage = { \"instances\" : [], \"descriptors\" : []} if instances : self . _storage [ \"instances\" ] . extend ( instances ) self . _storage [ \"descriptors\" ] . extend ( np . asarray ([ instance . descriptor for instance in instances ]) ) self . _threshold = threshold self . _dtype = dtype extend ( instances , novelty_scores = None , descriptors = None ) Extends the current archive with all the individuals inside iterable that have a sparseness value greater than the archive threshold. Parameters: instances ( Sequence [ Instance ] ) \u2013 Sequence of instances to be include in the archive. Source code in digneapy/archives/_base_archive.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def extend ( self , instances : Sequence [ Instance ], novelty_scores : Optional [ np . ndarray ] = None , descriptors : Optional [ np . ndarray ] = None , ): \"\"\"Extends the current archive with all the individuals inside iterable that have a sparseness value greater than the archive threshold. Args: instances (Sequence[Instance]): Sequence of instances to be include in the archive. \"\"\" scores = ( novelty_scores if novelty_scores is not None else np . asarray ([ instance . s for instance in instances ]) ) descriptors = ( descriptors if descriptors is not None else np . asarray ([ instance . descriptor for instance in instances ]) ) to_insert = np . where ( scores >= self . threshold )[ 0 ] self . _storage [ \"instances\" ] . extend (( instances [ i ] for i in to_insert )) self . _storage [ \"descriptors\" ] . extend ( descriptors [ to_insert ]) to_json () Converts the archive into a JSON object Returns: str ( str ) \u2013 JSON str of the archive content Source code in digneapy/archives/_base_archive.py 181 182 183 184 185 186 187 188 def to_json ( self ) -> str : \"\"\"Converts the archive into a JSON object Returns: str: JSON str of the archive content \"\"\" return json . dumps ( self . asdict (), indent = 4 )","title":" base archive"},{"location":"reference/archives/_base_archive/#archives._base_archive.Archive","text":"Class Archive Stores a collection of diverse Instances Source code in digneapy/archives/_base_archive.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 class Archive : \"\"\"Class Archive Stores a collection of diverse Instances \"\"\" def __init__ ( self , threshold : float , instances : Optional [ Sequence [ Instance ]] = None , dtype = np . float64 , ): \"\"\"Creates an instance of a Archive (unstructured) for QD algorithms Args: threshold (float): Minimum value of sparseness to include an Instance into the archive. instances (Iterable[Instance], optional): Instances to initialise the archive. Defaults to None. \"\"\" self . _storage = { \"instances\" : [], \"descriptors\" : []} if instances : self . _storage [ \"instances\" ] . extend ( instances ) self . _storage [ \"descriptors\" ] . extend ( np . asarray ([ instance . descriptor for instance in instances ]) ) self . _threshold = threshold self . _dtype = dtype @property def instances ( self ) -> Sequence [ Instance ]: return self . _storage [ \"instances\" ] @property def descriptors ( self ) -> np . ndarray : return np . asarray ( self . _storage [ \"descriptors\" ]) @property def threshold ( self ): return self . _threshold @threshold . setter def threshold ( self , t : float ): try : t_f = float ( t ) except Exception : msg = f \"The threshold value { t } is not a float in 'threshold' setter of class { self . __class__ . __name__ } \" raise TypeError ( msg ) self . _threshold = t_f def __iter__ ( self ): return iter ( self . _storage [ \"instances\" ]) def __str__ ( self ): return f \"Archive(threshold= { self . _threshold } ,data=(| { len ( self ) } |))\" def __repr__ ( self ): return f \"Archive(threshold= { self . _threshold } ,data=(| { len ( self ) } |))\" def __array__ ( self , dtype = None , copy = None ) -> np . ndarray : \"\"\"Creates a ndarray with the descriptors >>> import numpy as np >>> descriptors = [list(range(d, d + 5)) for d in range(10)] >>> archive = Archive(descriptors) >>> np_archive = np.array(archive) >>> assert len(np_archive) == len(archive) >>> assert type(np_archive) == type(np.zeros(1)) \"\"\" return np . asarray ( self . _storage [ \"instances\" ], dtype = dtype , copy = copy ) def __eq__ ( self , other : Self ): \"\"\"Compares whether to Archives are equal >>> import copy >>> variables = [list(range(d, d + 5)) for d in range(10)] >>> instances = [Instance(variables=v, s=1.0) for v in variables] >>> archive = Archive(threshold=0.0, instances=instances) >>> empty_archive = Archive(threshold=0.0) >>> a1 = copy.copy(archive) >>> assert a1 == archive >>> assert empty_archive != archive \"\"\" return len ( self ) == len ( other ) and all ( np . array_equal ( a , b ) for a , b in zip ( self . _storage [ \"descriptors\" ], other . _storage [ \"descriptors\" ]) ) def __hash__ ( self ): from functools import reduce hashes = ( hash ( i ) for i in self . instances ) return reduce ( lambda a , b : a ^ b , hashes , 0 ) def __bool__ ( self ): \"\"\"Returns True if len(self) > 1 >>> descriptors = [list(range(d, d + 5)) for d in range(10)] >>> archive = Archive(threshold=0.0, instances=descriptors) >>> empty_archive = Archive(threshold=0.0) >>> assert archive >>> assert not empty_archive \"\"\" return len ( self ) != 0 def __len__ ( self ): return len ( self . instances ) def __getitem__ ( self , key ): if isinstance ( key , slice ): cls = type ( self ) # To facilitate subclassing return cls ( self . _threshold , self . instances [ key ]) index = operator . index ( key ) return self . _storage [ \"instances\" ][ index ] def extend ( self , instances : Sequence [ Instance ], novelty_scores : Optional [ np . ndarray ] = None , descriptors : Optional [ np . ndarray ] = None , ): \"\"\"Extends the current archive with all the individuals inside iterable that have a sparseness value greater than the archive threshold. Args: instances (Sequence[Instance]): Sequence of instances to be include in the archive. \"\"\" scores = ( novelty_scores if novelty_scores is not None else np . asarray ([ instance . s for instance in instances ]) ) descriptors = ( descriptors if descriptors is not None else np . asarray ([ instance . descriptor for instance in instances ]) ) to_insert = np . where ( scores >= self . threshold )[ 0 ] self . _storage [ \"instances\" ] . extend (( instances [ i ] for i in to_insert )) self . _storage [ \"descriptors\" ] . extend ( descriptors [ to_insert ]) def __format__ ( self , fmt_spec = \"\" ): variables = self outer_fmt = \"( {} )\" components = ( format ( c , fmt_spec ) for c in variables ) return outer_fmt . format ( \", \" . join ( components )) def asdict ( self ) -> dict : return { \"threshold\" : self . _threshold , \"instances\" : { i : instance . asdict () for i , instance in enumerate ( self . _storage [ \"instances\" ]) }, } def to_json ( self ) -> str : \"\"\"Converts the archive into a JSON object Returns: str: JSON str of the archive content \"\"\" return json . dumps ( self . asdict (), indent = 4 )","title":"Archive"},{"location":"reference/archives/_base_archive/#archives._base_archive.Archive.__array__","text":"Creates a ndarray with the descriptors import numpy as np descriptors = [list(range(d, d + 5)) for d in range(10)] archive = Archive(descriptors) np_archive = np.array(archive) assert len(np_archive) == len(archive) assert type(np_archive) == type(np.zeros(1)) Source code in digneapy/archives/_base_archive.py 81 82 83 84 85 86 87 88 89 90 91 def __array__ ( self , dtype = None , copy = None ) -> np . ndarray : \"\"\"Creates a ndarray with the descriptors >>> import numpy as np >>> descriptors = [list(range(d, d + 5)) for d in range(10)] >>> archive = Archive(descriptors) >>> np_archive = np.array(archive) >>> assert len(np_archive) == len(archive) >>> assert type(np_archive) == type(np.zeros(1)) \"\"\" return np . asarray ( self . _storage [ \"instances\" ], dtype = dtype , copy = copy )","title":"__array__"},{"location":"reference/archives/_base_archive/#archives._base_archive.Archive.__bool__","text":"Returns True if len(self) > 1 descriptors = [list(range(d, d + 5)) for d in range(10)] archive = Archive(threshold=0.0, instances=descriptors) empty_archive = Archive(threshold=0.0) assert archive assert not empty_archive Source code in digneapy/archives/_base_archive.py 117 118 119 120 121 122 123 124 125 126 127 def __bool__ ( self ): \"\"\"Returns True if len(self) > 1 >>> descriptors = [list(range(d, d + 5)) for d in range(10)] >>> archive = Archive(threshold=0.0, instances=descriptors) >>> empty_archive = Archive(threshold=0.0) >>> assert archive >>> assert not empty_archive \"\"\" return len ( self ) != 0","title":"__bool__"},{"location":"reference/archives/_base_archive/#archives._base_archive.Archive.__eq__","text":"Compares whether to Archives are equal import copy variables = [list(range(d, d + 5)) for d in range(10)] instances = [Instance(variables=v, s=1.0) for v in variables] archive = Archive(threshold=0.0, instances=instances) empty_archive = Archive(threshold=0.0) a1 = copy.copy(archive) assert a1 == archive assert empty_archive != archive Source code in digneapy/archives/_base_archive.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def __eq__ ( self , other : Self ): \"\"\"Compares whether to Archives are equal >>> import copy >>> variables = [list(range(d, d + 5)) for d in range(10)] >>> instances = [Instance(variables=v, s=1.0) for v in variables] >>> archive = Archive(threshold=0.0, instances=instances) >>> empty_archive = Archive(threshold=0.0) >>> a1 = copy.copy(archive) >>> assert a1 == archive >>> assert empty_archive != archive \"\"\" return len ( self ) == len ( other ) and all ( np . array_equal ( a , b ) for a , b in zip ( self . _storage [ \"descriptors\" ], other . _storage [ \"descriptors\" ]) )","title":"__eq__"},{"location":"reference/archives/_base_archive/#archives._base_archive.Archive.__init__","text":"Creates an instance of a Archive (unstructured) for QD algorithms Parameters: threshold ( float ) \u2013 Minimum value of sparseness to include an Instance into the archive. instances ( Iterable [ Instance ] , default: None ) \u2013 Instances to initialise the archive. Defaults to None. Source code in digneapy/archives/_base_archive.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , threshold : float , instances : Optional [ Sequence [ Instance ]] = None , dtype = np . float64 , ): \"\"\"Creates an instance of a Archive (unstructured) for QD algorithms Args: threshold (float): Minimum value of sparseness to include an Instance into the archive. instances (Iterable[Instance], optional): Instances to initialise the archive. Defaults to None. \"\"\" self . _storage = { \"instances\" : [], \"descriptors\" : []} if instances : self . _storage [ \"instances\" ] . extend ( instances ) self . _storage [ \"descriptors\" ] . extend ( np . asarray ([ instance . descriptor for instance in instances ]) ) self . _threshold = threshold self . _dtype = dtype","title":"__init__"},{"location":"reference/archives/_base_archive/#archives._base_archive.Archive.extend","text":"Extends the current archive with all the individuals inside iterable that have a sparseness value greater than the archive threshold. Parameters: instances ( Sequence [ Instance ] ) \u2013 Sequence of instances to be include in the archive. Source code in digneapy/archives/_base_archive.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def extend ( self , instances : Sequence [ Instance ], novelty_scores : Optional [ np . ndarray ] = None , descriptors : Optional [ np . ndarray ] = None , ): \"\"\"Extends the current archive with all the individuals inside iterable that have a sparseness value greater than the archive threshold. Args: instances (Sequence[Instance]): Sequence of instances to be include in the archive. \"\"\" scores = ( novelty_scores if novelty_scores is not None else np . asarray ([ instance . s for instance in instances ]) ) descriptors = ( descriptors if descriptors is not None else np . asarray ([ instance . descriptor for instance in instances ]) ) to_insert = np . where ( scores >= self . threshold )[ 0 ] self . _storage [ \"instances\" ] . extend (( instances [ i ] for i in to_insert )) self . _storage [ \"descriptors\" ] . extend ( descriptors [ to_insert ])","title":"extend"},{"location":"reference/archives/_base_archive/#archives._base_archive.Archive.to_json","text":"Converts the archive into a JSON object Returns: str ( str ) \u2013 JSON str of the archive content Source code in digneapy/archives/_base_archive.py 181 182 183 184 185 186 187 188 def to_json ( self ) -> str : \"\"\"Converts the archive into a JSON object Returns: str: JSON str of the archive content \"\"\" return json . dumps ( self . asdict (), indent = 4 )","title":"to_json"},{"location":"reference/archives/_cvt_archive/","text":"@File : _cvt_archive.py @Time : 2024/09/18 14:44:44 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2024, Alejandro Marrero @Desc : None CVTArchive Bases: GridArchive , RNG An Archive that divides a high-dimensional measure space into k homogeneous geometric regions. Based on the paper from Vassiliades et al (2018) https://ieeexplore.ieee.org/document/8000667 The computational complexity of the method we provide for constructing the CVT (in Algorithm 1) is O(ndki), where n is the number of d-dimensional samples to be clustered, k is the number of clusters, and i is the number of iterations needed until convergence Source code in digneapy/archives/_cvt_archive.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 class CVTArchive ( GridArchive , RNG ): \"\"\"An Archive that divides a high-dimensional measure space into k homogeneous geometric regions. Based on the paper from Vassiliades et al (2018) <https://ieeexplore.ieee.org/document/8000667> > The computational complexity of the method we provide for constructing the CVT (in Algorithm 1) is O(ndki), > where n is the number of d-dimensional samples to be clustered, k is the number of clusters, > and i is the number of iterations needed until convergence \"\"\" def __init__ ( self , k : int , ranges : Sequence [ Tuple [ float , float ]], n_samples : int , centroids : Optional [ npt . NDArray | str ] = None , samples : Optional [ npt . NDArray | str ] = None , dtype = np . float64 , seed : int = 42 , ): \"\"\"Creates a CVTArchive object Args: k (int): Number of centroids (regions) to create ranges (Sequence[Tuple[float, float]]): Ranges of the measure space. Upper and lower bound of each dimension of the measure space, e.g. ``[(-1, 1), (-2, 2)]`` indicates the first dimension should have bounds :math:`[-1,1]` (inclusive), and the second dimension should have bounds :math:`[-2,2]` (inclusive). The legnth of ``ranges`` indicates the number of dimensions of the measure space. n_samples (int): Number of samples to generate before calculating the centroids. centroids (Optional[npt.NDArray | str], optional): Precalculated centroids for the archive. The options are a np.ndarray with the values of ``k`` centroids or a .txt with the centroids to be loaded by Numpy. Defaults to None. samples (Optional[npt.NDArray | str], optional): Precalculated samples for the archive. The options are a np.ndarray with the values of ``n_samples`` samples or a .txt with the samples to be loaded by Numpy. Defaults to None. Raises: ValueError: If len(ranges) <= 0. ValueError: If the number of samples is less than zero or less than the number of regions (k). ValueError: If the number of regions is less than zero. ValueError: If the samples file cannot be loaded. ValueError: If given a samples np.ndarray the number of samples in the file is different from the number of expected samples (n_samples). ValueError: If the centroids file cannot be loaded. ValueError: If given a centroids np.ndarray the number of centroids in the file is different from the number of regions (k). \"\"\" if k <= 0 : raise ValueError ( f \"The number of regions (k = { k } ) must be >= 1\" ) if len ( ranges ) <= 0 : raise ValueError ( f \"ranges must have length >= 1 and it has length { len ( ranges ) } \" ) if n_samples <= 0 or n_samples < k : raise ValueError ( f \"The number of samples (n_samples = { n_samples } ) must be >= 1 and >= regions (k = { k } )\" ) GridArchive . __init__ ( self , dimensions = ( 1 ,) * len ( ranges ), ranges = ranges , dtype = dtype ) self . _dimensions = len ( ranges ) ranges = list ( zip ( * ranges )) self . _lower_bounds = np . array ( ranges [ 0 ], dtype = self . _dtype ) self . _upper_bounds = np . array ( ranges [ 1 ], dtype = self . _dtype ) self . _interval = self . _upper_bounds - self . _lower_bounds self . _k = k self . _n_samples = n_samples self . _samples = None self . _centroids = None self . initialize_rng ( seed = seed ) self . _kmeans = KMeans ( n_clusters = self . _k , n_init = 1 , random_state = self . _seed ) # Loading samples if given if samples is not None : if isinstance ( samples , str ): try : self . _samples = np . load ( samples ) self . _n_samples = len ( self . _samples ) except Exception as _ : raise ValueError ( f \"Error in CVTArchive.__init__() loading the samples file { samples } .\" ) elif isinstance ( samples , np . ndarray ) and len ( samples ) != n_samples : raise ValueError ( f \"The number of samples { len ( samples ) } must be equal to the number of expected samples (n_samples = { n_samples } )\" ) else : self . _samples = np . asarray ( samples ) if centroids is not None : if isinstance ( centroids , str ): try : self . _centroids = np . load ( centroids ) self . _k = len ( self . _centroids ) except Exception as _ : raise ValueError ( f \"Error in CVTArchive.__init__() loading the centroids file { centroids } .\" ) elif isinstance ( centroids , np . ndarray ) and len ( centroids ) != k : raise ValueError ( f \"The number of centroids { len ( centroids ) } must be equal to the number of regions (k = { self . _k } )\" ) else : self . _centroids = np . asarray ( centroids ) else : # Generate centroids if self . _samples is None : # Generate uniform samples if not given rng = np . random . default_rng ( seed = self . _seed ) self . _samples = rng . uniform ( low = self . _lower_bounds , high = self . _upper_bounds , size = ( self . _n_samples , self . _dimensions ), ) self . _kmeans . fit ( self . _samples ) self . _centroids = self . _kmeans . cluster_centers_ self . _kdtree = KDTree ( self . _centroids , metric = \"euclidean\" ) @property def dimensions ( self ) -> int : \"\"\"Dimensions of the measure space used Returns: int: Dimensions of the measure space used \"\"\" return self . _dimensions @property def samples ( self ) -> np . ndarray : \"\"\"Returns the samples used to generate the centroids Returns: np.ndarray: Samples \"\"\" return self . _samples @property def centroids ( self ) -> np . ndarray : \"\"\"Returns k centroids calculated from the samples Returns: np.ndarray: K d-dimensional centroids \"\"\" return self . _centroids @property def regions ( self ) -> int : \"\"\"Number of regions (k) of centroids in the CVTArchive Returns: int: k \"\"\" return self . _k @property def bounds ( self ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Tuple with the lower and upper bounds of the measure space The first value is the lower bounds and the second value is the upper bounds. Each value is a list with the corresponding lower/upper bound of the ith dimension in the measure space \"\"\" return ( self . _lower_bounds , self . _upper_bounds ) @property def instances ( self ) -> list [ Instance ]: return list ( self . _storage . values ()) def __str__ ( self ): return f \"CVArchive(dim= { self . _dimensions } ,regions= { self . _k } ,centroids= { self . _centroids } )\" def __repr__ ( self ): return f \"CVArchive(dim= { self . _dimensions } ,regions= { self . _k } ,centroids= { self . _centroids } )\" def __iter__ ( self ): \"\"\"Iterates over the dictionary of instances Returns: Iterator: Yields position in the hypercube and instance located in such position \"\"\" return iter ( self . _storage . values ()) def lower_i ( self , i ) -> np . float64 : if i < 0 or i > len ( self . _lower_bounds ): msg = f \"index { i } is out of bounds. Valid values are [0- { len ( self . _lower_bounds ) } ]\" raise ValueError ( msg ) return self . _lower_bounds [ i ] def upper_i ( self , i ) -> np . float64 : if i < 0 or i > len ( self . _upper_bounds ): msg = f \"index { i } is out of bounds. Valid values are [0- { len ( self . _upper_bounds ) } ]\" raise ValueError ( msg ) return self . _upper_bounds [ i ] def index_of ( self , descriptors ) -> np . ndarray : \"\"\"Computes the indeces of a batch of descriptors. Args: descriptors (array-like): (batch_size, dimensions) array of descriptors for each instance Raises: ValueError: ``descriptors`` is not shape (batch_size, dimensions) Returns: np.ndarray: (batch_size, ) array of integer indices representing the flattened grid coordinates. \"\"\" descriptors = np . array ( descriptors ) if len ( descriptors ) == 0 : return np . empty ( 0 ) elif ( descriptors . ndim == 1 and descriptors . shape [ 0 ] != self . _dimensions or descriptors . ndim == 2 and descriptors . shape [ 1 ] != self . _dimensions ): raise ValueError ( f \"Expected descriptors to be an array with shape \" f \"(batch_size, dimensions) (i.e. shape \" f \"(batch_size, { self . _dimensions } )) but it had shape \" f \" { descriptors . shape } \" ) indices = self . _kdtree . query ( descriptors , return_distance = False ) indices = indices [:, 0 ] return indices . astype ( np . int32 ) def to_file ( self , file_pattern : str = \"CVTArchive\" ): \"\"\"Saves the centroids and the samples of the CVTArchive to .npy files Each attribute is saved in its own filename. Therefore, file_pattern is expected not to contain any extension Args: file_pattern (str, optional): Pattern of the expected filenames. Defaults to \"CVTArchive\". \"\"\" np . save ( f \" { file_pattern } _centroids.npy\" , self . _centroids ) np . save ( f \" { file_pattern } _samples.npy\" , self . _samples ) @classmethod def load_from_json ( cls , filename : str ): \"\"\"Creates a CVTArchive object from the content of a previously created JSON file Args: filename (str): Filename of the JSON file with the CVTArchive information Raises: ValueError: If there's any error while loading the file. (IOError) ValueError: If the JSON file does not contain all the expected keys Returns: Self: Returns a CVTArchive object \"\"\" expected_keys = { \"dimensions\" , \"n_samples\" , \"regions\" , \"lbs\" , \"ubs\" , \"centroids\" , \"samples\" , } try : with open ( filename , \"r\" ) as file : json_data = json . load ( file ) if expected_keys != json_data . keys (): raise ValueError ( f \"The JSON file does not contain all the minimum expected keys. Expected keys are { expected_keys } and got { json_data . keys () } \" ) _ranges = [ ( l_i , u_i ) for l_i , u_i in zip ( json_data [ \"lbs\" ], json_data [ \"ubs\" ]) ] new_archive = cls ( k = json_data [ \"regions\" ], ranges = _ranges , n_samples = json_data [ \"n_samples\" ], centroids = json_data [ \"centroids\" ], samples = json_data [ \"samples\" ], ) return new_archive except IOError as io : raise ValueError ( f \"Error opening file { filename } . Reason -> { io . strerror } \" ) def asdict ( self ) -> dict : return { \"dimensions\" : self . _dimensions , \"n_samples\" : self . _n_samples , \"regions\" : self . _k , \"lbs\" : self . _lower_bounds . tolist (), \"ubs\" : self . _upper_bounds . tolist (), \"centroids\" : self . _centroids . tolist (), \"samples\" : self . _samples . tolist (), \"instances\" : { i : instance . asdict () for i , instance in enumerate ( self . _storage . values ()) }, } def to_json ( self , filename : Optional [ str ] = None ) -> str : \"\"\"Returns the content of the CVTArchive in JSON format. Returns: str: String in JSON format with the content of the CVTArchive \"\"\" json_data = json . dumps ( self . asdict (), indent = 4 ) if filename is not None : filename = ( f \" { filename } .json\" if not filename . endswith ( \".json\" ) else filename ) with open ( filename , \"w\" ) as f : f . write ( json_data ) return json_data bounds property Tuple with the lower and upper bounds of the measure space The first value is the lower bounds and the second value is the upper bounds. Each value is a list with the corresponding lower/upper bound of the ith dimension in the measure space centroids property Returns k centroids calculated from the samples Returns: ndarray \u2013 np.ndarray: K d-dimensional centroids dimensions property Dimensions of the measure space used Returns: int ( int ) \u2013 Dimensions of the measure space used regions property Number of regions (k) of centroids in the CVTArchive Returns: int ( int ) \u2013 k samples property Returns the samples used to generate the centroids Returns: ndarray \u2013 np.ndarray: Samples __init__ ( k , ranges , n_samples , centroids = None , samples = None , dtype = np . float64 , seed = 42 ) Creates a CVTArchive object Parameters: k ( int ) \u2013 Number of centroids (regions) to create ranges ( Sequence [ Tuple [ float , float ]] ) \u2013 Ranges of the measure space. Upper and lower bound of each indicates the first dimension should have bounds \u2013 math: [-1,1] \u2013 math: [-2,2] (inclusive). The legnth of ranges indicates the number of dimensions of the measure space. n_samples ( int ) \u2013 Number of samples to generate before calculating the centroids. centroids ( Optional [ NDArray | str ] , default: None ) \u2013 Precalculated centroids for the archive. samples ( Optional [ NDArray | str ] , default: None ) \u2013 Precalculated samples for the archive. Raises: ValueError \u2013 If len(ranges) <= 0. ValueError \u2013 If the number of samples is less than zero or less than the number of regions (k). ValueError \u2013 If the number of regions is less than zero. ValueError \u2013 If the samples file cannot be loaded. ValueError \u2013 If given a samples np.ndarray the number of samples in the file is different from the number of expected samples (n_samples). ValueError \u2013 If the centroids file cannot be loaded. ValueError \u2013 If given a centroids np.ndarray the number of centroids in the file is different from the number of regions (k). Source code in digneapy/archives/_cvt_archive.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def __init__ ( self , k : int , ranges : Sequence [ Tuple [ float , float ]], n_samples : int , centroids : Optional [ npt . NDArray | str ] = None , samples : Optional [ npt . NDArray | str ] = None , dtype = np . float64 , seed : int = 42 , ): \"\"\"Creates a CVTArchive object Args: k (int): Number of centroids (regions) to create ranges (Sequence[Tuple[float, float]]): Ranges of the measure space. Upper and lower bound of each dimension of the measure space, e.g. ``[(-1, 1), (-2, 2)]`` indicates the first dimension should have bounds :math:`[-1,1]` (inclusive), and the second dimension should have bounds :math:`[-2,2]` (inclusive). The legnth of ``ranges`` indicates the number of dimensions of the measure space. n_samples (int): Number of samples to generate before calculating the centroids. centroids (Optional[npt.NDArray | str], optional): Precalculated centroids for the archive. The options are a np.ndarray with the values of ``k`` centroids or a .txt with the centroids to be loaded by Numpy. Defaults to None. samples (Optional[npt.NDArray | str], optional): Precalculated samples for the archive. The options are a np.ndarray with the values of ``n_samples`` samples or a .txt with the samples to be loaded by Numpy. Defaults to None. Raises: ValueError: If len(ranges) <= 0. ValueError: If the number of samples is less than zero or less than the number of regions (k). ValueError: If the number of regions is less than zero. ValueError: If the samples file cannot be loaded. ValueError: If given a samples np.ndarray the number of samples in the file is different from the number of expected samples (n_samples). ValueError: If the centroids file cannot be loaded. ValueError: If given a centroids np.ndarray the number of centroids in the file is different from the number of regions (k). \"\"\" if k <= 0 : raise ValueError ( f \"The number of regions (k = { k } ) must be >= 1\" ) if len ( ranges ) <= 0 : raise ValueError ( f \"ranges must have length >= 1 and it has length { len ( ranges ) } \" ) if n_samples <= 0 or n_samples < k : raise ValueError ( f \"The number of samples (n_samples = { n_samples } ) must be >= 1 and >= regions (k = { k } )\" ) GridArchive . __init__ ( self , dimensions = ( 1 ,) * len ( ranges ), ranges = ranges , dtype = dtype ) self . _dimensions = len ( ranges ) ranges = list ( zip ( * ranges )) self . _lower_bounds = np . array ( ranges [ 0 ], dtype = self . _dtype ) self . _upper_bounds = np . array ( ranges [ 1 ], dtype = self . _dtype ) self . _interval = self . _upper_bounds - self . _lower_bounds self . _k = k self . _n_samples = n_samples self . _samples = None self . _centroids = None self . initialize_rng ( seed = seed ) self . _kmeans = KMeans ( n_clusters = self . _k , n_init = 1 , random_state = self . _seed ) # Loading samples if given if samples is not None : if isinstance ( samples , str ): try : self . _samples = np . load ( samples ) self . _n_samples = len ( self . _samples ) except Exception as _ : raise ValueError ( f \"Error in CVTArchive.__init__() loading the samples file { samples } .\" ) elif isinstance ( samples , np . ndarray ) and len ( samples ) != n_samples : raise ValueError ( f \"The number of samples { len ( samples ) } must be equal to the number of expected samples (n_samples = { n_samples } )\" ) else : self . _samples = np . asarray ( samples ) if centroids is not None : if isinstance ( centroids , str ): try : self . _centroids = np . load ( centroids ) self . _k = len ( self . _centroids ) except Exception as _ : raise ValueError ( f \"Error in CVTArchive.__init__() loading the centroids file { centroids } .\" ) elif isinstance ( centroids , np . ndarray ) and len ( centroids ) != k : raise ValueError ( f \"The number of centroids { len ( centroids ) } must be equal to the number of regions (k = { self . _k } )\" ) else : self . _centroids = np . asarray ( centroids ) else : # Generate centroids if self . _samples is None : # Generate uniform samples if not given rng = np . random . default_rng ( seed = self . _seed ) self . _samples = rng . uniform ( low = self . _lower_bounds , high = self . _upper_bounds , size = ( self . _n_samples , self . _dimensions ), ) self . _kmeans . fit ( self . _samples ) self . _centroids = self . _kmeans . cluster_centers_ self . _kdtree = KDTree ( self . _centroids , metric = \"euclidean\" ) __iter__ () Iterates over the dictionary of instances Returns: Iterator \u2013 Yields position in the hypercube and instance located in such position Source code in digneapy/archives/_cvt_archive.py 201 202 203 204 205 206 207 def __iter__ ( self ): \"\"\"Iterates over the dictionary of instances Returns: Iterator: Yields position in the hypercube and instance located in such position \"\"\" return iter ( self . _storage . values ()) index_of ( descriptors ) Computes the indeces of a batch of descriptors. Parameters: descriptors ( array - like ) \u2013 (batch_size, dimensions) array of descriptors for each instance Raises: ValueError \u2013 descriptors is not shape (batch_size, dimensions) Returns: ndarray \u2013 np.ndarray: (batch_size, ) array of integer indices representing the flattened grid coordinates. Source code in digneapy/archives/_cvt_archive.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 def index_of ( self , descriptors ) -> np . ndarray : \"\"\"Computes the indeces of a batch of descriptors. Args: descriptors (array-like): (batch_size, dimensions) array of descriptors for each instance Raises: ValueError: ``descriptors`` is not shape (batch_size, dimensions) Returns: np.ndarray: (batch_size, ) array of integer indices representing the flattened grid coordinates. \"\"\" descriptors = np . array ( descriptors ) if len ( descriptors ) == 0 : return np . empty ( 0 ) elif ( descriptors . ndim == 1 and descriptors . shape [ 0 ] != self . _dimensions or descriptors . ndim == 2 and descriptors . shape [ 1 ] != self . _dimensions ): raise ValueError ( f \"Expected descriptors to be an array with shape \" f \"(batch_size, dimensions) (i.e. shape \" f \"(batch_size, { self . _dimensions } )) but it had shape \" f \" { descriptors . shape } \" ) indices = self . _kdtree . query ( descriptors , return_distance = False ) indices = indices [:, 0 ] return indices . astype ( np . int32 ) load_from_json ( filename ) classmethod Creates a CVTArchive object from the content of a previously created JSON file Parameters: filename ( str ) \u2013 Filename of the JSON file with the CVTArchive information Raises: ValueError \u2013 If there's any error while loading the file. (IOError) ValueError \u2013 If the JSON file does not contain all the expected keys Returns: Self \u2013 Returns a CVTArchive object Source code in digneapy/archives/_cvt_archive.py 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 @classmethod def load_from_json ( cls , filename : str ): \"\"\"Creates a CVTArchive object from the content of a previously created JSON file Args: filename (str): Filename of the JSON file with the CVTArchive information Raises: ValueError: If there's any error while loading the file. (IOError) ValueError: If the JSON file does not contain all the expected keys Returns: Self: Returns a CVTArchive object \"\"\" expected_keys = { \"dimensions\" , \"n_samples\" , \"regions\" , \"lbs\" , \"ubs\" , \"centroids\" , \"samples\" , } try : with open ( filename , \"r\" ) as file : json_data = json . load ( file ) if expected_keys != json_data . keys (): raise ValueError ( f \"The JSON file does not contain all the minimum expected keys. Expected keys are { expected_keys } and got { json_data . keys () } \" ) _ranges = [ ( l_i , u_i ) for l_i , u_i in zip ( json_data [ \"lbs\" ], json_data [ \"ubs\" ]) ] new_archive = cls ( k = json_data [ \"regions\" ], ranges = _ranges , n_samples = json_data [ \"n_samples\" ], centroids = json_data [ \"centroids\" ], samples = json_data [ \"samples\" ], ) return new_archive except IOError as io : raise ValueError ( f \"Error opening file { filename } . Reason -> { io . strerror } \" ) to_file ( file_pattern = 'CVTArchive' ) Saves the centroids and the samples of the CVTArchive to .npy files Each attribute is saved in its own filename. Therefore, file_pattern is expected not to contain any extension Parameters: file_pattern ( str , default: 'CVTArchive' ) \u2013 Pattern of the expected filenames. Defaults to \"CVTArchive\". Source code in digneapy/archives/_cvt_archive.py 254 255 256 257 258 259 260 261 262 263 def to_file ( self , file_pattern : str = \"CVTArchive\" ): \"\"\"Saves the centroids and the samples of the CVTArchive to .npy files Each attribute is saved in its own filename. Therefore, file_pattern is expected not to contain any extension Args: file_pattern (str, optional): Pattern of the expected filenames. Defaults to \"CVTArchive\". \"\"\" np . save ( f \" { file_pattern } _centroids.npy\" , self . _centroids ) np . save ( f \" { file_pattern } _samples.npy\" , self . _samples ) to_json ( filename = None ) Returns the content of the CVTArchive in JSON format. Returns: str ( str ) \u2013 String in JSON format with the content of the CVTArchive Source code in digneapy/archives/_cvt_archive.py 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 def to_json ( self , filename : Optional [ str ] = None ) -> str : \"\"\"Returns the content of the CVTArchive in JSON format. Returns: str: String in JSON format with the content of the CVTArchive \"\"\" json_data = json . dumps ( self . asdict (), indent = 4 ) if filename is not None : filename = ( f \" { filename } .json\" if not filename . endswith ( \".json\" ) else filename ) with open ( filename , \"w\" ) as f : f . write ( json_data ) return json_data","title":" cvt archive"},{"location":"reference/archives/_cvt_archive/#archives._cvt_archive.CVTArchive","text":"Bases: GridArchive , RNG An Archive that divides a high-dimensional measure space into k homogeneous geometric regions. Based on the paper from Vassiliades et al (2018) https://ieeexplore.ieee.org/document/8000667 The computational complexity of the method we provide for constructing the CVT (in Algorithm 1) is O(ndki), where n is the number of d-dimensional samples to be clustered, k is the number of clusters, and i is the number of iterations needed until convergence Source code in digneapy/archives/_cvt_archive.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 class CVTArchive ( GridArchive , RNG ): \"\"\"An Archive that divides a high-dimensional measure space into k homogeneous geometric regions. Based on the paper from Vassiliades et al (2018) <https://ieeexplore.ieee.org/document/8000667> > The computational complexity of the method we provide for constructing the CVT (in Algorithm 1) is O(ndki), > where n is the number of d-dimensional samples to be clustered, k is the number of clusters, > and i is the number of iterations needed until convergence \"\"\" def __init__ ( self , k : int , ranges : Sequence [ Tuple [ float , float ]], n_samples : int , centroids : Optional [ npt . NDArray | str ] = None , samples : Optional [ npt . NDArray | str ] = None , dtype = np . float64 , seed : int = 42 , ): \"\"\"Creates a CVTArchive object Args: k (int): Number of centroids (regions) to create ranges (Sequence[Tuple[float, float]]): Ranges of the measure space. Upper and lower bound of each dimension of the measure space, e.g. ``[(-1, 1), (-2, 2)]`` indicates the first dimension should have bounds :math:`[-1,1]` (inclusive), and the second dimension should have bounds :math:`[-2,2]` (inclusive). The legnth of ``ranges`` indicates the number of dimensions of the measure space. n_samples (int): Number of samples to generate before calculating the centroids. centroids (Optional[npt.NDArray | str], optional): Precalculated centroids for the archive. The options are a np.ndarray with the values of ``k`` centroids or a .txt with the centroids to be loaded by Numpy. Defaults to None. samples (Optional[npt.NDArray | str], optional): Precalculated samples for the archive. The options are a np.ndarray with the values of ``n_samples`` samples or a .txt with the samples to be loaded by Numpy. Defaults to None. Raises: ValueError: If len(ranges) <= 0. ValueError: If the number of samples is less than zero or less than the number of regions (k). ValueError: If the number of regions is less than zero. ValueError: If the samples file cannot be loaded. ValueError: If given a samples np.ndarray the number of samples in the file is different from the number of expected samples (n_samples). ValueError: If the centroids file cannot be loaded. ValueError: If given a centroids np.ndarray the number of centroids in the file is different from the number of regions (k). \"\"\" if k <= 0 : raise ValueError ( f \"The number of regions (k = { k } ) must be >= 1\" ) if len ( ranges ) <= 0 : raise ValueError ( f \"ranges must have length >= 1 and it has length { len ( ranges ) } \" ) if n_samples <= 0 or n_samples < k : raise ValueError ( f \"The number of samples (n_samples = { n_samples } ) must be >= 1 and >= regions (k = { k } )\" ) GridArchive . __init__ ( self , dimensions = ( 1 ,) * len ( ranges ), ranges = ranges , dtype = dtype ) self . _dimensions = len ( ranges ) ranges = list ( zip ( * ranges )) self . _lower_bounds = np . array ( ranges [ 0 ], dtype = self . _dtype ) self . _upper_bounds = np . array ( ranges [ 1 ], dtype = self . _dtype ) self . _interval = self . _upper_bounds - self . _lower_bounds self . _k = k self . _n_samples = n_samples self . _samples = None self . _centroids = None self . initialize_rng ( seed = seed ) self . _kmeans = KMeans ( n_clusters = self . _k , n_init = 1 , random_state = self . _seed ) # Loading samples if given if samples is not None : if isinstance ( samples , str ): try : self . _samples = np . load ( samples ) self . _n_samples = len ( self . _samples ) except Exception as _ : raise ValueError ( f \"Error in CVTArchive.__init__() loading the samples file { samples } .\" ) elif isinstance ( samples , np . ndarray ) and len ( samples ) != n_samples : raise ValueError ( f \"The number of samples { len ( samples ) } must be equal to the number of expected samples (n_samples = { n_samples } )\" ) else : self . _samples = np . asarray ( samples ) if centroids is not None : if isinstance ( centroids , str ): try : self . _centroids = np . load ( centroids ) self . _k = len ( self . _centroids ) except Exception as _ : raise ValueError ( f \"Error in CVTArchive.__init__() loading the centroids file { centroids } .\" ) elif isinstance ( centroids , np . ndarray ) and len ( centroids ) != k : raise ValueError ( f \"The number of centroids { len ( centroids ) } must be equal to the number of regions (k = { self . _k } )\" ) else : self . _centroids = np . asarray ( centroids ) else : # Generate centroids if self . _samples is None : # Generate uniform samples if not given rng = np . random . default_rng ( seed = self . _seed ) self . _samples = rng . uniform ( low = self . _lower_bounds , high = self . _upper_bounds , size = ( self . _n_samples , self . _dimensions ), ) self . _kmeans . fit ( self . _samples ) self . _centroids = self . _kmeans . cluster_centers_ self . _kdtree = KDTree ( self . _centroids , metric = \"euclidean\" ) @property def dimensions ( self ) -> int : \"\"\"Dimensions of the measure space used Returns: int: Dimensions of the measure space used \"\"\" return self . _dimensions @property def samples ( self ) -> np . ndarray : \"\"\"Returns the samples used to generate the centroids Returns: np.ndarray: Samples \"\"\" return self . _samples @property def centroids ( self ) -> np . ndarray : \"\"\"Returns k centroids calculated from the samples Returns: np.ndarray: K d-dimensional centroids \"\"\" return self . _centroids @property def regions ( self ) -> int : \"\"\"Number of regions (k) of centroids in the CVTArchive Returns: int: k \"\"\" return self . _k @property def bounds ( self ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Tuple with the lower and upper bounds of the measure space The first value is the lower bounds and the second value is the upper bounds. Each value is a list with the corresponding lower/upper bound of the ith dimension in the measure space \"\"\" return ( self . _lower_bounds , self . _upper_bounds ) @property def instances ( self ) -> list [ Instance ]: return list ( self . _storage . values ()) def __str__ ( self ): return f \"CVArchive(dim= { self . _dimensions } ,regions= { self . _k } ,centroids= { self . _centroids } )\" def __repr__ ( self ): return f \"CVArchive(dim= { self . _dimensions } ,regions= { self . _k } ,centroids= { self . _centroids } )\" def __iter__ ( self ): \"\"\"Iterates over the dictionary of instances Returns: Iterator: Yields position in the hypercube and instance located in such position \"\"\" return iter ( self . _storage . values ()) def lower_i ( self , i ) -> np . float64 : if i < 0 or i > len ( self . _lower_bounds ): msg = f \"index { i } is out of bounds. Valid values are [0- { len ( self . _lower_bounds ) } ]\" raise ValueError ( msg ) return self . _lower_bounds [ i ] def upper_i ( self , i ) -> np . float64 : if i < 0 or i > len ( self . _upper_bounds ): msg = f \"index { i } is out of bounds. Valid values are [0- { len ( self . _upper_bounds ) } ]\" raise ValueError ( msg ) return self . _upper_bounds [ i ] def index_of ( self , descriptors ) -> np . ndarray : \"\"\"Computes the indeces of a batch of descriptors. Args: descriptors (array-like): (batch_size, dimensions) array of descriptors for each instance Raises: ValueError: ``descriptors`` is not shape (batch_size, dimensions) Returns: np.ndarray: (batch_size, ) array of integer indices representing the flattened grid coordinates. \"\"\" descriptors = np . array ( descriptors ) if len ( descriptors ) == 0 : return np . empty ( 0 ) elif ( descriptors . ndim == 1 and descriptors . shape [ 0 ] != self . _dimensions or descriptors . ndim == 2 and descriptors . shape [ 1 ] != self . _dimensions ): raise ValueError ( f \"Expected descriptors to be an array with shape \" f \"(batch_size, dimensions) (i.e. shape \" f \"(batch_size, { self . _dimensions } )) but it had shape \" f \" { descriptors . shape } \" ) indices = self . _kdtree . query ( descriptors , return_distance = False ) indices = indices [:, 0 ] return indices . astype ( np . int32 ) def to_file ( self , file_pattern : str = \"CVTArchive\" ): \"\"\"Saves the centroids and the samples of the CVTArchive to .npy files Each attribute is saved in its own filename. Therefore, file_pattern is expected not to contain any extension Args: file_pattern (str, optional): Pattern of the expected filenames. Defaults to \"CVTArchive\". \"\"\" np . save ( f \" { file_pattern } _centroids.npy\" , self . _centroids ) np . save ( f \" { file_pattern } _samples.npy\" , self . _samples ) @classmethod def load_from_json ( cls , filename : str ): \"\"\"Creates a CVTArchive object from the content of a previously created JSON file Args: filename (str): Filename of the JSON file with the CVTArchive information Raises: ValueError: If there's any error while loading the file. (IOError) ValueError: If the JSON file does not contain all the expected keys Returns: Self: Returns a CVTArchive object \"\"\" expected_keys = { \"dimensions\" , \"n_samples\" , \"regions\" , \"lbs\" , \"ubs\" , \"centroids\" , \"samples\" , } try : with open ( filename , \"r\" ) as file : json_data = json . load ( file ) if expected_keys != json_data . keys (): raise ValueError ( f \"The JSON file does not contain all the minimum expected keys. Expected keys are { expected_keys } and got { json_data . keys () } \" ) _ranges = [ ( l_i , u_i ) for l_i , u_i in zip ( json_data [ \"lbs\" ], json_data [ \"ubs\" ]) ] new_archive = cls ( k = json_data [ \"regions\" ], ranges = _ranges , n_samples = json_data [ \"n_samples\" ], centroids = json_data [ \"centroids\" ], samples = json_data [ \"samples\" ], ) return new_archive except IOError as io : raise ValueError ( f \"Error opening file { filename } . Reason -> { io . strerror } \" ) def asdict ( self ) -> dict : return { \"dimensions\" : self . _dimensions , \"n_samples\" : self . _n_samples , \"regions\" : self . _k , \"lbs\" : self . _lower_bounds . tolist (), \"ubs\" : self . _upper_bounds . tolist (), \"centroids\" : self . _centroids . tolist (), \"samples\" : self . _samples . tolist (), \"instances\" : { i : instance . asdict () for i , instance in enumerate ( self . _storage . values ()) }, } def to_json ( self , filename : Optional [ str ] = None ) -> str : \"\"\"Returns the content of the CVTArchive in JSON format. Returns: str: String in JSON format with the content of the CVTArchive \"\"\" json_data = json . dumps ( self . asdict (), indent = 4 ) if filename is not None : filename = ( f \" { filename } .json\" if not filename . endswith ( \".json\" ) else filename ) with open ( filename , \"w\" ) as f : f . write ( json_data ) return json_data","title":"CVTArchive"},{"location":"reference/archives/_cvt_archive/#archives._cvt_archive.CVTArchive.bounds","text":"Tuple with the lower and upper bounds of the measure space The first value is the lower bounds and the second value is the upper bounds. Each value is a list with the corresponding lower/upper bound of the ith dimension in the measure space","title":"bounds"},{"location":"reference/archives/_cvt_archive/#archives._cvt_archive.CVTArchive.centroids","text":"Returns k centroids calculated from the samples Returns: ndarray \u2013 np.ndarray: K d-dimensional centroids","title":"centroids"},{"location":"reference/archives/_cvt_archive/#archives._cvt_archive.CVTArchive.dimensions","text":"Dimensions of the measure space used Returns: int ( int ) \u2013 Dimensions of the measure space used","title":"dimensions"},{"location":"reference/archives/_cvt_archive/#archives._cvt_archive.CVTArchive.regions","text":"Number of regions (k) of centroids in the CVTArchive Returns: int ( int ) \u2013 k","title":"regions"},{"location":"reference/archives/_cvt_archive/#archives._cvt_archive.CVTArchive.samples","text":"Returns the samples used to generate the centroids Returns: ndarray \u2013 np.ndarray: Samples","title":"samples"},{"location":"reference/archives/_cvt_archive/#archives._cvt_archive.CVTArchive.__init__","text":"Creates a CVTArchive object Parameters: k ( int ) \u2013 Number of centroids (regions) to create ranges ( Sequence [ Tuple [ float , float ]] ) \u2013 Ranges of the measure space. Upper and lower bound of each indicates the first dimension should have bounds \u2013 math: [-1,1] \u2013 math: [-2,2] (inclusive). The legnth of ranges indicates the number of dimensions of the measure space. n_samples ( int ) \u2013 Number of samples to generate before calculating the centroids. centroids ( Optional [ NDArray | str ] , default: None ) \u2013 Precalculated centroids for the archive. samples ( Optional [ NDArray | str ] , default: None ) \u2013 Precalculated samples for the archive. Raises: ValueError \u2013 If len(ranges) <= 0. ValueError \u2013 If the number of samples is less than zero or less than the number of regions (k). ValueError \u2013 If the number of regions is less than zero. ValueError \u2013 If the samples file cannot be loaded. ValueError \u2013 If given a samples np.ndarray the number of samples in the file is different from the number of expected samples (n_samples). ValueError \u2013 If the centroids file cannot be loaded. ValueError \u2013 If given a centroids np.ndarray the number of centroids in the file is different from the number of regions (k). Source code in digneapy/archives/_cvt_archive.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def __init__ ( self , k : int , ranges : Sequence [ Tuple [ float , float ]], n_samples : int , centroids : Optional [ npt . NDArray | str ] = None , samples : Optional [ npt . NDArray | str ] = None , dtype = np . float64 , seed : int = 42 , ): \"\"\"Creates a CVTArchive object Args: k (int): Number of centroids (regions) to create ranges (Sequence[Tuple[float, float]]): Ranges of the measure space. Upper and lower bound of each dimension of the measure space, e.g. ``[(-1, 1), (-2, 2)]`` indicates the first dimension should have bounds :math:`[-1,1]` (inclusive), and the second dimension should have bounds :math:`[-2,2]` (inclusive). The legnth of ``ranges`` indicates the number of dimensions of the measure space. n_samples (int): Number of samples to generate before calculating the centroids. centroids (Optional[npt.NDArray | str], optional): Precalculated centroids for the archive. The options are a np.ndarray with the values of ``k`` centroids or a .txt with the centroids to be loaded by Numpy. Defaults to None. samples (Optional[npt.NDArray | str], optional): Precalculated samples for the archive. The options are a np.ndarray with the values of ``n_samples`` samples or a .txt with the samples to be loaded by Numpy. Defaults to None. Raises: ValueError: If len(ranges) <= 0. ValueError: If the number of samples is less than zero or less than the number of regions (k). ValueError: If the number of regions is less than zero. ValueError: If the samples file cannot be loaded. ValueError: If given a samples np.ndarray the number of samples in the file is different from the number of expected samples (n_samples). ValueError: If the centroids file cannot be loaded. ValueError: If given a centroids np.ndarray the number of centroids in the file is different from the number of regions (k). \"\"\" if k <= 0 : raise ValueError ( f \"The number of regions (k = { k } ) must be >= 1\" ) if len ( ranges ) <= 0 : raise ValueError ( f \"ranges must have length >= 1 and it has length { len ( ranges ) } \" ) if n_samples <= 0 or n_samples < k : raise ValueError ( f \"The number of samples (n_samples = { n_samples } ) must be >= 1 and >= regions (k = { k } )\" ) GridArchive . __init__ ( self , dimensions = ( 1 ,) * len ( ranges ), ranges = ranges , dtype = dtype ) self . _dimensions = len ( ranges ) ranges = list ( zip ( * ranges )) self . _lower_bounds = np . array ( ranges [ 0 ], dtype = self . _dtype ) self . _upper_bounds = np . array ( ranges [ 1 ], dtype = self . _dtype ) self . _interval = self . _upper_bounds - self . _lower_bounds self . _k = k self . _n_samples = n_samples self . _samples = None self . _centroids = None self . initialize_rng ( seed = seed ) self . _kmeans = KMeans ( n_clusters = self . _k , n_init = 1 , random_state = self . _seed ) # Loading samples if given if samples is not None : if isinstance ( samples , str ): try : self . _samples = np . load ( samples ) self . _n_samples = len ( self . _samples ) except Exception as _ : raise ValueError ( f \"Error in CVTArchive.__init__() loading the samples file { samples } .\" ) elif isinstance ( samples , np . ndarray ) and len ( samples ) != n_samples : raise ValueError ( f \"The number of samples { len ( samples ) } must be equal to the number of expected samples (n_samples = { n_samples } )\" ) else : self . _samples = np . asarray ( samples ) if centroids is not None : if isinstance ( centroids , str ): try : self . _centroids = np . load ( centroids ) self . _k = len ( self . _centroids ) except Exception as _ : raise ValueError ( f \"Error in CVTArchive.__init__() loading the centroids file { centroids } .\" ) elif isinstance ( centroids , np . ndarray ) and len ( centroids ) != k : raise ValueError ( f \"The number of centroids { len ( centroids ) } must be equal to the number of regions (k = { self . _k } )\" ) else : self . _centroids = np . asarray ( centroids ) else : # Generate centroids if self . _samples is None : # Generate uniform samples if not given rng = np . random . default_rng ( seed = self . _seed ) self . _samples = rng . uniform ( low = self . _lower_bounds , high = self . _upper_bounds , size = ( self . _n_samples , self . _dimensions ), ) self . _kmeans . fit ( self . _samples ) self . _centroids = self . _kmeans . cluster_centers_ self . _kdtree = KDTree ( self . _centroids , metric = \"euclidean\" )","title":"__init__"},{"location":"reference/archives/_cvt_archive/#archives._cvt_archive.CVTArchive.__iter__","text":"Iterates over the dictionary of instances Returns: Iterator \u2013 Yields position in the hypercube and instance located in such position Source code in digneapy/archives/_cvt_archive.py 201 202 203 204 205 206 207 def __iter__ ( self ): \"\"\"Iterates over the dictionary of instances Returns: Iterator: Yields position in the hypercube and instance located in such position \"\"\" return iter ( self . _storage . values ())","title":"__iter__"},{"location":"reference/archives/_cvt_archive/#archives._cvt_archive.CVTArchive.index_of","text":"Computes the indeces of a batch of descriptors. Parameters: descriptors ( array - like ) \u2013 (batch_size, dimensions) array of descriptors for each instance Raises: ValueError \u2013 descriptors is not shape (batch_size, dimensions) Returns: ndarray \u2013 np.ndarray: (batch_size, ) array of integer indices representing the flattened grid coordinates. Source code in digneapy/archives/_cvt_archive.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 def index_of ( self , descriptors ) -> np . ndarray : \"\"\"Computes the indeces of a batch of descriptors. Args: descriptors (array-like): (batch_size, dimensions) array of descriptors for each instance Raises: ValueError: ``descriptors`` is not shape (batch_size, dimensions) Returns: np.ndarray: (batch_size, ) array of integer indices representing the flattened grid coordinates. \"\"\" descriptors = np . array ( descriptors ) if len ( descriptors ) == 0 : return np . empty ( 0 ) elif ( descriptors . ndim == 1 and descriptors . shape [ 0 ] != self . _dimensions or descriptors . ndim == 2 and descriptors . shape [ 1 ] != self . _dimensions ): raise ValueError ( f \"Expected descriptors to be an array with shape \" f \"(batch_size, dimensions) (i.e. shape \" f \"(batch_size, { self . _dimensions } )) but it had shape \" f \" { descriptors . shape } \" ) indices = self . _kdtree . query ( descriptors , return_distance = False ) indices = indices [:, 0 ] return indices . astype ( np . int32 )","title":"index_of"},{"location":"reference/archives/_cvt_archive/#archives._cvt_archive.CVTArchive.load_from_json","text":"Creates a CVTArchive object from the content of a previously created JSON file Parameters: filename ( str ) \u2013 Filename of the JSON file with the CVTArchive information Raises: ValueError \u2013 If there's any error while loading the file. (IOError) ValueError \u2013 If the JSON file does not contain all the expected keys Returns: Self \u2013 Returns a CVTArchive object Source code in digneapy/archives/_cvt_archive.py 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 @classmethod def load_from_json ( cls , filename : str ): \"\"\"Creates a CVTArchive object from the content of a previously created JSON file Args: filename (str): Filename of the JSON file with the CVTArchive information Raises: ValueError: If there's any error while loading the file. (IOError) ValueError: If the JSON file does not contain all the expected keys Returns: Self: Returns a CVTArchive object \"\"\" expected_keys = { \"dimensions\" , \"n_samples\" , \"regions\" , \"lbs\" , \"ubs\" , \"centroids\" , \"samples\" , } try : with open ( filename , \"r\" ) as file : json_data = json . load ( file ) if expected_keys != json_data . keys (): raise ValueError ( f \"The JSON file does not contain all the minimum expected keys. Expected keys are { expected_keys } and got { json_data . keys () } \" ) _ranges = [ ( l_i , u_i ) for l_i , u_i in zip ( json_data [ \"lbs\" ], json_data [ \"ubs\" ]) ] new_archive = cls ( k = json_data [ \"regions\" ], ranges = _ranges , n_samples = json_data [ \"n_samples\" ], centroids = json_data [ \"centroids\" ], samples = json_data [ \"samples\" ], ) return new_archive except IOError as io : raise ValueError ( f \"Error opening file { filename } . Reason -> { io . strerror } \" )","title":"load_from_json"},{"location":"reference/archives/_cvt_archive/#archives._cvt_archive.CVTArchive.to_file","text":"Saves the centroids and the samples of the CVTArchive to .npy files Each attribute is saved in its own filename. Therefore, file_pattern is expected not to contain any extension Parameters: file_pattern ( str , default: 'CVTArchive' ) \u2013 Pattern of the expected filenames. Defaults to \"CVTArchive\". Source code in digneapy/archives/_cvt_archive.py 254 255 256 257 258 259 260 261 262 263 def to_file ( self , file_pattern : str = \"CVTArchive\" ): \"\"\"Saves the centroids and the samples of the CVTArchive to .npy files Each attribute is saved in its own filename. Therefore, file_pattern is expected not to contain any extension Args: file_pattern (str, optional): Pattern of the expected filenames. Defaults to \"CVTArchive\". \"\"\" np . save ( f \" { file_pattern } _centroids.npy\" , self . _centroids ) np . save ( f \" { file_pattern } _samples.npy\" , self . _samples )","title":"to_file"},{"location":"reference/archives/_cvt_archive/#archives._cvt_archive.CVTArchive.to_json","text":"Returns the content of the CVTArchive in JSON format. Returns: str ( str ) \u2013 String in JSON format with the content of the CVTArchive Source code in digneapy/archives/_cvt_archive.py 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 def to_json ( self , filename : Optional [ str ] = None ) -> str : \"\"\"Returns the content of the CVTArchive in JSON format. Returns: str: String in JSON format with the content of the CVTArchive \"\"\" json_data = json . dumps ( self . asdict (), indent = 4 ) if filename is not None : filename = ( f \" { filename } .json\" if not filename . endswith ( \".json\" ) else filename ) with open ( filename , \"w\" ) as f : f . write ( json_data ) return json_data","title":"to_json"},{"location":"reference/archives/_grid_archive/","text":"@File : _grid_archive.py @Time : 2024/06/07 12:18:10 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2024, Alejandro Marrero @Desc : None GridArchive Bases: Archive An archive that divides each dimension into a uniformly-sized cells. The source code of this class is inspired by the GridArchive class of pyribs https://github.com/icaros-usc/pyribs/blob/master/ribs/archives/_grid_archive.py This archive is the container described in Mouret 2015 <https://arxiv.org/pdf/1504.04909.pdf> _. It can be visualized as an n-dimensional grid in the measure space that is divided into a certain number of cells in each dimension. Each cell contains an elite, i.e. a solution that maximizes the objective function for the measures in that cell. Source code in digneapy/archives/_grid_archive.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 class GridArchive ( Archive ): \"\"\"An archive that divides each dimension into a uniformly-sized cells. The source code of this class is inspired by the GridArchive class of pyribs <https://github.com/icaros-usc/pyribs/blob/master/ribs/archives/_grid_archive.py> This archive is the container described in `Mouret 2015 <https://arxiv.org/pdf/1504.04909.pdf>`_. It can be visualized as an n-dimensional grid in the measure space that is divided into a certain number of cells in each dimension. Each cell contains an elite, i.e. a solution that `maximizes` the objective function for the measures in that cell. \"\"\" def __init__ ( self , dimensions : Sequence [ int ], ranges : Sequence [ Tuple [ float , float ]], instances : Optional [ Iterable [ Instance ]] = None , eps : float = 1e-6 , dtype = np . float64 , ): \"\"\"Creates a GridArchive instance Args: dimensions (Sequence[int]): (array-like of int): Number of cells in each dimension of the measure space, e.g. ``[20, 30, 40]`` indicates there should be 3 dimensions with 20, 30, and 40 cells. (The number of dimensions is implicitly defined in the length of this argument). ranges (Sequence[Tuple[float]]): (array-like of (float, float)): Upper and lower bound of each dimension of the measure space, e.g. ``[(-1, 1), (-2, 2)]`` indicates the first dimension should have bounds :math:`[-1,1]` (inclusive), and the second dimension should have bounds :math:`[-2,2]` (inclusive). ``ranges`` should be the same length as ``dims``. instances (Optional[Iterable[Instance]], optional): Instances to pre-initialise the archive. Defaults to None. eps (float, optional): Due to floating point precision errors, we add a small epsilon when computing the archive indices in the :meth:`index_of` method -- refer to the implementation `here. Defaults to 1e-6. dtype(str or data-type): Data type of the solutions, objectives, and measures. Raises: ValueError: ``dimensions`` and ``ranges`` are not the same length \"\"\" Archive . __init__ ( self , threshold = np . finfo ( np . float32 ) . max , dtype = dtype ) if len ( ranges ) == 0 or len ( dimensions ) == 0 : raise ValueError ( \"dimensions and ranges must have length >= 1\" ) if len ( ranges ) != len ( dimensions ): raise ValueError ( f \"len(dimensions) = { len ( dimensions ) } != len(ranges) = { len ( ranges ) } in GridArchive.__init__()\" ) self . _dimensions = np . asarray ( dimensions ) ranges = list ( zip ( * ranges )) self . _lower_bounds = np . array ( ranges [ 0 ], dtype = dtype ) self . _upper_bounds = np . array ( ranges [ 1 ], dtype = dtype ) self . _interval = self . _upper_bounds - self . _lower_bounds self . _eps = eps self . _cells = np . prod ( self . _dimensions , dtype = object ) self . _grid : Dict [ int , np . ndarray ] = {} self . _storage : Dict [ int , Instance ] = {} _bounds = [] for dimension , l_b , u_b in zip ( self . _dimensions , self . _lower_bounds , self . _upper_bounds ): _bounds . append ( np . linspace ( l_b , u_b , dimension )) self . _boundaries = np . asarray ( _bounds ) if instances is not None : self . extend ( instances ) @property def dimensions ( self ): return self . _dimensions @property def bounds ( self ): \"\"\"list of numpy.ndarray: The boundaries of the cells in each dimension. Entry ``i`` in this list is an array that contains the boundaries of the cells in dimension ``i``. The array contains ``self.dims[i] + 1`` entries laid out like this:: Archive cells: | 0 | 1 | ... | self.dims[i] | boundaries[i]: 0 1 2 self.dims[i] - 1 self.dims[i] Thus, ``boundaries[i][j]`` and ``boundaries[i][j + 1]`` are the lower and upper bounds of cell ``j`` in dimension ``i``. To access the lower bounds of all the cells in dimension ``i``, use ``boundaries[i][:-1]``, and to access all the upper bounds, use ``boundaries[i][1:]``. \"\"\" return self . _boundaries @property def n_cells ( self ): return self . _cells @property def coverage ( self ): \"\"\"Get the coverage of the hypercube space. The coverage is calculated has the number of cells filled over the total space available. Returns: float: Filled cells over the total available. \"\"\" if len ( self . _grid ) == 0 : return 0.0 return len ( self . _grid ) / self . _cells @property def filled_cells ( self ): return self . _grid . keys () @property def instances ( self ) -> Sequence [ Instance ]: return list ( self . _storage . values ()) def __str__ ( self ): return f \"GridArchive(dim= { self . _dimensions } ,cells= { self . _cells } ,bounds= { self . _boundaries } )\" def __repr__ ( self ): return f \"GridArchive(dim= { self . _dimensions } ,cells= { self . _cells } ,bounds= { self . _boundaries } )\" def __len__ ( self ): return len ( self . _grid ) def __getitem__ ( self , key ): \"\"\"Returns a dictionary with the descriptors as the keys. The values are the instances found. Note that some of the given keys may not be in the archive. Args: key (array-like or descriptor): Descriptors of the instances that want to retrieve. Valid examples are: - archive[[0,11], [0,5]] --> Get the instances with the descriptors (0,11) and (0, 5) - archive[0,11] --> Get the instances at indices 0 and 11 Raises: TypeError: If the key is an slice. Not allowed. ValueError: If the shape of the keys are not valid. Returns: dict: Returns a dict with the found instances. \"\"\" if isinstance ( key , slice ): raise TypeError ( \"Slicing is not available in GridArchive. Use 1D index or descriptor-type indices\" ) descriptors = np . asarray ( key ) if descriptors . ndim == 1 : indices = descriptors elif descriptors . ndim == 2 and descriptors . shape [ 1 ] == len ( self . _dimensions ): indices = self . index_of ( descriptors ) . tolist () else : raise ValueError ( f \"Expected descriptors to be an array with shape \" f \"(batch_size, dimensions) (i.e. shape \" f \"(batch_size, { len ( self . _dimensions ) } )) but it had shape \" f \" { descriptors . shape } \" ) if isinstance ( indices , int ): indices = [ indices ] instances = [ self . _storage [ idx ] for idx in indices ] return instances def __iter__ ( self ): \"\"\"Iterates over the dictionary of instances Returns: Iterator: Yields position in the hypercube and instance located in such position \"\"\" return iter ( self . _storage . values ()) def lower_i ( self , i ): if i < 0 or i > len ( self . _lower_bounds ): msg = f \"index { i } is out of bounds. Valid values are [0- { len ( self . _boundaries ) } ]\" raise ValueError ( msg ) return self . _lower_bounds [ i ] def upper_i ( self , i ): if i < 0 or i > len ( self . _upper_bounds ): msg = f \"index { i } is out of bounds. Valid values are [0- { len ( self . _boundaries ) } ]\" raise ValueError ( msg ) return self . _upper_bounds [ i ] def append ( self , instance : Instance , descriptor : Optional [ np . ndarray ] = None ): \"\"\"Inserts an Instance into the Grid Args: instance (Instance): Instace to be inserted Raises: TypeError: ``instance`` is not a instance of the class Instance. \"\"\" if not isinstance ( instance , Instance ): msg = \"Only objects of type Instance can be inserted into a GridArchive\" raise TypeError ( msg ) descriptor = ( np . asarray ( instance . descriptor ) if descriptor is None else descriptor ) index = self . index_of ([ descriptor ])[ 0 ] if index not in self . _grid or instance > self . _grid [ index ]: self . _grid [ index ] = descriptor self . _storage [ index ] = instance . clone () def extend ( self , instances : Sequence [ Instance ], descriptors : Optional [ np . ndarray ] = None , * args , ** kwargs , ): \"\"\"Includes all the instances in iterable into the Grid Args: iterable (Iterable[Instance]): Iterable of instances \"\"\" if not all ( isinstance ( i , Instance ) for i in instances ): msg = \"Only objects of type Instance can be inserted into a GridArchive\" raise TypeError ( msg ) if descriptors is None : try : descriptors = np . asarray ([ i . descriptor for i in instances ]) except AttributeError as e : print ( \"Instances do not have a descriptor yet and the value descriptor is None\" ) raise ( e ) indices = self . index_of ( descriptors ) for idx , instance , descriptor in zip ( indices , instances , descriptors , strict = True ): if idx not in self . _grid or instance . fitness > self . _storage [ idx ] . fitness : self . _storage [ idx ] = instance . clone () self . _grid [ idx ] = descriptor def remove ( self , descriptors : np . ndarray ): \"\"\"Removes all the instances with the matching descriptors in iterable from the grid\"\"\" indices_to_remove = self . index_of ( descriptors ) for index in indices_to_remove : if index in self . _grid : del self . _grid [ index ] del self . _storage [ index ] def purge_unfeasible ( self , attr : str = \"p\" ): \"\"\"Removes all the unfeasible instances from the grid\"\"\" keys_to_remove = [ i for i in self . _storage . keys () if getattr ( self . _storage [ i ], attr ) < 0 ] for i in keys_to_remove : del self . _grid [ i ] del self . _storage [ i ] def index_of ( self , descriptors ): \"\"\"Computes the indices of a batch of descriptors. Args: descriptors (array-like): (batch_size, dimensions) array of descriptors for each instance Raises: ValueError: ``descriptors`` is not shape (batch_size, dimensions) Returns: np.ndarray: (batch_size, ) array of integer indices representing the flattened grid coordinates. \"\"\" if len ( descriptors ) == 0 : return np . empty ( 0 ) descriptors = np . asarray ( descriptors ) if ( descriptors . ndim == 1 and descriptors . shape [ 0 ] != len ( self . _dimensions ) or descriptors . ndim == 2 and descriptors . shape [ 1 ] != len ( self . _dimensions ) ): raise ValueError ( f \"Expected descriptors to be an array with shape \" f \"(batch_size, dimensions) (i.e. shape \" f \"(batch_size, { len ( self . _dimensions ) } )) but it had shape \" f \" { descriptors . shape } \" ) grid_indices = ( ( self . _dimensions * ( descriptors - self . _lower_bounds ) + self . _eps ) / self . _interval ) . astype ( int ) # Clip the indexes to make sure they are in the expected range for each dimension clipped = np . clip ( grid_indices , 0 , self . _dimensions - 1 ) return self . _grid_to_int_index ( clipped ) def _grid_to_int_index ( self , grid_indices ) -> np . ndarray : grid_indices = np . asarray ( grid_indices ) if len ( self . _dimensions ) > 64 : strides = np . cumprod (( 1 ,) + tuple ( self . _dimensions [:: - 1 ][: - 1 ]))[:: - 1 ] # Reshape strides to (1, num_dimensions) to make it broadcastable with grid_indices strides = strides . reshape ( 1 , - 1 ) flattened_indices = np . sum ( grid_indices * strides , axis = 1 , dtype = object ) else : flattened_indices = np . ravel_multi_index ( grid_indices . T , self . _dimensions ) . astype ( int ) return flattened_indices def int_to_grid_index ( self , int_indices ) -> np . ndarray : int_indices = np . asarray ( int_indices ) if len ( self . _dimensions ) > 64 : # Manually unravel the index for dimensions > 64 unravel_indices = [] remaining_indices = int_indices . astype ( object ) for dim_size in self . _dimensions [:: - 1 ]: unravel_indices . append ( remaining_indices % dim_size ) remaining_indices //= dim_size unravel_indices = np . array ( unravel_indices [:: - 1 ]) . T else : unravel_indices = np . asarray ( np . unravel_index ( int_indices , self . _dimensions , ) ) . T . astype ( int ) return unravel_indices def asdict ( self ) -> dict : return { \"dimensions\" : self . _dimensions . tolist (), \"lbs\" : self . _lower_bounds . tolist (), \"ubs\" : self . _upper_bounds . tolist (), \"n_cells\" : self . _cells , \"instances\" : { i : instance . asdict () for i , instance in enumerate ( self . _storage . values ()) }, } def to_json ( self ) -> str : return json . dumps ( self . asdict (), indent = 4 ) bounds property list of numpy.ndarray: The boundaries of the cells in each dimension. Entry i in this list is an array that contains the boundaries of the cells in dimension i . The array contains self.dims[i] + 1 entries laid out like this:: Archive cells: | 0 | 1 | ... | self.dims[i] | boundaries[i]: 0 1 2 self.dims[i] - 1 self.dims[i] Thus, boundaries[i][j] and boundaries[i][j + 1] are the lower and upper bounds of cell j in dimension i . To access the lower bounds of all the cells in dimension i , use boundaries[i][:-1] , and to access all the upper bounds, use boundaries[i][1:] . coverage property Get the coverage of the hypercube space. The coverage is calculated has the number of cells filled over the total space available. Returns: float \u2013 Filled cells over the total available. __getitem__ ( key ) Returns a dictionary with the descriptors as the keys. The values are the instances found. Note that some of the given keys may not be in the archive. Parameters: key ( array - like or descriptor ) \u2013 Descriptors of the instances that want to retrieve. Valid examples are \u2013 Raises: TypeError \u2013 If the key is an slice. Not allowed. ValueError \u2013 If the shape of the keys are not valid. Returns: dict \u2013 Returns a dict with the found instances. Source code in digneapy/archives/_grid_archive.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def __getitem__ ( self , key ): \"\"\"Returns a dictionary with the descriptors as the keys. The values are the instances found. Note that some of the given keys may not be in the archive. Args: key (array-like or descriptor): Descriptors of the instances that want to retrieve. Valid examples are: - archive[[0,11], [0,5]] --> Get the instances with the descriptors (0,11) and (0, 5) - archive[0,11] --> Get the instances at indices 0 and 11 Raises: TypeError: If the key is an slice. Not allowed. ValueError: If the shape of the keys are not valid. Returns: dict: Returns a dict with the found instances. \"\"\" if isinstance ( key , slice ): raise TypeError ( \"Slicing is not available in GridArchive. Use 1D index or descriptor-type indices\" ) descriptors = np . asarray ( key ) if descriptors . ndim == 1 : indices = descriptors elif descriptors . ndim == 2 and descriptors . shape [ 1 ] == len ( self . _dimensions ): indices = self . index_of ( descriptors ) . tolist () else : raise ValueError ( f \"Expected descriptors to be an array with shape \" f \"(batch_size, dimensions) (i.e. shape \" f \"(batch_size, { len ( self . _dimensions ) } )) but it had shape \" f \" { descriptors . shape } \" ) if isinstance ( indices , int ): indices = [ indices ] instances = [ self . _storage [ idx ] for idx in indices ] return instances __init__ ( dimensions , ranges , instances = None , eps = 1e-06 , dtype = np . float64 ) Creates a GridArchive instance Parameters: dimensions ( Sequence [ int ] ) \u2013 (array-like of int): Number of cells in each dimension of the ranges ( Sequence [ Tuple [ float ]] ) \u2013 (array-like of (float, float)): Upper and lower bound of each indicates the first dimension should have bounds \u2013 math: [-1,1] \u2013 math: [-2,2] (inclusive). ranges should be the same length as instances ( Optional [ Iterable [ Instance ]] , default: None ) \u2013 Instances to pre-initialise the archive. Defaults to None. eps ( float , default: 1e-06 ) \u2013 Due to floating point precision errors, we add a small epsilon when computing the archive indices in the \u2013 meth: index_of dtype ( str or data - type , default: float64 ) \u2013 Data type of the solutions, objectives, Raises: ValueError \u2013 dimensions and ranges are not the same length Source code in digneapy/archives/_grid_archive.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def __init__ ( self , dimensions : Sequence [ int ], ranges : Sequence [ Tuple [ float , float ]], instances : Optional [ Iterable [ Instance ]] = None , eps : float = 1e-6 , dtype = np . float64 , ): \"\"\"Creates a GridArchive instance Args: dimensions (Sequence[int]): (array-like of int): Number of cells in each dimension of the measure space, e.g. ``[20, 30, 40]`` indicates there should be 3 dimensions with 20, 30, and 40 cells. (The number of dimensions is implicitly defined in the length of this argument). ranges (Sequence[Tuple[float]]): (array-like of (float, float)): Upper and lower bound of each dimension of the measure space, e.g. ``[(-1, 1), (-2, 2)]`` indicates the first dimension should have bounds :math:`[-1,1]` (inclusive), and the second dimension should have bounds :math:`[-2,2]` (inclusive). ``ranges`` should be the same length as ``dims``. instances (Optional[Iterable[Instance]], optional): Instances to pre-initialise the archive. Defaults to None. eps (float, optional): Due to floating point precision errors, we add a small epsilon when computing the archive indices in the :meth:`index_of` method -- refer to the implementation `here. Defaults to 1e-6. dtype(str or data-type): Data type of the solutions, objectives, and measures. Raises: ValueError: ``dimensions`` and ``ranges`` are not the same length \"\"\" Archive . __init__ ( self , threshold = np . finfo ( np . float32 ) . max , dtype = dtype ) if len ( ranges ) == 0 or len ( dimensions ) == 0 : raise ValueError ( \"dimensions and ranges must have length >= 1\" ) if len ( ranges ) != len ( dimensions ): raise ValueError ( f \"len(dimensions) = { len ( dimensions ) } != len(ranges) = { len ( ranges ) } in GridArchive.__init__()\" ) self . _dimensions = np . asarray ( dimensions ) ranges = list ( zip ( * ranges )) self . _lower_bounds = np . array ( ranges [ 0 ], dtype = dtype ) self . _upper_bounds = np . array ( ranges [ 1 ], dtype = dtype ) self . _interval = self . _upper_bounds - self . _lower_bounds self . _eps = eps self . _cells = np . prod ( self . _dimensions , dtype = object ) self . _grid : Dict [ int , np . ndarray ] = {} self . _storage : Dict [ int , Instance ] = {} _bounds = [] for dimension , l_b , u_b in zip ( self . _dimensions , self . _lower_bounds , self . _upper_bounds ): _bounds . append ( np . linspace ( l_b , u_b , dimension )) self . _boundaries = np . asarray ( _bounds ) if instances is not None : self . extend ( instances ) __iter__ () Iterates over the dictionary of instances Returns: Iterator \u2013 Yields position in the hypercube and instance located in such position Source code in digneapy/archives/_grid_archive.py 191 192 193 194 195 196 197 def __iter__ ( self ): \"\"\"Iterates over the dictionary of instances Returns: Iterator: Yields position in the hypercube and instance located in such position \"\"\" return iter ( self . _storage . values ()) append ( instance , descriptor = None ) Inserts an Instance into the Grid Parameters: instance ( Instance ) \u2013 Instace to be inserted Raises: TypeError \u2013 instance is not a instance of the class Instance. Source code in digneapy/archives/_grid_archive.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def append ( self , instance : Instance , descriptor : Optional [ np . ndarray ] = None ): \"\"\"Inserts an Instance into the Grid Args: instance (Instance): Instace to be inserted Raises: TypeError: ``instance`` is not a instance of the class Instance. \"\"\" if not isinstance ( instance , Instance ): msg = \"Only objects of type Instance can be inserted into a GridArchive\" raise TypeError ( msg ) descriptor = ( np . asarray ( instance . descriptor ) if descriptor is None else descriptor ) index = self . index_of ([ descriptor ])[ 0 ] if index not in self . _grid or instance > self . _grid [ index ]: self . _grid [ index ] = descriptor self . _storage [ index ] = instance . clone () extend ( instances , descriptors = None , * args , ** kwargs ) Includes all the instances in iterable into the Grid Parameters: iterable ( Iterable [ Instance ] ) \u2013 Iterable of instances Source code in digneapy/archives/_grid_archive.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 def extend ( self , instances : Sequence [ Instance ], descriptors : Optional [ np . ndarray ] = None , * args , ** kwargs , ): \"\"\"Includes all the instances in iterable into the Grid Args: iterable (Iterable[Instance]): Iterable of instances \"\"\" if not all ( isinstance ( i , Instance ) for i in instances ): msg = \"Only objects of type Instance can be inserted into a GridArchive\" raise TypeError ( msg ) if descriptors is None : try : descriptors = np . asarray ([ i . descriptor for i in instances ]) except AttributeError as e : print ( \"Instances do not have a descriptor yet and the value descriptor is None\" ) raise ( e ) indices = self . index_of ( descriptors ) for idx , instance , descriptor in zip ( indices , instances , descriptors , strict = True ): if idx not in self . _grid or instance . fitness > self . _storage [ idx ] . fitness : self . _storage [ idx ] = instance . clone () self . _grid [ idx ] = descriptor index_of ( descriptors ) Computes the indices of a batch of descriptors. Parameters: descriptors ( array - like ) \u2013 (batch_size, dimensions) array of descriptors for each instance Raises: ValueError \u2013 descriptors is not shape (batch_size, dimensions) Returns: \u2013 np.ndarray: (batch_size, ) array of integer indices representing the flattened grid coordinates. Source code in digneapy/archives/_grid_archive.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def index_of ( self , descriptors ): \"\"\"Computes the indices of a batch of descriptors. Args: descriptors (array-like): (batch_size, dimensions) array of descriptors for each instance Raises: ValueError: ``descriptors`` is not shape (batch_size, dimensions) Returns: np.ndarray: (batch_size, ) array of integer indices representing the flattened grid coordinates. \"\"\" if len ( descriptors ) == 0 : return np . empty ( 0 ) descriptors = np . asarray ( descriptors ) if ( descriptors . ndim == 1 and descriptors . shape [ 0 ] != len ( self . _dimensions ) or descriptors . ndim == 2 and descriptors . shape [ 1 ] != len ( self . _dimensions ) ): raise ValueError ( f \"Expected descriptors to be an array with shape \" f \"(batch_size, dimensions) (i.e. shape \" f \"(batch_size, { len ( self . _dimensions ) } )) but it had shape \" f \" { descriptors . shape } \" ) grid_indices = ( ( self . _dimensions * ( descriptors - self . _lower_bounds ) + self . _eps ) / self . _interval ) . astype ( int ) # Clip the indexes to make sure they are in the expected range for each dimension clipped = np . clip ( grid_indices , 0 , self . _dimensions - 1 ) return self . _grid_to_int_index ( clipped ) purge_unfeasible ( attr = 'p' ) Removes all the unfeasible instances from the grid Source code in digneapy/archives/_grid_archive.py 272 273 274 275 276 277 278 279 def purge_unfeasible ( self , attr : str = \"p\" ): \"\"\"Removes all the unfeasible instances from the grid\"\"\" keys_to_remove = [ i for i in self . _storage . keys () if getattr ( self . _storage [ i ], attr ) < 0 ] for i in keys_to_remove : del self . _grid [ i ] del self . _storage [ i ] remove ( descriptors ) Removes all the instances with the matching descriptors in iterable from the grid Source code in digneapy/archives/_grid_archive.py 263 264 265 266 267 268 269 270 def remove ( self , descriptors : np . ndarray ): \"\"\"Removes all the instances with the matching descriptors in iterable from the grid\"\"\" indices_to_remove = self . index_of ( descriptors ) for index in indices_to_remove : if index in self . _grid : del self . _grid [ index ] del self . _storage [ index ]","title":" grid archive"},{"location":"reference/archives/_grid_archive/#archives._grid_archive.GridArchive","text":"Bases: Archive An archive that divides each dimension into a uniformly-sized cells. The source code of this class is inspired by the GridArchive class of pyribs https://github.com/icaros-usc/pyribs/blob/master/ribs/archives/_grid_archive.py This archive is the container described in Mouret 2015 <https://arxiv.org/pdf/1504.04909.pdf> _. It can be visualized as an n-dimensional grid in the measure space that is divided into a certain number of cells in each dimension. Each cell contains an elite, i.e. a solution that maximizes the objective function for the measures in that cell. Source code in digneapy/archives/_grid_archive.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 class GridArchive ( Archive ): \"\"\"An archive that divides each dimension into a uniformly-sized cells. The source code of this class is inspired by the GridArchive class of pyribs <https://github.com/icaros-usc/pyribs/blob/master/ribs/archives/_grid_archive.py> This archive is the container described in `Mouret 2015 <https://arxiv.org/pdf/1504.04909.pdf>`_. It can be visualized as an n-dimensional grid in the measure space that is divided into a certain number of cells in each dimension. Each cell contains an elite, i.e. a solution that `maximizes` the objective function for the measures in that cell. \"\"\" def __init__ ( self , dimensions : Sequence [ int ], ranges : Sequence [ Tuple [ float , float ]], instances : Optional [ Iterable [ Instance ]] = None , eps : float = 1e-6 , dtype = np . float64 , ): \"\"\"Creates a GridArchive instance Args: dimensions (Sequence[int]): (array-like of int): Number of cells in each dimension of the measure space, e.g. ``[20, 30, 40]`` indicates there should be 3 dimensions with 20, 30, and 40 cells. (The number of dimensions is implicitly defined in the length of this argument). ranges (Sequence[Tuple[float]]): (array-like of (float, float)): Upper and lower bound of each dimension of the measure space, e.g. ``[(-1, 1), (-2, 2)]`` indicates the first dimension should have bounds :math:`[-1,1]` (inclusive), and the second dimension should have bounds :math:`[-2,2]` (inclusive). ``ranges`` should be the same length as ``dims``. instances (Optional[Iterable[Instance]], optional): Instances to pre-initialise the archive. Defaults to None. eps (float, optional): Due to floating point precision errors, we add a small epsilon when computing the archive indices in the :meth:`index_of` method -- refer to the implementation `here. Defaults to 1e-6. dtype(str or data-type): Data type of the solutions, objectives, and measures. Raises: ValueError: ``dimensions`` and ``ranges`` are not the same length \"\"\" Archive . __init__ ( self , threshold = np . finfo ( np . float32 ) . max , dtype = dtype ) if len ( ranges ) == 0 or len ( dimensions ) == 0 : raise ValueError ( \"dimensions and ranges must have length >= 1\" ) if len ( ranges ) != len ( dimensions ): raise ValueError ( f \"len(dimensions) = { len ( dimensions ) } != len(ranges) = { len ( ranges ) } in GridArchive.__init__()\" ) self . _dimensions = np . asarray ( dimensions ) ranges = list ( zip ( * ranges )) self . _lower_bounds = np . array ( ranges [ 0 ], dtype = dtype ) self . _upper_bounds = np . array ( ranges [ 1 ], dtype = dtype ) self . _interval = self . _upper_bounds - self . _lower_bounds self . _eps = eps self . _cells = np . prod ( self . _dimensions , dtype = object ) self . _grid : Dict [ int , np . ndarray ] = {} self . _storage : Dict [ int , Instance ] = {} _bounds = [] for dimension , l_b , u_b in zip ( self . _dimensions , self . _lower_bounds , self . _upper_bounds ): _bounds . append ( np . linspace ( l_b , u_b , dimension )) self . _boundaries = np . asarray ( _bounds ) if instances is not None : self . extend ( instances ) @property def dimensions ( self ): return self . _dimensions @property def bounds ( self ): \"\"\"list of numpy.ndarray: The boundaries of the cells in each dimension. Entry ``i`` in this list is an array that contains the boundaries of the cells in dimension ``i``. The array contains ``self.dims[i] + 1`` entries laid out like this:: Archive cells: | 0 | 1 | ... | self.dims[i] | boundaries[i]: 0 1 2 self.dims[i] - 1 self.dims[i] Thus, ``boundaries[i][j]`` and ``boundaries[i][j + 1]`` are the lower and upper bounds of cell ``j`` in dimension ``i``. To access the lower bounds of all the cells in dimension ``i``, use ``boundaries[i][:-1]``, and to access all the upper bounds, use ``boundaries[i][1:]``. \"\"\" return self . _boundaries @property def n_cells ( self ): return self . _cells @property def coverage ( self ): \"\"\"Get the coverage of the hypercube space. The coverage is calculated has the number of cells filled over the total space available. Returns: float: Filled cells over the total available. \"\"\" if len ( self . _grid ) == 0 : return 0.0 return len ( self . _grid ) / self . _cells @property def filled_cells ( self ): return self . _grid . keys () @property def instances ( self ) -> Sequence [ Instance ]: return list ( self . _storage . values ()) def __str__ ( self ): return f \"GridArchive(dim= { self . _dimensions } ,cells= { self . _cells } ,bounds= { self . _boundaries } )\" def __repr__ ( self ): return f \"GridArchive(dim= { self . _dimensions } ,cells= { self . _cells } ,bounds= { self . _boundaries } )\" def __len__ ( self ): return len ( self . _grid ) def __getitem__ ( self , key ): \"\"\"Returns a dictionary with the descriptors as the keys. The values are the instances found. Note that some of the given keys may not be in the archive. Args: key (array-like or descriptor): Descriptors of the instances that want to retrieve. Valid examples are: - archive[[0,11], [0,5]] --> Get the instances with the descriptors (0,11) and (0, 5) - archive[0,11] --> Get the instances at indices 0 and 11 Raises: TypeError: If the key is an slice. Not allowed. ValueError: If the shape of the keys are not valid. Returns: dict: Returns a dict with the found instances. \"\"\" if isinstance ( key , slice ): raise TypeError ( \"Slicing is not available in GridArchive. Use 1D index or descriptor-type indices\" ) descriptors = np . asarray ( key ) if descriptors . ndim == 1 : indices = descriptors elif descriptors . ndim == 2 and descriptors . shape [ 1 ] == len ( self . _dimensions ): indices = self . index_of ( descriptors ) . tolist () else : raise ValueError ( f \"Expected descriptors to be an array with shape \" f \"(batch_size, dimensions) (i.e. shape \" f \"(batch_size, { len ( self . _dimensions ) } )) but it had shape \" f \" { descriptors . shape } \" ) if isinstance ( indices , int ): indices = [ indices ] instances = [ self . _storage [ idx ] for idx in indices ] return instances def __iter__ ( self ): \"\"\"Iterates over the dictionary of instances Returns: Iterator: Yields position in the hypercube and instance located in such position \"\"\" return iter ( self . _storage . values ()) def lower_i ( self , i ): if i < 0 or i > len ( self . _lower_bounds ): msg = f \"index { i } is out of bounds. Valid values are [0- { len ( self . _boundaries ) } ]\" raise ValueError ( msg ) return self . _lower_bounds [ i ] def upper_i ( self , i ): if i < 0 or i > len ( self . _upper_bounds ): msg = f \"index { i } is out of bounds. Valid values are [0- { len ( self . _boundaries ) } ]\" raise ValueError ( msg ) return self . _upper_bounds [ i ] def append ( self , instance : Instance , descriptor : Optional [ np . ndarray ] = None ): \"\"\"Inserts an Instance into the Grid Args: instance (Instance): Instace to be inserted Raises: TypeError: ``instance`` is not a instance of the class Instance. \"\"\" if not isinstance ( instance , Instance ): msg = \"Only objects of type Instance can be inserted into a GridArchive\" raise TypeError ( msg ) descriptor = ( np . asarray ( instance . descriptor ) if descriptor is None else descriptor ) index = self . index_of ([ descriptor ])[ 0 ] if index not in self . _grid or instance > self . _grid [ index ]: self . _grid [ index ] = descriptor self . _storage [ index ] = instance . clone () def extend ( self , instances : Sequence [ Instance ], descriptors : Optional [ np . ndarray ] = None , * args , ** kwargs , ): \"\"\"Includes all the instances in iterable into the Grid Args: iterable (Iterable[Instance]): Iterable of instances \"\"\" if not all ( isinstance ( i , Instance ) for i in instances ): msg = \"Only objects of type Instance can be inserted into a GridArchive\" raise TypeError ( msg ) if descriptors is None : try : descriptors = np . asarray ([ i . descriptor for i in instances ]) except AttributeError as e : print ( \"Instances do not have a descriptor yet and the value descriptor is None\" ) raise ( e ) indices = self . index_of ( descriptors ) for idx , instance , descriptor in zip ( indices , instances , descriptors , strict = True ): if idx not in self . _grid or instance . fitness > self . _storage [ idx ] . fitness : self . _storage [ idx ] = instance . clone () self . _grid [ idx ] = descriptor def remove ( self , descriptors : np . ndarray ): \"\"\"Removes all the instances with the matching descriptors in iterable from the grid\"\"\" indices_to_remove = self . index_of ( descriptors ) for index in indices_to_remove : if index in self . _grid : del self . _grid [ index ] del self . _storage [ index ] def purge_unfeasible ( self , attr : str = \"p\" ): \"\"\"Removes all the unfeasible instances from the grid\"\"\" keys_to_remove = [ i for i in self . _storage . keys () if getattr ( self . _storage [ i ], attr ) < 0 ] for i in keys_to_remove : del self . _grid [ i ] del self . _storage [ i ] def index_of ( self , descriptors ): \"\"\"Computes the indices of a batch of descriptors. Args: descriptors (array-like): (batch_size, dimensions) array of descriptors for each instance Raises: ValueError: ``descriptors`` is not shape (batch_size, dimensions) Returns: np.ndarray: (batch_size, ) array of integer indices representing the flattened grid coordinates. \"\"\" if len ( descriptors ) == 0 : return np . empty ( 0 ) descriptors = np . asarray ( descriptors ) if ( descriptors . ndim == 1 and descriptors . shape [ 0 ] != len ( self . _dimensions ) or descriptors . ndim == 2 and descriptors . shape [ 1 ] != len ( self . _dimensions ) ): raise ValueError ( f \"Expected descriptors to be an array with shape \" f \"(batch_size, dimensions) (i.e. shape \" f \"(batch_size, { len ( self . _dimensions ) } )) but it had shape \" f \" { descriptors . shape } \" ) grid_indices = ( ( self . _dimensions * ( descriptors - self . _lower_bounds ) + self . _eps ) / self . _interval ) . astype ( int ) # Clip the indexes to make sure they are in the expected range for each dimension clipped = np . clip ( grid_indices , 0 , self . _dimensions - 1 ) return self . _grid_to_int_index ( clipped ) def _grid_to_int_index ( self , grid_indices ) -> np . ndarray : grid_indices = np . asarray ( grid_indices ) if len ( self . _dimensions ) > 64 : strides = np . cumprod (( 1 ,) + tuple ( self . _dimensions [:: - 1 ][: - 1 ]))[:: - 1 ] # Reshape strides to (1, num_dimensions) to make it broadcastable with grid_indices strides = strides . reshape ( 1 , - 1 ) flattened_indices = np . sum ( grid_indices * strides , axis = 1 , dtype = object ) else : flattened_indices = np . ravel_multi_index ( grid_indices . T , self . _dimensions ) . astype ( int ) return flattened_indices def int_to_grid_index ( self , int_indices ) -> np . ndarray : int_indices = np . asarray ( int_indices ) if len ( self . _dimensions ) > 64 : # Manually unravel the index for dimensions > 64 unravel_indices = [] remaining_indices = int_indices . astype ( object ) for dim_size in self . _dimensions [:: - 1 ]: unravel_indices . append ( remaining_indices % dim_size ) remaining_indices //= dim_size unravel_indices = np . array ( unravel_indices [:: - 1 ]) . T else : unravel_indices = np . asarray ( np . unravel_index ( int_indices , self . _dimensions , ) ) . T . astype ( int ) return unravel_indices def asdict ( self ) -> dict : return { \"dimensions\" : self . _dimensions . tolist (), \"lbs\" : self . _lower_bounds . tolist (), \"ubs\" : self . _upper_bounds . tolist (), \"n_cells\" : self . _cells , \"instances\" : { i : instance . asdict () for i , instance in enumerate ( self . _storage . values ()) }, } def to_json ( self ) -> str : return json . dumps ( self . asdict (), indent = 4 )","title":"GridArchive"},{"location":"reference/archives/_grid_archive/#archives._grid_archive.GridArchive.bounds","text":"list of numpy.ndarray: The boundaries of the cells in each dimension. Entry i in this list is an array that contains the boundaries of the cells in dimension i . The array contains self.dims[i] + 1 entries laid out like this:: Archive cells: | 0 | 1 | ... | self.dims[i] | boundaries[i]: 0 1 2 self.dims[i] - 1 self.dims[i] Thus, boundaries[i][j] and boundaries[i][j + 1] are the lower and upper bounds of cell j in dimension i . To access the lower bounds of all the cells in dimension i , use boundaries[i][:-1] , and to access all the upper bounds, use boundaries[i][1:] .","title":"bounds"},{"location":"reference/archives/_grid_archive/#archives._grid_archive.GridArchive.coverage","text":"Get the coverage of the hypercube space. The coverage is calculated has the number of cells filled over the total space available. Returns: float \u2013 Filled cells over the total available.","title":"coverage"},{"location":"reference/archives/_grid_archive/#archives._grid_archive.GridArchive.__getitem__","text":"Returns a dictionary with the descriptors as the keys. The values are the instances found. Note that some of the given keys may not be in the archive. Parameters: key ( array - like or descriptor ) \u2013 Descriptors of the instances that want to retrieve. Valid examples are \u2013 Raises: TypeError \u2013 If the key is an slice. Not allowed. ValueError \u2013 If the shape of the keys are not valid. Returns: dict \u2013 Returns a dict with the found instances. Source code in digneapy/archives/_grid_archive.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def __getitem__ ( self , key ): \"\"\"Returns a dictionary with the descriptors as the keys. The values are the instances found. Note that some of the given keys may not be in the archive. Args: key (array-like or descriptor): Descriptors of the instances that want to retrieve. Valid examples are: - archive[[0,11], [0,5]] --> Get the instances with the descriptors (0,11) and (0, 5) - archive[0,11] --> Get the instances at indices 0 and 11 Raises: TypeError: If the key is an slice. Not allowed. ValueError: If the shape of the keys are not valid. Returns: dict: Returns a dict with the found instances. \"\"\" if isinstance ( key , slice ): raise TypeError ( \"Slicing is not available in GridArchive. Use 1D index or descriptor-type indices\" ) descriptors = np . asarray ( key ) if descriptors . ndim == 1 : indices = descriptors elif descriptors . ndim == 2 and descriptors . shape [ 1 ] == len ( self . _dimensions ): indices = self . index_of ( descriptors ) . tolist () else : raise ValueError ( f \"Expected descriptors to be an array with shape \" f \"(batch_size, dimensions) (i.e. shape \" f \"(batch_size, { len ( self . _dimensions ) } )) but it had shape \" f \" { descriptors . shape } \" ) if isinstance ( indices , int ): indices = [ indices ] instances = [ self . _storage [ idx ] for idx in indices ] return instances","title":"__getitem__"},{"location":"reference/archives/_grid_archive/#archives._grid_archive.GridArchive.__init__","text":"Creates a GridArchive instance Parameters: dimensions ( Sequence [ int ] ) \u2013 (array-like of int): Number of cells in each dimension of the ranges ( Sequence [ Tuple [ float ]] ) \u2013 (array-like of (float, float)): Upper and lower bound of each indicates the first dimension should have bounds \u2013 math: [-1,1] \u2013 math: [-2,2] (inclusive). ranges should be the same length as instances ( Optional [ Iterable [ Instance ]] , default: None ) \u2013 Instances to pre-initialise the archive. Defaults to None. eps ( float , default: 1e-06 ) \u2013 Due to floating point precision errors, we add a small epsilon when computing the archive indices in the \u2013 meth: index_of dtype ( str or data - type , default: float64 ) \u2013 Data type of the solutions, objectives, Raises: ValueError \u2013 dimensions and ranges are not the same length Source code in digneapy/archives/_grid_archive.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def __init__ ( self , dimensions : Sequence [ int ], ranges : Sequence [ Tuple [ float , float ]], instances : Optional [ Iterable [ Instance ]] = None , eps : float = 1e-6 , dtype = np . float64 , ): \"\"\"Creates a GridArchive instance Args: dimensions (Sequence[int]): (array-like of int): Number of cells in each dimension of the measure space, e.g. ``[20, 30, 40]`` indicates there should be 3 dimensions with 20, 30, and 40 cells. (The number of dimensions is implicitly defined in the length of this argument). ranges (Sequence[Tuple[float]]): (array-like of (float, float)): Upper and lower bound of each dimension of the measure space, e.g. ``[(-1, 1), (-2, 2)]`` indicates the first dimension should have bounds :math:`[-1,1]` (inclusive), and the second dimension should have bounds :math:`[-2,2]` (inclusive). ``ranges`` should be the same length as ``dims``. instances (Optional[Iterable[Instance]], optional): Instances to pre-initialise the archive. Defaults to None. eps (float, optional): Due to floating point precision errors, we add a small epsilon when computing the archive indices in the :meth:`index_of` method -- refer to the implementation `here. Defaults to 1e-6. dtype(str or data-type): Data type of the solutions, objectives, and measures. Raises: ValueError: ``dimensions`` and ``ranges`` are not the same length \"\"\" Archive . __init__ ( self , threshold = np . finfo ( np . float32 ) . max , dtype = dtype ) if len ( ranges ) == 0 or len ( dimensions ) == 0 : raise ValueError ( \"dimensions and ranges must have length >= 1\" ) if len ( ranges ) != len ( dimensions ): raise ValueError ( f \"len(dimensions) = { len ( dimensions ) } != len(ranges) = { len ( ranges ) } in GridArchive.__init__()\" ) self . _dimensions = np . asarray ( dimensions ) ranges = list ( zip ( * ranges )) self . _lower_bounds = np . array ( ranges [ 0 ], dtype = dtype ) self . _upper_bounds = np . array ( ranges [ 1 ], dtype = dtype ) self . _interval = self . _upper_bounds - self . _lower_bounds self . _eps = eps self . _cells = np . prod ( self . _dimensions , dtype = object ) self . _grid : Dict [ int , np . ndarray ] = {} self . _storage : Dict [ int , Instance ] = {} _bounds = [] for dimension , l_b , u_b in zip ( self . _dimensions , self . _lower_bounds , self . _upper_bounds ): _bounds . append ( np . linspace ( l_b , u_b , dimension )) self . _boundaries = np . asarray ( _bounds ) if instances is not None : self . extend ( instances )","title":"__init__"},{"location":"reference/archives/_grid_archive/#archives._grid_archive.GridArchive.__iter__","text":"Iterates over the dictionary of instances Returns: Iterator \u2013 Yields position in the hypercube and instance located in such position Source code in digneapy/archives/_grid_archive.py 191 192 193 194 195 196 197 def __iter__ ( self ): \"\"\"Iterates over the dictionary of instances Returns: Iterator: Yields position in the hypercube and instance located in such position \"\"\" return iter ( self . _storage . values ())","title":"__iter__"},{"location":"reference/archives/_grid_archive/#archives._grid_archive.GridArchive.append","text":"Inserts an Instance into the Grid Parameters: instance ( Instance ) \u2013 Instace to be inserted Raises: TypeError \u2013 instance is not a instance of the class Instance. Source code in digneapy/archives/_grid_archive.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def append ( self , instance : Instance , descriptor : Optional [ np . ndarray ] = None ): \"\"\"Inserts an Instance into the Grid Args: instance (Instance): Instace to be inserted Raises: TypeError: ``instance`` is not a instance of the class Instance. \"\"\" if not isinstance ( instance , Instance ): msg = \"Only objects of type Instance can be inserted into a GridArchive\" raise TypeError ( msg ) descriptor = ( np . asarray ( instance . descriptor ) if descriptor is None else descriptor ) index = self . index_of ([ descriptor ])[ 0 ] if index not in self . _grid or instance > self . _grid [ index ]: self . _grid [ index ] = descriptor self . _storage [ index ] = instance . clone ()","title":"append"},{"location":"reference/archives/_grid_archive/#archives._grid_archive.GridArchive.extend","text":"Includes all the instances in iterable into the Grid Parameters: iterable ( Iterable [ Instance ] ) \u2013 Iterable of instances Source code in digneapy/archives/_grid_archive.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 def extend ( self , instances : Sequence [ Instance ], descriptors : Optional [ np . ndarray ] = None , * args , ** kwargs , ): \"\"\"Includes all the instances in iterable into the Grid Args: iterable (Iterable[Instance]): Iterable of instances \"\"\" if not all ( isinstance ( i , Instance ) for i in instances ): msg = \"Only objects of type Instance can be inserted into a GridArchive\" raise TypeError ( msg ) if descriptors is None : try : descriptors = np . asarray ([ i . descriptor for i in instances ]) except AttributeError as e : print ( \"Instances do not have a descriptor yet and the value descriptor is None\" ) raise ( e ) indices = self . index_of ( descriptors ) for idx , instance , descriptor in zip ( indices , instances , descriptors , strict = True ): if idx not in self . _grid or instance . fitness > self . _storage [ idx ] . fitness : self . _storage [ idx ] = instance . clone () self . _grid [ idx ] = descriptor","title":"extend"},{"location":"reference/archives/_grid_archive/#archives._grid_archive.GridArchive.index_of","text":"Computes the indices of a batch of descriptors. Parameters: descriptors ( array - like ) \u2013 (batch_size, dimensions) array of descriptors for each instance Raises: ValueError \u2013 descriptors is not shape (batch_size, dimensions) Returns: \u2013 np.ndarray: (batch_size, ) array of integer indices representing the flattened grid coordinates. Source code in digneapy/archives/_grid_archive.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def index_of ( self , descriptors ): \"\"\"Computes the indices of a batch of descriptors. Args: descriptors (array-like): (batch_size, dimensions) array of descriptors for each instance Raises: ValueError: ``descriptors`` is not shape (batch_size, dimensions) Returns: np.ndarray: (batch_size, ) array of integer indices representing the flattened grid coordinates. \"\"\" if len ( descriptors ) == 0 : return np . empty ( 0 ) descriptors = np . asarray ( descriptors ) if ( descriptors . ndim == 1 and descriptors . shape [ 0 ] != len ( self . _dimensions ) or descriptors . ndim == 2 and descriptors . shape [ 1 ] != len ( self . _dimensions ) ): raise ValueError ( f \"Expected descriptors to be an array with shape \" f \"(batch_size, dimensions) (i.e. shape \" f \"(batch_size, { len ( self . _dimensions ) } )) but it had shape \" f \" { descriptors . shape } \" ) grid_indices = ( ( self . _dimensions * ( descriptors - self . _lower_bounds ) + self . _eps ) / self . _interval ) . astype ( int ) # Clip the indexes to make sure they are in the expected range for each dimension clipped = np . clip ( grid_indices , 0 , self . _dimensions - 1 ) return self . _grid_to_int_index ( clipped )","title":"index_of"},{"location":"reference/archives/_grid_archive/#archives._grid_archive.GridArchive.purge_unfeasible","text":"Removes all the unfeasible instances from the grid Source code in digneapy/archives/_grid_archive.py 272 273 274 275 276 277 278 279 def purge_unfeasible ( self , attr : str = \"p\" ): \"\"\"Removes all the unfeasible instances from the grid\"\"\" keys_to_remove = [ i for i in self . _storage . keys () if getattr ( self . _storage [ i ], attr ) < 0 ] for i in keys_to_remove : del self . _grid [ i ] del self . _storage [ i ]","title":"purge_unfeasible"},{"location":"reference/archives/_grid_archive/#archives._grid_archive.GridArchive.remove","text":"Removes all the instances with the matching descriptors in iterable from the grid Source code in digneapy/archives/_grid_archive.py 263 264 265 266 267 268 269 270 def remove ( self , descriptors : np . ndarray ): \"\"\"Removes all the instances with the matching descriptors in iterable from the grid\"\"\" indices_to_remove = self . index_of ( descriptors ) for index in indices_to_remove : if index in self . _grid : del self . _grid [ index ] del self . _storage [ index ]","title":"remove"},{"location":"reference/domains/","text":"@File : init .py @Time : 2023/10/30 12:35:54 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2023, Alejandro Marrero @Desc : None BPP Bases: Problem Source code in digneapy/domains/bpp.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 class BPP ( Problem ): def __init__ ( self , items : Iterable [ int ], capacity : int , seed : int = 42 , * args , ** kwargs , ): self . _items = tuple ( items ) self . _capacity = capacity dim = len ( self . _items ) assert len ( self . _items ) > 0 assert self . _capacity > 0 bounds = list (( 0 , dim - 1 ) for _ in range ( dim )) super () . __init__ ( dimension = dim , bounds = bounds , name = \"BPP\" , seed = seed ) def evaluate ( self , individual : Sequence | Solution ) -> tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Bin Packing. The fitness of the solution is the amount of unused space, as well as the number of bins for a specific solution. Falkenauer (1998) performance metric defined as: (x) = \\\\frac{\\\\sum_{k=1}^{N} \\\\left(\\\\frac{fill_k}{C}\\\\right)^2}{N} Args: individual (Sequence | Solution): Individual to evaluate Returns: Tuple[float]: Falkenauer Fitness \"\"\" if len ( individual ) != self . _dimension : msg = f \"Mismatch between individual variables ( { len ( individual ) } ) and instance variables ( { self . _dimension } ) in { self . __class__ . __name__ } \" raise ValueError ( msg ) used_bins = np . max ( individual ) . astype ( int ) + 1 fill_i = np . zeros ( used_bins ) for item_idx , bin in enumerate ( individual ): fill_i [ bin ] += self . _items [ item_idx ] fitness = ( sum ((( f_i / self . _capacity ) * ( f_i / self . _capacity )) for f_i in fill_i ) / used_bins ) if isinstance ( individual , Solution ): individual . fitness = fitness individual . objectives = ( fitness ,) return ( fitness ,) def __call__ ( self , individual : Sequence | Solution ) -> tuple [ float ]: return self . evaluate ( individual ) def __repr__ ( self ): return f \"BPP<n= { self . _dimension } ,C= { self . _capacity } ,I= { self . _items } >\" def __len__ ( self ): return self . _dimension def __array__ ( self , dtype = np . int32 , copy : Optional [ bool ] = False ) -> npt . ArrayLike : return np . asarray ([ self . _capacity , * self . _items ], dtype = dtype , copy = copy ) def create_solution ( self ) -> Solution : items = list ( range ( self . _dimension )) return Solution ( variables = items ) def to_file ( self , filename : str = \"instance.bpp\" ): with open ( filename , \"w\" ) as file : file . write ( f \" { len ( self ) } \\t { self . _capacity } \\n\\n \" ) content = \" \\n \" . join ( str ( i ) for i in self . _items ) file . write ( content ) @classmethod def from_file ( cls , filename : str ): with open ( filename ) as f : lines = f . readlines () lines = [ line . rstrip () for line in lines ] ( _ , capacity ) = lines [ 0 ] . split () items = list ( int ( i ) for i in lines [ 2 :]) return cls ( items = items , capacity = int ( capacity )) def to_instance ( self ) -> Instance : _vars = [ self . _capacity , * self . _items ] return Instance ( variables = _vars ) evaluate ( individual ) Evaluates the candidate individual with the information of the Bin Packing. The fitness of the solution is the amount of unused space, as well as the number of bins for a specific solution. Falkenauer (1998) performance metric defined as: (x) = \\frac{\\sum_{k=1}^{N} \\left(\\frac{fill_k}{C}\\right)^2}{N} Parameters: individual ( Sequence | Solution ) \u2013 Individual to evaluate Returns: tuple [ float ] \u2013 Tuple[float]: Falkenauer Fitness Source code in digneapy/domains/bpp.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def evaluate ( self , individual : Sequence | Solution ) -> tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Bin Packing. The fitness of the solution is the amount of unused space, as well as the number of bins for a specific solution. Falkenauer (1998) performance metric defined as: (x) = \\\\frac{\\\\sum_{k=1}^{N} \\\\left(\\\\frac{fill_k}{C}\\\\right)^2}{N} Args: individual (Sequence | Solution): Individual to evaluate Returns: Tuple[float]: Falkenauer Fitness \"\"\" if len ( individual ) != self . _dimension : msg = f \"Mismatch between individual variables ( { len ( individual ) } ) and instance variables ( { self . _dimension } ) in { self . __class__ . __name__ } \" raise ValueError ( msg ) used_bins = np . max ( individual ) . astype ( int ) + 1 fill_i = np . zeros ( used_bins ) for item_idx , bin in enumerate ( individual ): fill_i [ bin ] += self . _items [ item_idx ] fitness = ( sum ((( f_i / self . _capacity ) * ( f_i / self . _capacity )) for f_i in fill_i ) / used_bins ) if isinstance ( individual , Solution ): individual . fitness = fitness individual . objectives = ( fitness ,) return ( fitness ,) BPPDomain Bases: Domain Source code in digneapy/domains/bpp.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 class BPPDomain ( Domain ): __capacity_approaches = ( \"evolved\" , \"percentage\" , \"fixed\" ) __feat_names = names = \"mean,std,median,max,min,tiny,small,medium,large,huge\" . split ( \",\" ) def __init__ ( self , dimension : int = 50 , min_i : int = 1 , max_i : int = 1000 , capacity_approach : str = \"fixed\" , max_capacity : int = 100 , capacity_ratio : float = 0.8 , seed : int = 42 , ): if dimension < 0 : raise ValueError ( f \"Expected dimension > 0 got { dimension } \" ) if min_i < 0 : raise ValueError ( f \"Expected min_i > 0 got { min_i } \" ) if max_i < 0 : raise ValueError ( f \"Expected max_i > 0 got { max_i } \" ) if min_i > max_i : raise ValueError ( f \"Expected min_i to be less than max_i got ( { min_i } , { max_i } )\" ) self . _dimension = dimension self . _min_i = min_i self . _max_i = max_i self . _max_capacity = max_capacity if capacity_ratio < 0.0 or capacity_ratio > 1.0 or not float ( capacity_ratio ): self . capacity_ratio = 0.8 # Default msg = \"The capacity ratio must be a float number in the range [0.0-1.0]. Set as 0.8 as default.\" print ( msg ) else : self . capacity_ratio = capacity_ratio if capacity_approach not in self . __capacity_approaches : msg = f \"The capacity approach { capacity_approach } is not available. Please choose between { self . __capacity_approaches } . Evolved approach set as default.\" print ( msg ) self . _capacity_approach = \"fixed\" else : self . _capacity_approach = capacity_approach bounds = [( 1.0 , self . _max_capacity )] + [ ( self . _min_i , self . _max_i ) for _ in range ( self . _dimension ) ] super () . __init__ ( dimension = dimension , bounds = bounds , name = \"BPP\" , seed = seed ) @property def capacity_approach ( self ): return self . _capacity_approach @capacity_approach . setter def capacity_approach ( self , app ): \"\"\"Setter for the Maximum capacity generator approach. It forces to update the variable to one of the specify values Args: app (str): Approach for setting the capacity. It should be fixed, evolved or percentage. \"\"\" if app not in self . __capacity_approaches : msg = f \"The capacity approach { app } is not available. Please choose between { self . __capacity_approaches } . Evolved approach set as default.\" print ( msg ) self . _capacity_approach = \"fixed\" else : self . _capacity_approach = app def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances for the domain. Args: n (int, optional): Number of instances to generate. Defaults to 1. Returns: List[Instance]: A list of Instance objects created from the raw numpy generation \"\"\" instances = np . empty ( shape = ( n , self . dimension + 1 ), dtype = np . int32 ) instances = self . _rng . integers ( low = self . _min_i , high = self . _max_i , size = ( n , self . _dimension + 1 ), dtype = int ) # Sets the capacity according to the method match self . capacity_approach : case \"evolved\" : instances [:, 0 ] = self . _rng . integers ( 1 , self . _max_capacity , size = n ) case \"percentage\" : instances [:, 0 ] = ( np . sum ( instances [:, 1 :], axis = 1 , dtype = int ) * self . capacity_ratio ) case \"fixed\" : instances [:, 0 ] = self . _max_capacity return list ( Instance ( i ) for i in instances ) def extract_features ( self , instances : Sequence [ Instance ]) -> np . ndarray : \"\"\"Extract the features of the instance based on the BPP domain. For the BPP the features are: N, Capacity, MeanWeights, MedianWeights, VarianceWeights, MaxWeight, MinWeight, Huge, Large, Medium, Small, Tiny Args: instances (Instance): Instances to extract the features from Returns: np.ndarray: Values of each feature \"\"\" if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances ) norm_variables = np . asarray ( instances , copy = True ) norm_variables [:, 1 :] = norm_variables [:, 1 :] / norm_variables [:, [ 0 ]] return np . column_stack ( [ np . mean ( norm_variables , axis = 1 ), np . std ( norm_variables , axis = 1 ), np . median ( norm_variables , axis = 1 ), np . max ( norm_variables , axis = 1 ), np . min ( norm_variables , axis = 1 ), np . mean ( norm_variables > 0.5 , axis = 1 ), # Huge np . mean ( ( 0.5 >= norm_variables ) & ( norm_variables > 0.33333333333 ), axis = 1 ), np . mean ( ( 0.33333333333 >= norm_variables ) & ( norm_variables > 0.25 ), axis = 1 ), np . mean ( 0.25 >= norm_variables , axis = 1 ), # Small np . mean ( 0.1 >= norm_variables , axis = 1 ), # Tiny ], ) . astype ( np . float32 ) def extract_features_as_dict ( self , instances : Sequence [ Instance ] ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instances. The key are the names of each feature and the values are the values extracted from instance. Args: instances (Sequence[Instance]): Instances to extract the features from. Returns: Dict[str, np.float32]: Dictionary with the names/values of each feature \"\"\" features = self . extract_features ( instances ) named_features : list [ dict [ str , np . float32 ]] = [{}] * len ( features ) for i , feats in enumerate ( features ): named_features [ i ] = { k : v for k , v in zip ( BPPDomain . __feat_names , feats )} return named_features def generate_problems_from_instances ( self , instances : Sequence [ Instance ] ) -> List [ Problem ]: if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances ) # Assume evolved capacities capacities = instances [:, 0 ] . astype ( np . int32 ) match self . capacity_approach : case \"percentage\" : capacities [:] = ( np . sum ( instances [:, 1 :], axis = 1 ) * self . capacity_ratio ) . astype ( np . int32 ) case \"fixed\" : capacities [:] = self . _max_capacity return list ( BPP ( items = instances [ i , 1 :], capacity = capacities [ i ]) for i in range ( len ( instances )) ) extract_features ( instances ) Extract the features of the instance based on the BPP domain. For the BPP the features are: N, Capacity, MeanWeights, MedianWeights, VarianceWeights, MaxWeight, MinWeight, Huge, Large, Medium, Small, Tiny Parameters: instances ( Instance ) \u2013 Instances to extract the features from Returns: ndarray \u2013 np.ndarray: Values of each feature Source code in digneapy/domains/bpp.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def extract_features ( self , instances : Sequence [ Instance ]) -> np . ndarray : \"\"\"Extract the features of the instance based on the BPP domain. For the BPP the features are: N, Capacity, MeanWeights, MedianWeights, VarianceWeights, MaxWeight, MinWeight, Huge, Large, Medium, Small, Tiny Args: instances (Instance): Instances to extract the features from Returns: np.ndarray: Values of each feature \"\"\" if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances ) norm_variables = np . asarray ( instances , copy = True ) norm_variables [:, 1 :] = norm_variables [:, 1 :] / norm_variables [:, [ 0 ]] return np . column_stack ( [ np . mean ( norm_variables , axis = 1 ), np . std ( norm_variables , axis = 1 ), np . median ( norm_variables , axis = 1 ), np . max ( norm_variables , axis = 1 ), np . min ( norm_variables , axis = 1 ), np . mean ( norm_variables > 0.5 , axis = 1 ), # Huge np . mean ( ( 0.5 >= norm_variables ) & ( norm_variables > 0.33333333333 ), axis = 1 ), np . mean ( ( 0.33333333333 >= norm_variables ) & ( norm_variables > 0.25 ), axis = 1 ), np . mean ( 0.25 >= norm_variables , axis = 1 ), # Small np . mean ( 0.1 >= norm_variables , axis = 1 ), # Tiny ], ) . astype ( np . float32 ) extract_features_as_dict ( instances ) Creates a dictionary with the features of the instances. The key are the names of each feature and the values are the values extracted from instance. Parameters: instances ( Sequence [ Instance ] ) \u2013 Instances to extract the features from. Returns: Dict[str, np.float32]: Dictionary with the names/values of each feature Source code in digneapy/domains/bpp.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 def extract_features_as_dict ( self , instances : Sequence [ Instance ] ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instances. The key are the names of each feature and the values are the values extracted from instance. Args: instances (Sequence[Instance]): Instances to extract the features from. Returns: Dict[str, np.float32]: Dictionary with the names/values of each feature \"\"\" features = self . extract_features ( instances ) named_features : list [ dict [ str , np . float32 ]] = [{}] * len ( features ) for i , feats in enumerate ( features ): named_features [ i ] = { k : v for k , v in zip ( BPPDomain . __feat_names , feats )} return named_features generate_instances ( n = 1 ) Generates N instances for the domain. Parameters: n ( int , default: 1 ) \u2013 Number of instances to generate. Defaults to 1. Returns: List [ Instance ] \u2013 List[Instance]: A list of Instance objects created from the raw numpy generation Source code in digneapy/domains/bpp.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances for the domain. Args: n (int, optional): Number of instances to generate. Defaults to 1. Returns: List[Instance]: A list of Instance objects created from the raw numpy generation \"\"\" instances = np . empty ( shape = ( n , self . dimension + 1 ), dtype = np . int32 ) instances = self . _rng . integers ( low = self . _min_i , high = self . _max_i , size = ( n , self . _dimension + 1 ), dtype = int ) # Sets the capacity according to the method match self . capacity_approach : case \"evolved\" : instances [:, 0 ] = self . _rng . integers ( 1 , self . _max_capacity , size = n ) case \"percentage\" : instances [:, 0 ] = ( np . sum ( instances [:, 1 :], axis = 1 , dtype = int ) * self . capacity_ratio ) case \"fixed\" : instances [:, 0 ] = self . _max_capacity return list ( Instance ( i ) for i in instances ) Knapsack Bases: Problem Source code in digneapy/domains/kp.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 class Knapsack ( Problem ): def __init__ ( self , profits : Sequence [ int ], weights : Sequence [ int ], capacity : int = 0 , seed : int = 42 , * args , ** kwargs , ): if len ( profits ) != len ( weights ): raise ValueError ( f \"The number of weights and profits is different in Knapsack. Got { len ( weights ) } weights and { len ( profits ) } profits\" ) if capacity <= 0 : raise ValueError ( f \"Capacity must be a positive integer. Got { capacity } \" ) super () . __init__ ( dimension = len ( profits ), bounds = [], name = \"KP\" , seed = seed ) self . weights = weights self . profits = profits self . capacity = capacity self . penalty_factor = 100.0 def get_bounds_at ( self , i : int ) -> tuple : if i < 0 or i > self . _dimension : raise ValueError ( f \"Index { i } out-of-range. The bounds are 0- { self . _dimension } \" ) return ( 0 , 1 ) @property def bounds ( self ): return list (( 0 , 1 ) for _ in range ( self . _dimension )) def evaluate ( self , individual : Sequence | Solution | np . ndarray ) -> Tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Knapsack Args: individual (Sequence | Solution): Individual to evaluate Raises: ValueError: Raises an error if the len(individual) != len(profits or weights) Returns: Tuple[float]: Profit \"\"\" if len ( individual ) != self . _dimension : msg = f \"Mismatch between individual variables and instance variables in { self . __class__ . __name__ } \" raise ValueError ( msg ) profit = np . dot ( individual , self . profits ) packed = np . dot ( individual , self . weights ) difference = max ( 0 , packed - self . capacity ) penalty = self . penalty_factor * difference profit -= penalty return ( profit ,) def __call__ ( self , individual : Sequence | Solution | np . ndarray ) -> Tuple [ float ]: return self . evaluate ( individual ) def __array__ ( self , dtype = np . int32 , copy : Optional [ bool ] = None ) -> npt . ArrayLike : \"\"\"Creates a numpy array from the Knapsack instance description. Returns: npt.ArrayLike: 1d numpy array of size 1 + (2 * dimension) \"\"\" return np . asarray ( [ self . capacity , * list ( itertools . chain . from_iterable ([ * zip ( self . weights , self . profits )]) ), ], dtype = dtype , copy = copy , ) def __repr__ ( self ): return f \"KP<n= { len ( self . profits ) } ,C= { self . capacity } >\" def __len__ ( self ): return len ( self . weights ) def create_solution ( self ) -> Solution : chromosome = self . _rng . integers ( low = 0 , high = 1 , size = self . _dimension ) return Solution ( variables = chromosome ) def to_file ( self , filename : str = \"instance.kp\" ): with open ( filename , \"w\" ) as file : file . write ( f \" { len ( self ) } \\t { self . capacity } \\n\\n \" ) content = \" \\n \" . join ( f \" { w_i } \\t { p_i } \" for w_i , p_i in zip ( self . weights , self . profits ) ) file . write ( content ) @classmethod def from_file ( cls , filename : str ): content = np . loadtxt ( filename , dtype = int ) capacity = content [ 0 ][ 1 ] weights , profits = content [ 1 :, 0 ], content [ 1 :, 1 ] return cls ( profits = profits , weights = weights , capacity = capacity ) def to_instance ( self ) -> Instance : _vars = [ self . capacity ] + list ( itertools . chain . from_iterable ([ * zip ( self . weights , self . profits )]) ) return Instance ( variables = _vars ) __array__ ( dtype = np . int32 , copy = None ) Creates a numpy array from the Knapsack instance description. Returns: ArrayLike \u2013 npt.ArrayLike: 1d numpy array of size 1 + (2 * dimension) Source code in digneapy/domains/kp.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def __array__ ( self , dtype = np . int32 , copy : Optional [ bool ] = None ) -> npt . ArrayLike : \"\"\"Creates a numpy array from the Knapsack instance description. Returns: npt.ArrayLike: 1d numpy array of size 1 + (2 * dimension) \"\"\" return np . asarray ( [ self . capacity , * list ( itertools . chain . from_iterable ([ * zip ( self . weights , self . profits )]) ), ], dtype = dtype , copy = copy , ) evaluate ( individual ) Evaluates the candidate individual with the information of the Knapsack Parameters: individual ( Sequence | Solution ) \u2013 Individual to evaluate Raises: ValueError \u2013 Raises an error if the len(individual) != len(profits or weights) Returns: Tuple [ float ] \u2013 Tuple[float]: Profit Source code in digneapy/domains/kp.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def evaluate ( self , individual : Sequence | Solution | np . ndarray ) -> Tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Knapsack Args: individual (Sequence | Solution): Individual to evaluate Raises: ValueError: Raises an error if the len(individual) != len(profits or weights) Returns: Tuple[float]: Profit \"\"\" if len ( individual ) != self . _dimension : msg = f \"Mismatch between individual variables and instance variables in { self . __class__ . __name__ } \" raise ValueError ( msg ) profit = np . dot ( individual , self . profits ) packed = np . dot ( individual , self . weights ) difference = max ( 0 , packed - self . capacity ) penalty = self . penalty_factor * difference profit -= penalty return ( profit ,) KnapsackDomain Bases: Domain Source code in digneapy/domains/kp.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 class KnapsackDomain ( Domain ): __capacity_approaches = ( \"evolved\" , \"percentage\" , \"fixed\" ) def __init__ ( self , dimension : int = 50 , min_p : int = 1 , min_w : int = 1 , max_p : int = 1_000 , max_w : int = 1_000 , capacity_approach : str = \"evolved\" , max_capacity : int = int ( 1e5 ), capacity_ratio : float = 0.8 , seed : Optional [ int ] = None , ): self . min_p = min_p self . min_w = min_w self . max_p = max_p self . max_w = max_w self . max_capacity = max_capacity if capacity_ratio < 0.0 or capacity_ratio > 1.0 or not float ( capacity_ratio ): self . capacity_ratio = 0.8 # Default msg = \"The capacity ratio must be a float number in the range [0.0-1.0]. Set as 0.8 as default.\" print ( msg ) else : self . capacity_ratio = capacity_ratio if capacity_approach not in self . __capacity_approaches : msg = f \"The capacity approach { capacity_approach } is not available. Please choose between { self . __capacity_approaches } . Evolved approach set as default.\" print ( msg ) self . _capacity_approach = \"evolved\" else : self . _capacity_approach = capacity_approach bounds = [( 1.0 , self . max_capacity )] + [ ( min_w , max_w ) if i % 2 == 0 else ( min_p , max_p ) for i in range ( 2 * dimension ) ] super () . __init__ ( dimension = dimension , bounds = bounds , name = \"KP\" , feat_names = \"capacity,max_p,max_w,min_p,min_w,avg_eff,mean,std\" . split ( \",\" ), seed = seed , ) @property def capacity_approach ( self ): return self . _capacity_approach @capacity_approach . setter def capacity_approach ( self , app ): \"\"\"Setter for the Maximum capacity generator approach. It forces to update the variable to one of the specify values Args: app (str): Approach for setting the capacity. It should be fixed, evolved or percentage. \"\"\" if app not in self . __capacity_approaches : msg = f \"The capacity approach { app } is not available. Please choose between { self . __capacity_approaches } . Evolved approach set as default.\" print ( msg ) self . _capacity_approach = \"evolved\" else : self . _capacity_approach = app def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances for the domain. Args: n (int, optional): Number of instances to generate. Defaults to 1. Returns: List[Instance]: A list of Instance objects created from the raw numpy generation \"\"\" weights_and_profits = np . empty ( shape = ( n , self . dimension * 2 ), dtype = np . int32 ) weights_and_profits [:, 0 :: 2 ] = self . _rng . integers ( low = self . min_w , high = self . max_w , size = ( n , self . dimension ) ) weights_and_profits [:, 1 :: 2 ] = self . _rng . integers ( low = self . min_p , high = self . max_p , size = ( n , self . dimension ) ) # Assume fixed capacities = np . full ( n , fill_value = self . max_capacity , dtype = np . int32 ) match self . capacity_approach : case \"evolved\" : capacities [:] = self . _rng . integers ( 1 , self . max_capacity , size = n ) case \"percentage\" : capacities [:] = ( np . sum ( weights_and_profits [:, 1 :: 2 ], axis = 1 ) * self . capacity_ratio ) . astype ( np . int32 ) return list ( Instance ( i ) for i in np . column_stack (( capacities , weights_and_profits )) ) def extract_features ( self , instances : Sequence [ Instance ] | np . ndarray ) -> np . ndarray : \"\"\"Extract the features of the instance based on the domain Args: instances (Sequence[Instance]): Instances to extract the features from. Returns: ArrayLike: 2d array with the features of each instance \"\"\" if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances , copy = True ) features = np . empty ( shape = ( len ( instances ), 8 ), dtype = np . float32 ) weights = instances [:, 1 :: 2 ] profits = instances [:, 2 :: 2 ] features [:, 0 ] = instances [:, 0 ] # Qs features [:, 1 ] = np . max ( profits , axis = 1 ) features [:, 2 ] = np . max ( weights , axis = 1 ) features [:, 3 ] = np . min ( profits , axis = 1 ) features [:, 4 ] = np . min ( weights , axis = 1 ) features [:, 5 ] = np . mean ( profits / weights ) features [:, 6 ] = np . mean ( instances [:, 1 :], axis = 1 ) features [:, 7 ] = np . std ( instances [:, 1 :], axis = 1 ) return features def extract_features_as_dict ( self , instances : Sequence [ Instance ] | np . ndarray ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Args: instances (Sequence[Instance]): Instances to extract the features from. They should in the an array form. Returns: Dict[str, float]: Dictionary with the names/values of each feature \"\"\" features = self . extract_features ( instances ) named_features : list [ dict [ str , np . float32 ]] = [{}] * len ( features ) for i , feats in enumerate ( features ): named_features [ i ] = { k : v for k , v in zip ( self . feat_names , feats )} return named_features def generate_problems_from_instances ( self , instances : Sequence [ Instance ] | np . ndarray ) -> List : \"\"\"Generates a List of Knapsack objects from the instances Args: instances (Sequence[Instance]): Instances to create the problems from Returns: List: List containing len(instances) objects of type Knapsack \"\"\" if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances ) capacities = instances [:, 0 ] . astype ( int ) weights = instances [:, 1 :: 2 ] . astype ( int ) profits = instances [:, 2 :: 2 ] . astype ( int ) # Sets the capacity according to the method if self . capacity_approach == \"percentage\" : capacities [:] = ( np . sum ( weights , axis = 1 ) * self . capacity_ratio ) . astype ( np . int32 ) elif self . capacity_approach == \"fixed\" : capacities [:] = self . max_capacity return list ( Knapsack ( profits = profits [ i ], weights = weights [ i ], capacity = capacities [ i ]) for i in range ( len ( instances )) ) extract_features ( instances ) Extract the features of the instance based on the domain Parameters: instances ( Sequence [ Instance ] ) \u2013 Instances to extract the features from. Returns: ArrayLike ( ndarray ) \u2013 2d array with the features of each instance Source code in digneapy/domains/kp.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 def extract_features ( self , instances : Sequence [ Instance ] | np . ndarray ) -> np . ndarray : \"\"\"Extract the features of the instance based on the domain Args: instances (Sequence[Instance]): Instances to extract the features from. Returns: ArrayLike: 2d array with the features of each instance \"\"\" if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances , copy = True ) features = np . empty ( shape = ( len ( instances ), 8 ), dtype = np . float32 ) weights = instances [:, 1 :: 2 ] profits = instances [:, 2 :: 2 ] features [:, 0 ] = instances [:, 0 ] # Qs features [:, 1 ] = np . max ( profits , axis = 1 ) features [:, 2 ] = np . max ( weights , axis = 1 ) features [:, 3 ] = np . min ( profits , axis = 1 ) features [:, 4 ] = np . min ( weights , axis = 1 ) features [:, 5 ] = np . mean ( profits / weights ) features [:, 6 ] = np . mean ( instances [:, 1 :], axis = 1 ) features [:, 7 ] = np . std ( instances [:, 1 :], axis = 1 ) return features extract_features_as_dict ( instances ) Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Parameters: instances ( Sequence [ Instance ] ) \u2013 Instances to extract the features from. They should in the an array form. Returns: List [ Dict [ str , float32 ]] \u2013 Dict[str, float]: Dictionary with the names/values of each feature Source code in digneapy/domains/kp.py 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def extract_features_as_dict ( self , instances : Sequence [ Instance ] | np . ndarray ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Args: instances (Sequence[Instance]): Instances to extract the features from. They should in the an array form. Returns: Dict[str, float]: Dictionary with the names/values of each feature \"\"\" features = self . extract_features ( instances ) named_features : list [ dict [ str , np . float32 ]] = [{}] * len ( features ) for i , feats in enumerate ( features ): named_features [ i ] = { k : v for k , v in zip ( self . feat_names , feats )} return named_features generate_instances ( n = 1 ) Generates N instances for the domain. Parameters: n ( int , default: 1 ) \u2013 Number of instances to generate. Defaults to 1. Returns: List [ Instance ] \u2013 List[Instance]: A list of Instance objects created from the raw numpy generation Source code in digneapy/domains/kp.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances for the domain. Args: n (int, optional): Number of instances to generate. Defaults to 1. Returns: List[Instance]: A list of Instance objects created from the raw numpy generation \"\"\" weights_and_profits = np . empty ( shape = ( n , self . dimension * 2 ), dtype = np . int32 ) weights_and_profits [:, 0 :: 2 ] = self . _rng . integers ( low = self . min_w , high = self . max_w , size = ( n , self . dimension ) ) weights_and_profits [:, 1 :: 2 ] = self . _rng . integers ( low = self . min_p , high = self . max_p , size = ( n , self . dimension ) ) # Assume fixed capacities = np . full ( n , fill_value = self . max_capacity , dtype = np . int32 ) match self . capacity_approach : case \"evolved\" : capacities [:] = self . _rng . integers ( 1 , self . max_capacity , size = n ) case \"percentage\" : capacities [:] = ( np . sum ( weights_and_profits [:, 1 :: 2 ], axis = 1 ) * self . capacity_ratio ) . astype ( np . int32 ) return list ( Instance ( i ) for i in np . column_stack (( capacities , weights_and_profits )) ) generate_problems_from_instances ( instances ) Generates a List of Knapsack objects from the instances Parameters: instances ( Sequence [ Instance ] ) \u2013 Instances to create the problems from Returns: List ( List ) \u2013 List containing len(instances) objects of type Knapsack Source code in digneapy/domains/kp.py 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 def generate_problems_from_instances ( self , instances : Sequence [ Instance ] | np . ndarray ) -> List : \"\"\"Generates a List of Knapsack objects from the instances Args: instances (Sequence[Instance]): Instances to create the problems from Returns: List: List containing len(instances) objects of type Knapsack \"\"\" if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances ) capacities = instances [:, 0 ] . astype ( int ) weights = instances [:, 1 :: 2 ] . astype ( int ) profits = instances [:, 2 :: 2 ] . astype ( int ) # Sets the capacity according to the method if self . capacity_approach == \"percentage\" : capacities [:] = ( np . sum ( weights , axis = 1 ) * self . capacity_ratio ) . astype ( np . int32 ) elif self . capacity_approach == \"fixed\" : capacities [:] = self . max_capacity return list ( Knapsack ( profits = profits [ i ], weights = weights [ i ], capacity = capacities [ i ]) for i in range ( len ( instances )) ) TSP Bases: Problem Symmetric Travelling Salesman Problem Source code in digneapy/domains/tsp.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 class TSP ( Problem ): \"\"\"Symmetric Travelling Salesman Problem\"\"\" def __init__ ( self , nodes : int , coords : np . ndarray , seed : int = 42 , * args , ** kwargs , ): \"\"\"Creates a new Symmetric Travelling Salesman Problem Args: nodes (int): Number of nodes/cities in the instance to solve coords (np.ndarray(N, 2)): Coordinates of each node/city. \"\"\" self . _nodes = nodes if coords . shape [ 1 ] != 2 : raise ValueError ( f \"Expected coordinates shape to be (N, 2). Instead coords has the following shape: { coords . shape } \" ) if not isinstance ( coords , np . ndarray ): coords = np . asarray ( coords ) self . _coords = coords x_min , y_min = np . min ( self . _coords , axis = 0 ) x_max , y_max = np . max ( self . _coords , axis = 0 ) bounds = list ((( x_min , y_min ), ( x_max , y_max )) for _ in range ( self . _nodes )) super () . __init__ ( dimension = nodes , bounds = bounds , name = \"TSP\" , seed = seed ) self . _distances = np . zeros (( self . _nodes , self . _nodes )) differences = self . _coords [:, np . newaxis , :] - self . _coords [ np . newaxis , :, :] self . _distances = np . sqrt ( np . sum ( differences ** 2 , axis =- 1 )) def __evaluate_constraints ( self , individual : Sequence | Solution ) -> bool : counter = Counter ( individual ) if any ( counter [ c ] != 1 for c in counter if c != 0 ) or ( individual [ 0 ] != 0 or individual [ - 1 ] != 0 ): return False return True def evaluate ( self , individual : Sequence | Solution ) -> tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Travelling Salesmas Problem. The fitness of the solution is the fraction of the sum of the distances of the tour Args: individual (Sequence | Solution): Individual to evaluate Returns: Tuple[float]: Fitness \"\"\" if len ( individual ) != self . _nodes + 1 : msg = f \"Mismatch between individual variables ( { len ( individual ) } ) and instance variables ( { self . _nodes } ) in { self . __class__ . __name__ } . A solution for the TSP must be a sequence of len { self . _nodes + 1 } \" raise ValueError ( msg ) penalty : np . float64 = np . float64 ( 0 ) if self . __evaluate_constraints ( individual ): distance : float = 0.0 for i in range ( len ( individual ) - 2 ): distance += self . _distances [ individual [ i ]][ individual [ i + 1 ]] fitness = 1.0 / distance else : fitness = 2.938736e-39 # --> 1.0 / np.float.max penalty = np . finfo ( np . float64 ) . max if isinstance ( individual , Solution ): individual . fitness = fitness individual . objectives = ( fitness ,) individual . constraints = ( penalty ,) return ( fitness ,) def __call__ ( self , individual : Sequence | Solution ) -> tuple [ float ]: return self . evaluate ( individual ) def __repr__ ( self ): return f \"TSP<n= { self . _nodes } >\" def __len__ ( self ): return self . _nodes def __array__ ( self , dtype = np . float32 , copy : Optional [ bool ] = True ) -> npt . ArrayLike : return np . asarray ( self . _coords , dtype = dtype , copy = copy ) def create_solution ( self ) -> Solution : items = [ 0 ] + list ( range ( 1 , self . _nodes )) + [ 0 ] return Solution ( variables = items ) def to_file ( self , filename : str = \"instance.tsp\" ): with open ( filename , \"w\" ) as file : file . write ( f \" { len ( self ) } \\n\\n \" ) content = \" \\n \" . join ( f \" { x } \\t { y } \" for ( x , y ) in self . _coords ) file . write ( content ) @classmethod def from_file ( cls , filename : str ) -> Self : # TODO: Improve using np.loadtxt with open ( filename ) as f : lines = f . readlines () lines = [ line . rstrip () for line in lines ] nodes = int ( lines [ 0 ]) coords = np . zeros ( shape = ( nodes , 2 ), dtype = np . float32 ) for i , line in enumerate ( lines [ 2 :]): x , y = line . split () coords [ i ] = [ np . float32 ( x ), np . float32 ( y )] return cls ( nodes = nodes , coords = coords ) def to_instance ( self ) -> Instance : return Instance ( variables = self . _coords . flatten ()) __init__ ( nodes , coords , seed = 42 , * args , ** kwargs ) Creates a new Symmetric Travelling Salesman Problem Parameters: nodes ( int ) \u2013 Number of nodes/cities in the instance to solve coords ( ndarray ( N , 2) ) \u2013 Coordinates of each node/city. Source code in digneapy/domains/tsp.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , nodes : int , coords : np . ndarray , seed : int = 42 , * args , ** kwargs , ): \"\"\"Creates a new Symmetric Travelling Salesman Problem Args: nodes (int): Number of nodes/cities in the instance to solve coords (np.ndarray(N, 2)): Coordinates of each node/city. \"\"\" self . _nodes = nodes if coords . shape [ 1 ] != 2 : raise ValueError ( f \"Expected coordinates shape to be (N, 2). Instead coords has the following shape: { coords . shape } \" ) if not isinstance ( coords , np . ndarray ): coords = np . asarray ( coords ) self . _coords = coords x_min , y_min = np . min ( self . _coords , axis = 0 ) x_max , y_max = np . max ( self . _coords , axis = 0 ) bounds = list ((( x_min , y_min ), ( x_max , y_max )) for _ in range ( self . _nodes )) super () . __init__ ( dimension = nodes , bounds = bounds , name = \"TSP\" , seed = seed ) self . _distances = np . zeros (( self . _nodes , self . _nodes )) differences = self . _coords [:, np . newaxis , :] - self . _coords [ np . newaxis , :, :] self . _distances = np . sqrt ( np . sum ( differences ** 2 , axis =- 1 )) evaluate ( individual ) Evaluates the candidate individual with the information of the Travelling Salesmas Problem. The fitness of the solution is the fraction of the sum of the distances of the tour Args: individual (Sequence | Solution): Individual to evaluate Returns: tuple [ float ] \u2013 Tuple[float]: Fitness Source code in digneapy/domains/tsp.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def evaluate ( self , individual : Sequence | Solution ) -> tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Travelling Salesmas Problem. The fitness of the solution is the fraction of the sum of the distances of the tour Args: individual (Sequence | Solution): Individual to evaluate Returns: Tuple[float]: Fitness \"\"\" if len ( individual ) != self . _nodes + 1 : msg = f \"Mismatch between individual variables ( { len ( individual ) } ) and instance variables ( { self . _nodes } ) in { self . __class__ . __name__ } . A solution for the TSP must be a sequence of len { self . _nodes + 1 } \" raise ValueError ( msg ) penalty : np . float64 = np . float64 ( 0 ) if self . __evaluate_constraints ( individual ): distance : float = 0.0 for i in range ( len ( individual ) - 2 ): distance += self . _distances [ individual [ i ]][ individual [ i + 1 ]] fitness = 1.0 / distance else : fitness = 2.938736e-39 # --> 1.0 / np.float.max penalty = np . finfo ( np . float64 ) . max if isinstance ( individual , Solution ): individual . fitness = fitness individual . objectives = ( fitness ,) individual . constraints = ( penalty ,) return ( fitness ,) TSPDomain Bases: Domain Domain to generate instances for the Symmetric Travelling Salesman Problem. Source code in digneapy/domains/tsp.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 class TSPDomain ( Domain ): \"\"\"Domain to generate instances for the Symmetric Travelling Salesman Problem.\"\"\" __FEAT_NAMES = \"size,std_distances,centroid_x,centroid_y,radius,fraction_distances,area,variance_nnNds,variation_nnNds,cluster_ratio,mean_cluster_radius\" . split ( \",\" ) def __init__ ( self , dimension : int = 100 , x_range : Tuple [ int , int ] = ( 0 , 1000 ), y_range : Tuple [ int , int ] = ( 0 , 1000 ), seed : int = 42 , ): \"\"\"Creates a new TSPDomain to generate instances for the Symmetric Travelling Salesman Problem Args: dimension (int, optional): Dimension of the instances to generate. Defaults to 100. x_range (Tuple[int, int], optional): Ranges for the Xs coordinates of each node/city. Defaults to (0, 1000). y_range (Tuple[int, int], optional): Ranges for the ys coordinates of each node/city. Defaults to (0, 1000). Raises: ValueError: If dimension is < 0 ValueError: If x_range OR y_range does not have 2 dimensions each ValueError: If minimum ranges are greater than maximum ranges \"\"\" if dimension < 0 : raise ValueError ( f \"Expected dimension > 0 got { dimension } \" ) if len ( x_range ) != 2 or len ( y_range ) != 2 : raise ValueError ( f \"Expected x_range and y_range to be a tuple with only to integers. Got: x_range = { x_range } and y_range = { y_range } \" ) x_min , x_max = x_range y_min , y_max = y_range if x_min < 0 or x_max <= x_min : raise ValueError ( f \"Expected x_range to be (x_min, x_max) where x_min >= 0 and x_max > x_min. Got: x_range { x_range } \" ) if y_min < 0 or y_max <= y_min : raise ValueError ( f \"Expected y_range to be (y_min, y_max) where y_min >= 0 and y_max > y_min. Got: y_range { y_range } \" ) self . _x_range = x_range self . _y_range = y_range __bounds = [ ( x_min , x_max ) if i % 2 == 0 else ( y_min , y_max ) for i in range ( dimension * 2 ) ] super () . __init__ ( dimension = dimension , bounds = __bounds , name = \"TSP\" , seed = seed ) def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances using numpy. It can return the instances in two formats: 1. A numpy ndarray with the definition of the instances 2. A list of Instance objects created from the raw numpy generation Args: n (int, optional): Number of instances to generate. Defaults to 1. cast (bool, optional): Whether to cast the raw data to Instance objects. Defaults to False. Returns: List[Instance]: Sequence of instances \"\"\" instances = np . empty ( shape = ( n , self . dimension * 2 ), dtype = np . float32 ) instances [:, 0 :: 2 ] = self . _rng . uniform ( low = self . _x_range [ 0 ], high = self . _x_range [ 1 ], size = ( n , ( self . dimension )), ) instances [:, 1 :: 2 ] = self . _rng . uniform ( low = self . _y_range [ 0 ], high = self . _y_range [ 1 ], size = ( n , ( self . dimension )), ) return list ( Instance ( coords ) for coords in instances ) def extract_features ( self , instances : Sequence [ Instance ]) -> np . ndarray : \"\"\"Extract the features of the instance based on the TSP domain. For the TSP the features are: - Size - Standard deviation of the distances - Centroid coordinates - Radius of the instance - Fraction of distinct distances - Rectangular area - Variance of the normalised nearest neighbours distances - Coefficient of variation of the nearest neighbours distances - Cluster ratio - Mean cluster radius Args: instance (Instance): Instance to extract the features from Returns: Tuple[float]: Values of each feature \"\"\" _instances = np . asarray ( instances , copy = True ) N_INSTANCES = len ( _instances ) N_CITIES = len ( _instances [ 0 ]) // 2 # self.dimension // 2 assert _instances is not instances coords = np . asarray ( _instances , copy = True ) . reshape (( N_INSTANCES , N_CITIES , 2 )) xs = coords [:, :, 0 ] ys = coords [:, :, 1 ] areas = ( ( np . max ( xs , axis = 1 ) - np . min ( xs , axis = 1 )) * ( np . max ( ys , axis = 1 ) - np . min ( ys , axis = 1 )) ) . astype ( np . float64 ) # Compute distances for all instances distances = np . zeros (( N_INSTANCES , N_CITIES , N_CITIES )) differences = coords [:, :, np . newaxis , :] - coords [:, np . newaxis , :, :] distances = np . sqrt ( np . sum ( differences ** 2 , axis =- 1 )) mask = ~ np . eye ( N_CITIES , dtype = bool ) std_distances = np . std ( distances [:, mask ], axis = 1 ) centroids = np . mean ( coords , axis = 1 ) expanded_centroids = centroids [:, np . newaxis , :] centroids_distances = np . linalg . norm ( coords - expanded_centroids , axis =- 1 ) radius = np . mean ( centroids_distances , axis = 1 ) fractions = np . array ( [ np . unique ( d [ np . triu_indices_from ( d , k = 1 )]) . size / ( N_CITIES * ( N_CITIES - 1 ) / 2 ) for d in distances ] ) # Top five only norm_distances = np . sort ( distances , axis = 2 )[:, :, :: - 1 ][:, :, : 5 ] / np . max ( distances , axis = ( 1 , 2 ), keepdims = True ) variance_nnds = np . var ( norm_distances , axis = ( 1 , 2 )) variation_nnds = variance_nnds / np . mean ( norm_distances , axis = ( 1 , 2 )) cluster_ratio = np . empty ( shape = N_INSTANCES , dtype = np . float64 ) mean_cluster_radius = np . empty ( shape = N_INSTANCES , dtype = np . float64 ) for i in range ( N_INSTANCES ): scale = np . mean ( np . std ( coords [ i ], axis = 0 )) dbscan = DBSCAN ( eps = 0.2 * scale , min_samples = 1 ) labels = dbscan . fit_predict ( coords [ i ]) unique_labels = [ label for label in set ( labels ) if label != - 1 ] cluster_ratio [ i ] = len ( unique_labels ) / N_CITIES # Cluster radius cluster_radius = np . empty ( shape = len ( unique_labels ), dtype = np . float64 ) for j , label_id in enumerate ( unique_labels ): points_in_cluster = coords [ i ][ labels == label_id ] cluster_centroid = np . mean ( points_in_cluster , axis = 0 ) cluster_radius [ j ] = np . mean ( np . linalg . norm ( points_in_cluster - cluster_centroid , axis = 1 ) ) mean_cluster_radius [ i ] = ( np . mean ( cluster_radius ) if cluster_radius . size > 0 else 0.0 ) return np . column_stack ( [ np . full ( shape = len ( _instances ), fill_value = N_CITIES ), std_distances , centroids [:, 0 ], centroids [:, 1 ], radius , fractions , areas , variance_nnds , variation_nnds , cluster_ratio , mean_cluster_radius , ] ) . astype ( np . float64 ) def extract_features_as_dict ( self , instances : Sequence [ Instance ] ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Args: instance (Instance): Instance to extract the features from Returns: Mapping[str, float]: Dictionary with the names/values of each feature \"\"\" features = self . extract_features ( instances ) named_features : list [ dict [ str , np . float32 ]] = [{}] * len ( features ) for i , feats in enumerate ( features ): named_features [ i ] = { k : v for k , v in zip ( TSPDomain . __FEAT_NAMES , feats )} return named_features def generate_problem_from_instance ( self , instance : Instance ) -> TSP : n_nodes = len ( instance ) // 2 coords = np . array ([ * zip ( instance [:: 2 ], instance [ 1 :: 2 ])]) return TSP ( nodes = n_nodes , coords = coords ) def generate_problems_from_instances ( self , instances : Sequence [ Instance ] ) -> List [ Problem ]: if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances ) dimension = instances . shape [ 1 ] // 2 return list ( TSP ( nodes = dimension , coords = np . array ([ * zip ( instance [ 0 :: 2 ], instance [ 1 :: 2 ])]) ) for instance in instances ) __init__ ( dimension = 100 , x_range = ( 0 , 1000 ), y_range = ( 0 , 1000 ), seed = 42 ) Creates a new TSPDomain to generate instances for the Symmetric Travelling Salesman Problem Parameters: dimension ( int , default: 100 ) \u2013 Dimension of the instances to generate. Defaults to 100. x_range ( Tuple [ int , int ] , default: (0, 1000) ) \u2013 Ranges for the Xs coordinates of each node/city. Defaults to (0, 1000). y_range ( Tuple [ int , int ] , default: (0, 1000) ) \u2013 Ranges for the ys coordinates of each node/city. Defaults to (0, 1000). Raises: ValueError \u2013 If dimension is < 0 ValueError \u2013 If x_range OR y_range does not have 2 dimensions each ValueError \u2013 If minimum ranges are greater than maximum ranges Source code in digneapy/domains/tsp.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def __init__ ( self , dimension : int = 100 , x_range : Tuple [ int , int ] = ( 0 , 1000 ), y_range : Tuple [ int , int ] = ( 0 , 1000 ), seed : int = 42 , ): \"\"\"Creates a new TSPDomain to generate instances for the Symmetric Travelling Salesman Problem Args: dimension (int, optional): Dimension of the instances to generate. Defaults to 100. x_range (Tuple[int, int], optional): Ranges for the Xs coordinates of each node/city. Defaults to (0, 1000). y_range (Tuple[int, int], optional): Ranges for the ys coordinates of each node/city. Defaults to (0, 1000). Raises: ValueError: If dimension is < 0 ValueError: If x_range OR y_range does not have 2 dimensions each ValueError: If minimum ranges are greater than maximum ranges \"\"\" if dimension < 0 : raise ValueError ( f \"Expected dimension > 0 got { dimension } \" ) if len ( x_range ) != 2 or len ( y_range ) != 2 : raise ValueError ( f \"Expected x_range and y_range to be a tuple with only to integers. Got: x_range = { x_range } and y_range = { y_range } \" ) x_min , x_max = x_range y_min , y_max = y_range if x_min < 0 or x_max <= x_min : raise ValueError ( f \"Expected x_range to be (x_min, x_max) where x_min >= 0 and x_max > x_min. Got: x_range { x_range } \" ) if y_min < 0 or y_max <= y_min : raise ValueError ( f \"Expected y_range to be (y_min, y_max) where y_min >= 0 and y_max > y_min. Got: y_range { y_range } \" ) self . _x_range = x_range self . _y_range = y_range __bounds = [ ( x_min , x_max ) if i % 2 == 0 else ( y_min , y_max ) for i in range ( dimension * 2 ) ] super () . __init__ ( dimension = dimension , bounds = __bounds , name = \"TSP\" , seed = seed ) extract_features ( instances ) Extract the features of the instance based on the TSP domain. For the TSP the features are: - Size - Standard deviation of the distances - Centroid coordinates - Radius of the instance - Fraction of distinct distances - Rectangular area - Variance of the normalised nearest neighbours distances - Coefficient of variation of the nearest neighbours distances - Cluster ratio - Mean cluster radius Args: instance (Instance): Instance to extract the features from Returns: ndarray \u2013 Tuple[float]: Values of each feature Source code in digneapy/domains/tsp.py 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 def extract_features ( self , instances : Sequence [ Instance ]) -> np . ndarray : \"\"\"Extract the features of the instance based on the TSP domain. For the TSP the features are: - Size - Standard deviation of the distances - Centroid coordinates - Radius of the instance - Fraction of distinct distances - Rectangular area - Variance of the normalised nearest neighbours distances - Coefficient of variation of the nearest neighbours distances - Cluster ratio - Mean cluster radius Args: instance (Instance): Instance to extract the features from Returns: Tuple[float]: Values of each feature \"\"\" _instances = np . asarray ( instances , copy = True ) N_INSTANCES = len ( _instances ) N_CITIES = len ( _instances [ 0 ]) // 2 # self.dimension // 2 assert _instances is not instances coords = np . asarray ( _instances , copy = True ) . reshape (( N_INSTANCES , N_CITIES , 2 )) xs = coords [:, :, 0 ] ys = coords [:, :, 1 ] areas = ( ( np . max ( xs , axis = 1 ) - np . min ( xs , axis = 1 )) * ( np . max ( ys , axis = 1 ) - np . min ( ys , axis = 1 )) ) . astype ( np . float64 ) # Compute distances for all instances distances = np . zeros (( N_INSTANCES , N_CITIES , N_CITIES )) differences = coords [:, :, np . newaxis , :] - coords [:, np . newaxis , :, :] distances = np . sqrt ( np . sum ( differences ** 2 , axis =- 1 )) mask = ~ np . eye ( N_CITIES , dtype = bool ) std_distances = np . std ( distances [:, mask ], axis = 1 ) centroids = np . mean ( coords , axis = 1 ) expanded_centroids = centroids [:, np . newaxis , :] centroids_distances = np . linalg . norm ( coords - expanded_centroids , axis =- 1 ) radius = np . mean ( centroids_distances , axis = 1 ) fractions = np . array ( [ np . unique ( d [ np . triu_indices_from ( d , k = 1 )]) . size / ( N_CITIES * ( N_CITIES - 1 ) / 2 ) for d in distances ] ) # Top five only norm_distances = np . sort ( distances , axis = 2 )[:, :, :: - 1 ][:, :, : 5 ] / np . max ( distances , axis = ( 1 , 2 ), keepdims = True ) variance_nnds = np . var ( norm_distances , axis = ( 1 , 2 )) variation_nnds = variance_nnds / np . mean ( norm_distances , axis = ( 1 , 2 )) cluster_ratio = np . empty ( shape = N_INSTANCES , dtype = np . float64 ) mean_cluster_radius = np . empty ( shape = N_INSTANCES , dtype = np . float64 ) for i in range ( N_INSTANCES ): scale = np . mean ( np . std ( coords [ i ], axis = 0 )) dbscan = DBSCAN ( eps = 0.2 * scale , min_samples = 1 ) labels = dbscan . fit_predict ( coords [ i ]) unique_labels = [ label for label in set ( labels ) if label != - 1 ] cluster_ratio [ i ] = len ( unique_labels ) / N_CITIES # Cluster radius cluster_radius = np . empty ( shape = len ( unique_labels ), dtype = np . float64 ) for j , label_id in enumerate ( unique_labels ): points_in_cluster = coords [ i ][ labels == label_id ] cluster_centroid = np . mean ( points_in_cluster , axis = 0 ) cluster_radius [ j ] = np . mean ( np . linalg . norm ( points_in_cluster - cluster_centroid , axis = 1 ) ) mean_cluster_radius [ i ] = ( np . mean ( cluster_radius ) if cluster_radius . size > 0 else 0.0 ) return np . column_stack ( [ np . full ( shape = len ( _instances ), fill_value = N_CITIES ), std_distances , centroids [:, 0 ], centroids [:, 1 ], radius , fractions , areas , variance_nnds , variation_nnds , cluster_ratio , mean_cluster_radius , ] ) . astype ( np . float64 ) extract_features_as_dict ( instances ) Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Parameters: instance ( Instance ) \u2013 Instance to extract the features from Returns: List [ Dict [ str , float32 ]] \u2013 Mapping[str, float]: Dictionary with the names/values of each feature Source code in digneapy/domains/tsp.py 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 def extract_features_as_dict ( self , instances : Sequence [ Instance ] ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Args: instance (Instance): Instance to extract the features from Returns: Mapping[str, float]: Dictionary with the names/values of each feature \"\"\" features = self . extract_features ( instances ) named_features : list [ dict [ str , np . float32 ]] = [{}] * len ( features ) for i , feats in enumerate ( features ): named_features [ i ] = { k : v for k , v in zip ( TSPDomain . __FEAT_NAMES , feats )} return named_features generate_instances ( n = 1 ) Generates N instances using numpy. It can return the instances in two formats: 1. A numpy ndarray with the definition of the instances 2. A list of Instance objects created from the raw numpy generation Parameters: n ( int , default: 1 ) \u2013 Number of instances to generate. Defaults to 1. cast ( bool ) \u2013 Whether to cast the raw data to Instance objects. Defaults to False. Returns: List [ Instance ] \u2013 List[Instance]: Sequence of instances Source code in digneapy/domains/tsp.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances using numpy. It can return the instances in two formats: 1. A numpy ndarray with the definition of the instances 2. A list of Instance objects created from the raw numpy generation Args: n (int, optional): Number of instances to generate. Defaults to 1. cast (bool, optional): Whether to cast the raw data to Instance objects. Defaults to False. Returns: List[Instance]: Sequence of instances \"\"\" instances = np . empty ( shape = ( n , self . dimension * 2 ), dtype = np . float32 ) instances [:, 0 :: 2 ] = self . _rng . uniform ( low = self . _x_range [ 0 ], high = self . _x_range [ 1 ], size = ( n , ( self . dimension )), ) instances [:, 1 :: 2 ] = self . _rng . uniform ( low = self . _y_range [ 0 ], high = self . _y_range [ 1 ], size = ( n , ( self . dimension )), ) return list ( Instance ( coords ) for coords in instances )","title":"Index"},{"location":"reference/domains/#domains.BPP","text":"Bases: Problem Source code in digneapy/domains/bpp.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 class BPP ( Problem ): def __init__ ( self , items : Iterable [ int ], capacity : int , seed : int = 42 , * args , ** kwargs , ): self . _items = tuple ( items ) self . _capacity = capacity dim = len ( self . _items ) assert len ( self . _items ) > 0 assert self . _capacity > 0 bounds = list (( 0 , dim - 1 ) for _ in range ( dim )) super () . __init__ ( dimension = dim , bounds = bounds , name = \"BPP\" , seed = seed ) def evaluate ( self , individual : Sequence | Solution ) -> tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Bin Packing. The fitness of the solution is the amount of unused space, as well as the number of bins for a specific solution. Falkenauer (1998) performance metric defined as: (x) = \\\\frac{\\\\sum_{k=1}^{N} \\\\left(\\\\frac{fill_k}{C}\\\\right)^2}{N} Args: individual (Sequence | Solution): Individual to evaluate Returns: Tuple[float]: Falkenauer Fitness \"\"\" if len ( individual ) != self . _dimension : msg = f \"Mismatch between individual variables ( { len ( individual ) } ) and instance variables ( { self . _dimension } ) in { self . __class__ . __name__ } \" raise ValueError ( msg ) used_bins = np . max ( individual ) . astype ( int ) + 1 fill_i = np . zeros ( used_bins ) for item_idx , bin in enumerate ( individual ): fill_i [ bin ] += self . _items [ item_idx ] fitness = ( sum ((( f_i / self . _capacity ) * ( f_i / self . _capacity )) for f_i in fill_i ) / used_bins ) if isinstance ( individual , Solution ): individual . fitness = fitness individual . objectives = ( fitness ,) return ( fitness ,) def __call__ ( self , individual : Sequence | Solution ) -> tuple [ float ]: return self . evaluate ( individual ) def __repr__ ( self ): return f \"BPP<n= { self . _dimension } ,C= { self . _capacity } ,I= { self . _items } >\" def __len__ ( self ): return self . _dimension def __array__ ( self , dtype = np . int32 , copy : Optional [ bool ] = False ) -> npt . ArrayLike : return np . asarray ([ self . _capacity , * self . _items ], dtype = dtype , copy = copy ) def create_solution ( self ) -> Solution : items = list ( range ( self . _dimension )) return Solution ( variables = items ) def to_file ( self , filename : str = \"instance.bpp\" ): with open ( filename , \"w\" ) as file : file . write ( f \" { len ( self ) } \\t { self . _capacity } \\n\\n \" ) content = \" \\n \" . join ( str ( i ) for i in self . _items ) file . write ( content ) @classmethod def from_file ( cls , filename : str ): with open ( filename ) as f : lines = f . readlines () lines = [ line . rstrip () for line in lines ] ( _ , capacity ) = lines [ 0 ] . split () items = list ( int ( i ) for i in lines [ 2 :]) return cls ( items = items , capacity = int ( capacity )) def to_instance ( self ) -> Instance : _vars = [ self . _capacity , * self . _items ] return Instance ( variables = _vars )","title":"BPP"},{"location":"reference/domains/#domains.BPP.evaluate","text":"Evaluates the candidate individual with the information of the Bin Packing. The fitness of the solution is the amount of unused space, as well as the number of bins for a specific solution. Falkenauer (1998) performance metric defined as: (x) = \\frac{\\sum_{k=1}^{N} \\left(\\frac{fill_k}{C}\\right)^2}{N} Parameters: individual ( Sequence | Solution ) \u2013 Individual to evaluate Returns: tuple [ float ] \u2013 Tuple[float]: Falkenauer Fitness Source code in digneapy/domains/bpp.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def evaluate ( self , individual : Sequence | Solution ) -> tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Bin Packing. The fitness of the solution is the amount of unused space, as well as the number of bins for a specific solution. Falkenauer (1998) performance metric defined as: (x) = \\\\frac{\\\\sum_{k=1}^{N} \\\\left(\\\\frac{fill_k}{C}\\\\right)^2}{N} Args: individual (Sequence | Solution): Individual to evaluate Returns: Tuple[float]: Falkenauer Fitness \"\"\" if len ( individual ) != self . _dimension : msg = f \"Mismatch between individual variables ( { len ( individual ) } ) and instance variables ( { self . _dimension } ) in { self . __class__ . __name__ } \" raise ValueError ( msg ) used_bins = np . max ( individual ) . astype ( int ) + 1 fill_i = np . zeros ( used_bins ) for item_idx , bin in enumerate ( individual ): fill_i [ bin ] += self . _items [ item_idx ] fitness = ( sum ((( f_i / self . _capacity ) * ( f_i / self . _capacity )) for f_i in fill_i ) / used_bins ) if isinstance ( individual , Solution ): individual . fitness = fitness individual . objectives = ( fitness ,) return ( fitness ,)","title":"evaluate"},{"location":"reference/domains/#domains.BPPDomain","text":"Bases: Domain Source code in digneapy/domains/bpp.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 class BPPDomain ( Domain ): __capacity_approaches = ( \"evolved\" , \"percentage\" , \"fixed\" ) __feat_names = names = \"mean,std,median,max,min,tiny,small,medium,large,huge\" . split ( \",\" ) def __init__ ( self , dimension : int = 50 , min_i : int = 1 , max_i : int = 1000 , capacity_approach : str = \"fixed\" , max_capacity : int = 100 , capacity_ratio : float = 0.8 , seed : int = 42 , ): if dimension < 0 : raise ValueError ( f \"Expected dimension > 0 got { dimension } \" ) if min_i < 0 : raise ValueError ( f \"Expected min_i > 0 got { min_i } \" ) if max_i < 0 : raise ValueError ( f \"Expected max_i > 0 got { max_i } \" ) if min_i > max_i : raise ValueError ( f \"Expected min_i to be less than max_i got ( { min_i } , { max_i } )\" ) self . _dimension = dimension self . _min_i = min_i self . _max_i = max_i self . _max_capacity = max_capacity if capacity_ratio < 0.0 or capacity_ratio > 1.0 or not float ( capacity_ratio ): self . capacity_ratio = 0.8 # Default msg = \"The capacity ratio must be a float number in the range [0.0-1.0]. Set as 0.8 as default.\" print ( msg ) else : self . capacity_ratio = capacity_ratio if capacity_approach not in self . __capacity_approaches : msg = f \"The capacity approach { capacity_approach } is not available. Please choose between { self . __capacity_approaches } . Evolved approach set as default.\" print ( msg ) self . _capacity_approach = \"fixed\" else : self . _capacity_approach = capacity_approach bounds = [( 1.0 , self . _max_capacity )] + [ ( self . _min_i , self . _max_i ) for _ in range ( self . _dimension ) ] super () . __init__ ( dimension = dimension , bounds = bounds , name = \"BPP\" , seed = seed ) @property def capacity_approach ( self ): return self . _capacity_approach @capacity_approach . setter def capacity_approach ( self , app ): \"\"\"Setter for the Maximum capacity generator approach. It forces to update the variable to one of the specify values Args: app (str): Approach for setting the capacity. It should be fixed, evolved or percentage. \"\"\" if app not in self . __capacity_approaches : msg = f \"The capacity approach { app } is not available. Please choose between { self . __capacity_approaches } . Evolved approach set as default.\" print ( msg ) self . _capacity_approach = \"fixed\" else : self . _capacity_approach = app def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances for the domain. Args: n (int, optional): Number of instances to generate. Defaults to 1. Returns: List[Instance]: A list of Instance objects created from the raw numpy generation \"\"\" instances = np . empty ( shape = ( n , self . dimension + 1 ), dtype = np . int32 ) instances = self . _rng . integers ( low = self . _min_i , high = self . _max_i , size = ( n , self . _dimension + 1 ), dtype = int ) # Sets the capacity according to the method match self . capacity_approach : case \"evolved\" : instances [:, 0 ] = self . _rng . integers ( 1 , self . _max_capacity , size = n ) case \"percentage\" : instances [:, 0 ] = ( np . sum ( instances [:, 1 :], axis = 1 , dtype = int ) * self . capacity_ratio ) case \"fixed\" : instances [:, 0 ] = self . _max_capacity return list ( Instance ( i ) for i in instances ) def extract_features ( self , instances : Sequence [ Instance ]) -> np . ndarray : \"\"\"Extract the features of the instance based on the BPP domain. For the BPP the features are: N, Capacity, MeanWeights, MedianWeights, VarianceWeights, MaxWeight, MinWeight, Huge, Large, Medium, Small, Tiny Args: instances (Instance): Instances to extract the features from Returns: np.ndarray: Values of each feature \"\"\" if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances ) norm_variables = np . asarray ( instances , copy = True ) norm_variables [:, 1 :] = norm_variables [:, 1 :] / norm_variables [:, [ 0 ]] return np . column_stack ( [ np . mean ( norm_variables , axis = 1 ), np . std ( norm_variables , axis = 1 ), np . median ( norm_variables , axis = 1 ), np . max ( norm_variables , axis = 1 ), np . min ( norm_variables , axis = 1 ), np . mean ( norm_variables > 0.5 , axis = 1 ), # Huge np . mean ( ( 0.5 >= norm_variables ) & ( norm_variables > 0.33333333333 ), axis = 1 ), np . mean ( ( 0.33333333333 >= norm_variables ) & ( norm_variables > 0.25 ), axis = 1 ), np . mean ( 0.25 >= norm_variables , axis = 1 ), # Small np . mean ( 0.1 >= norm_variables , axis = 1 ), # Tiny ], ) . astype ( np . float32 ) def extract_features_as_dict ( self , instances : Sequence [ Instance ] ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instances. The key are the names of each feature and the values are the values extracted from instance. Args: instances (Sequence[Instance]): Instances to extract the features from. Returns: Dict[str, np.float32]: Dictionary with the names/values of each feature \"\"\" features = self . extract_features ( instances ) named_features : list [ dict [ str , np . float32 ]] = [{}] * len ( features ) for i , feats in enumerate ( features ): named_features [ i ] = { k : v for k , v in zip ( BPPDomain . __feat_names , feats )} return named_features def generate_problems_from_instances ( self , instances : Sequence [ Instance ] ) -> List [ Problem ]: if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances ) # Assume evolved capacities capacities = instances [:, 0 ] . astype ( np . int32 ) match self . capacity_approach : case \"percentage\" : capacities [:] = ( np . sum ( instances [:, 1 :], axis = 1 ) * self . capacity_ratio ) . astype ( np . int32 ) case \"fixed\" : capacities [:] = self . _max_capacity return list ( BPP ( items = instances [ i , 1 :], capacity = capacities [ i ]) for i in range ( len ( instances )) )","title":"BPPDomain"},{"location":"reference/domains/#domains.BPPDomain.extract_features","text":"Extract the features of the instance based on the BPP domain. For the BPP the features are: N, Capacity, MeanWeights, MedianWeights, VarianceWeights, MaxWeight, MinWeight, Huge, Large, Medium, Small, Tiny Parameters: instances ( Instance ) \u2013 Instances to extract the features from Returns: ndarray \u2013 np.ndarray: Values of each feature Source code in digneapy/domains/bpp.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def extract_features ( self , instances : Sequence [ Instance ]) -> np . ndarray : \"\"\"Extract the features of the instance based on the BPP domain. For the BPP the features are: N, Capacity, MeanWeights, MedianWeights, VarianceWeights, MaxWeight, MinWeight, Huge, Large, Medium, Small, Tiny Args: instances (Instance): Instances to extract the features from Returns: np.ndarray: Values of each feature \"\"\" if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances ) norm_variables = np . asarray ( instances , copy = True ) norm_variables [:, 1 :] = norm_variables [:, 1 :] / norm_variables [:, [ 0 ]] return np . column_stack ( [ np . mean ( norm_variables , axis = 1 ), np . std ( norm_variables , axis = 1 ), np . median ( norm_variables , axis = 1 ), np . max ( norm_variables , axis = 1 ), np . min ( norm_variables , axis = 1 ), np . mean ( norm_variables > 0.5 , axis = 1 ), # Huge np . mean ( ( 0.5 >= norm_variables ) & ( norm_variables > 0.33333333333 ), axis = 1 ), np . mean ( ( 0.33333333333 >= norm_variables ) & ( norm_variables > 0.25 ), axis = 1 ), np . mean ( 0.25 >= norm_variables , axis = 1 ), # Small np . mean ( 0.1 >= norm_variables , axis = 1 ), # Tiny ], ) . astype ( np . float32 )","title":"extract_features"},{"location":"reference/domains/#domains.BPPDomain.extract_features_as_dict","text":"Creates a dictionary with the features of the instances. The key are the names of each feature and the values are the values extracted from instance. Parameters: instances ( Sequence [ Instance ] ) \u2013 Instances to extract the features from. Returns: Dict[str, np.float32]: Dictionary with the names/values of each feature Source code in digneapy/domains/bpp.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 def extract_features_as_dict ( self , instances : Sequence [ Instance ] ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instances. The key are the names of each feature and the values are the values extracted from instance. Args: instances (Sequence[Instance]): Instances to extract the features from. Returns: Dict[str, np.float32]: Dictionary with the names/values of each feature \"\"\" features = self . extract_features ( instances ) named_features : list [ dict [ str , np . float32 ]] = [{}] * len ( features ) for i , feats in enumerate ( features ): named_features [ i ] = { k : v for k , v in zip ( BPPDomain . __feat_names , feats )} return named_features","title":"extract_features_as_dict"},{"location":"reference/domains/#domains.BPPDomain.generate_instances","text":"Generates N instances for the domain. Parameters: n ( int , default: 1 ) \u2013 Number of instances to generate. Defaults to 1. Returns: List [ Instance ] \u2013 List[Instance]: A list of Instance objects created from the raw numpy generation Source code in digneapy/domains/bpp.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances for the domain. Args: n (int, optional): Number of instances to generate. Defaults to 1. Returns: List[Instance]: A list of Instance objects created from the raw numpy generation \"\"\" instances = np . empty ( shape = ( n , self . dimension + 1 ), dtype = np . int32 ) instances = self . _rng . integers ( low = self . _min_i , high = self . _max_i , size = ( n , self . _dimension + 1 ), dtype = int ) # Sets the capacity according to the method match self . capacity_approach : case \"evolved\" : instances [:, 0 ] = self . _rng . integers ( 1 , self . _max_capacity , size = n ) case \"percentage\" : instances [:, 0 ] = ( np . sum ( instances [:, 1 :], axis = 1 , dtype = int ) * self . capacity_ratio ) case \"fixed\" : instances [:, 0 ] = self . _max_capacity return list ( Instance ( i ) for i in instances )","title":"generate_instances"},{"location":"reference/domains/#domains.Knapsack","text":"Bases: Problem Source code in digneapy/domains/kp.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 class Knapsack ( Problem ): def __init__ ( self , profits : Sequence [ int ], weights : Sequence [ int ], capacity : int = 0 , seed : int = 42 , * args , ** kwargs , ): if len ( profits ) != len ( weights ): raise ValueError ( f \"The number of weights and profits is different in Knapsack. Got { len ( weights ) } weights and { len ( profits ) } profits\" ) if capacity <= 0 : raise ValueError ( f \"Capacity must be a positive integer. Got { capacity } \" ) super () . __init__ ( dimension = len ( profits ), bounds = [], name = \"KP\" , seed = seed ) self . weights = weights self . profits = profits self . capacity = capacity self . penalty_factor = 100.0 def get_bounds_at ( self , i : int ) -> tuple : if i < 0 or i > self . _dimension : raise ValueError ( f \"Index { i } out-of-range. The bounds are 0- { self . _dimension } \" ) return ( 0 , 1 ) @property def bounds ( self ): return list (( 0 , 1 ) for _ in range ( self . _dimension )) def evaluate ( self , individual : Sequence | Solution | np . ndarray ) -> Tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Knapsack Args: individual (Sequence | Solution): Individual to evaluate Raises: ValueError: Raises an error if the len(individual) != len(profits or weights) Returns: Tuple[float]: Profit \"\"\" if len ( individual ) != self . _dimension : msg = f \"Mismatch between individual variables and instance variables in { self . __class__ . __name__ } \" raise ValueError ( msg ) profit = np . dot ( individual , self . profits ) packed = np . dot ( individual , self . weights ) difference = max ( 0 , packed - self . capacity ) penalty = self . penalty_factor * difference profit -= penalty return ( profit ,) def __call__ ( self , individual : Sequence | Solution | np . ndarray ) -> Tuple [ float ]: return self . evaluate ( individual ) def __array__ ( self , dtype = np . int32 , copy : Optional [ bool ] = None ) -> npt . ArrayLike : \"\"\"Creates a numpy array from the Knapsack instance description. Returns: npt.ArrayLike: 1d numpy array of size 1 + (2 * dimension) \"\"\" return np . asarray ( [ self . capacity , * list ( itertools . chain . from_iterable ([ * zip ( self . weights , self . profits )]) ), ], dtype = dtype , copy = copy , ) def __repr__ ( self ): return f \"KP<n= { len ( self . profits ) } ,C= { self . capacity } >\" def __len__ ( self ): return len ( self . weights ) def create_solution ( self ) -> Solution : chromosome = self . _rng . integers ( low = 0 , high = 1 , size = self . _dimension ) return Solution ( variables = chromosome ) def to_file ( self , filename : str = \"instance.kp\" ): with open ( filename , \"w\" ) as file : file . write ( f \" { len ( self ) } \\t { self . capacity } \\n\\n \" ) content = \" \\n \" . join ( f \" { w_i } \\t { p_i } \" for w_i , p_i in zip ( self . weights , self . profits ) ) file . write ( content ) @classmethod def from_file ( cls , filename : str ): content = np . loadtxt ( filename , dtype = int ) capacity = content [ 0 ][ 1 ] weights , profits = content [ 1 :, 0 ], content [ 1 :, 1 ] return cls ( profits = profits , weights = weights , capacity = capacity ) def to_instance ( self ) -> Instance : _vars = [ self . capacity ] + list ( itertools . chain . from_iterable ([ * zip ( self . weights , self . profits )]) ) return Instance ( variables = _vars )","title":"Knapsack"},{"location":"reference/domains/#domains.Knapsack.__array__","text":"Creates a numpy array from the Knapsack instance description. Returns: ArrayLike \u2013 npt.ArrayLike: 1d numpy array of size 1 + (2 * dimension) Source code in digneapy/domains/kp.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def __array__ ( self , dtype = np . int32 , copy : Optional [ bool ] = None ) -> npt . ArrayLike : \"\"\"Creates a numpy array from the Knapsack instance description. Returns: npt.ArrayLike: 1d numpy array of size 1 + (2 * dimension) \"\"\" return np . asarray ( [ self . capacity , * list ( itertools . chain . from_iterable ([ * zip ( self . weights , self . profits )]) ), ], dtype = dtype , copy = copy , )","title":"__array__"},{"location":"reference/domains/#domains.Knapsack.evaluate","text":"Evaluates the candidate individual with the information of the Knapsack Parameters: individual ( Sequence | Solution ) \u2013 Individual to evaluate Raises: ValueError \u2013 Raises an error if the len(individual) != len(profits or weights) Returns: Tuple [ float ] \u2013 Tuple[float]: Profit Source code in digneapy/domains/kp.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def evaluate ( self , individual : Sequence | Solution | np . ndarray ) -> Tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Knapsack Args: individual (Sequence | Solution): Individual to evaluate Raises: ValueError: Raises an error if the len(individual) != len(profits or weights) Returns: Tuple[float]: Profit \"\"\" if len ( individual ) != self . _dimension : msg = f \"Mismatch between individual variables and instance variables in { self . __class__ . __name__ } \" raise ValueError ( msg ) profit = np . dot ( individual , self . profits ) packed = np . dot ( individual , self . weights ) difference = max ( 0 , packed - self . capacity ) penalty = self . penalty_factor * difference profit -= penalty return ( profit ,)","title":"evaluate"},{"location":"reference/domains/#domains.KnapsackDomain","text":"Bases: Domain Source code in digneapy/domains/kp.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 class KnapsackDomain ( Domain ): __capacity_approaches = ( \"evolved\" , \"percentage\" , \"fixed\" ) def __init__ ( self , dimension : int = 50 , min_p : int = 1 , min_w : int = 1 , max_p : int = 1_000 , max_w : int = 1_000 , capacity_approach : str = \"evolved\" , max_capacity : int = int ( 1e5 ), capacity_ratio : float = 0.8 , seed : Optional [ int ] = None , ): self . min_p = min_p self . min_w = min_w self . max_p = max_p self . max_w = max_w self . max_capacity = max_capacity if capacity_ratio < 0.0 or capacity_ratio > 1.0 or not float ( capacity_ratio ): self . capacity_ratio = 0.8 # Default msg = \"The capacity ratio must be a float number in the range [0.0-1.0]. Set as 0.8 as default.\" print ( msg ) else : self . capacity_ratio = capacity_ratio if capacity_approach not in self . __capacity_approaches : msg = f \"The capacity approach { capacity_approach } is not available. Please choose between { self . __capacity_approaches } . Evolved approach set as default.\" print ( msg ) self . _capacity_approach = \"evolved\" else : self . _capacity_approach = capacity_approach bounds = [( 1.0 , self . max_capacity )] + [ ( min_w , max_w ) if i % 2 == 0 else ( min_p , max_p ) for i in range ( 2 * dimension ) ] super () . __init__ ( dimension = dimension , bounds = bounds , name = \"KP\" , feat_names = \"capacity,max_p,max_w,min_p,min_w,avg_eff,mean,std\" . split ( \",\" ), seed = seed , ) @property def capacity_approach ( self ): return self . _capacity_approach @capacity_approach . setter def capacity_approach ( self , app ): \"\"\"Setter for the Maximum capacity generator approach. It forces to update the variable to one of the specify values Args: app (str): Approach for setting the capacity. It should be fixed, evolved or percentage. \"\"\" if app not in self . __capacity_approaches : msg = f \"The capacity approach { app } is not available. Please choose between { self . __capacity_approaches } . Evolved approach set as default.\" print ( msg ) self . _capacity_approach = \"evolved\" else : self . _capacity_approach = app def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances for the domain. Args: n (int, optional): Number of instances to generate. Defaults to 1. Returns: List[Instance]: A list of Instance objects created from the raw numpy generation \"\"\" weights_and_profits = np . empty ( shape = ( n , self . dimension * 2 ), dtype = np . int32 ) weights_and_profits [:, 0 :: 2 ] = self . _rng . integers ( low = self . min_w , high = self . max_w , size = ( n , self . dimension ) ) weights_and_profits [:, 1 :: 2 ] = self . _rng . integers ( low = self . min_p , high = self . max_p , size = ( n , self . dimension ) ) # Assume fixed capacities = np . full ( n , fill_value = self . max_capacity , dtype = np . int32 ) match self . capacity_approach : case \"evolved\" : capacities [:] = self . _rng . integers ( 1 , self . max_capacity , size = n ) case \"percentage\" : capacities [:] = ( np . sum ( weights_and_profits [:, 1 :: 2 ], axis = 1 ) * self . capacity_ratio ) . astype ( np . int32 ) return list ( Instance ( i ) for i in np . column_stack (( capacities , weights_and_profits )) ) def extract_features ( self , instances : Sequence [ Instance ] | np . ndarray ) -> np . ndarray : \"\"\"Extract the features of the instance based on the domain Args: instances (Sequence[Instance]): Instances to extract the features from. Returns: ArrayLike: 2d array with the features of each instance \"\"\" if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances , copy = True ) features = np . empty ( shape = ( len ( instances ), 8 ), dtype = np . float32 ) weights = instances [:, 1 :: 2 ] profits = instances [:, 2 :: 2 ] features [:, 0 ] = instances [:, 0 ] # Qs features [:, 1 ] = np . max ( profits , axis = 1 ) features [:, 2 ] = np . max ( weights , axis = 1 ) features [:, 3 ] = np . min ( profits , axis = 1 ) features [:, 4 ] = np . min ( weights , axis = 1 ) features [:, 5 ] = np . mean ( profits / weights ) features [:, 6 ] = np . mean ( instances [:, 1 :], axis = 1 ) features [:, 7 ] = np . std ( instances [:, 1 :], axis = 1 ) return features def extract_features_as_dict ( self , instances : Sequence [ Instance ] | np . ndarray ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Args: instances (Sequence[Instance]): Instances to extract the features from. They should in the an array form. Returns: Dict[str, float]: Dictionary with the names/values of each feature \"\"\" features = self . extract_features ( instances ) named_features : list [ dict [ str , np . float32 ]] = [{}] * len ( features ) for i , feats in enumerate ( features ): named_features [ i ] = { k : v for k , v in zip ( self . feat_names , feats )} return named_features def generate_problems_from_instances ( self , instances : Sequence [ Instance ] | np . ndarray ) -> List : \"\"\"Generates a List of Knapsack objects from the instances Args: instances (Sequence[Instance]): Instances to create the problems from Returns: List: List containing len(instances) objects of type Knapsack \"\"\" if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances ) capacities = instances [:, 0 ] . astype ( int ) weights = instances [:, 1 :: 2 ] . astype ( int ) profits = instances [:, 2 :: 2 ] . astype ( int ) # Sets the capacity according to the method if self . capacity_approach == \"percentage\" : capacities [:] = ( np . sum ( weights , axis = 1 ) * self . capacity_ratio ) . astype ( np . int32 ) elif self . capacity_approach == \"fixed\" : capacities [:] = self . max_capacity return list ( Knapsack ( profits = profits [ i ], weights = weights [ i ], capacity = capacities [ i ]) for i in range ( len ( instances )) )","title":"KnapsackDomain"},{"location":"reference/domains/#domains.KnapsackDomain.extract_features","text":"Extract the features of the instance based on the domain Parameters: instances ( Sequence [ Instance ] ) \u2013 Instances to extract the features from. Returns: ArrayLike ( ndarray ) \u2013 2d array with the features of each instance Source code in digneapy/domains/kp.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 def extract_features ( self , instances : Sequence [ Instance ] | np . ndarray ) -> np . ndarray : \"\"\"Extract the features of the instance based on the domain Args: instances (Sequence[Instance]): Instances to extract the features from. Returns: ArrayLike: 2d array with the features of each instance \"\"\" if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances , copy = True ) features = np . empty ( shape = ( len ( instances ), 8 ), dtype = np . float32 ) weights = instances [:, 1 :: 2 ] profits = instances [:, 2 :: 2 ] features [:, 0 ] = instances [:, 0 ] # Qs features [:, 1 ] = np . max ( profits , axis = 1 ) features [:, 2 ] = np . max ( weights , axis = 1 ) features [:, 3 ] = np . min ( profits , axis = 1 ) features [:, 4 ] = np . min ( weights , axis = 1 ) features [:, 5 ] = np . mean ( profits / weights ) features [:, 6 ] = np . mean ( instances [:, 1 :], axis = 1 ) features [:, 7 ] = np . std ( instances [:, 1 :], axis = 1 ) return features","title":"extract_features"},{"location":"reference/domains/#domains.KnapsackDomain.extract_features_as_dict","text":"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Parameters: instances ( Sequence [ Instance ] ) \u2013 Instances to extract the features from. They should in the an array form. Returns: List [ Dict [ str , float32 ]] \u2013 Dict[str, float]: Dictionary with the names/values of each feature Source code in digneapy/domains/kp.py 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def extract_features_as_dict ( self , instances : Sequence [ Instance ] | np . ndarray ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Args: instances (Sequence[Instance]): Instances to extract the features from. They should in the an array form. Returns: Dict[str, float]: Dictionary with the names/values of each feature \"\"\" features = self . extract_features ( instances ) named_features : list [ dict [ str , np . float32 ]] = [{}] * len ( features ) for i , feats in enumerate ( features ): named_features [ i ] = { k : v for k , v in zip ( self . feat_names , feats )} return named_features","title":"extract_features_as_dict"},{"location":"reference/domains/#domains.KnapsackDomain.generate_instances","text":"Generates N instances for the domain. Parameters: n ( int , default: 1 ) \u2013 Number of instances to generate. Defaults to 1. Returns: List [ Instance ] \u2013 List[Instance]: A list of Instance objects created from the raw numpy generation Source code in digneapy/domains/kp.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances for the domain. Args: n (int, optional): Number of instances to generate. Defaults to 1. Returns: List[Instance]: A list of Instance objects created from the raw numpy generation \"\"\" weights_and_profits = np . empty ( shape = ( n , self . dimension * 2 ), dtype = np . int32 ) weights_and_profits [:, 0 :: 2 ] = self . _rng . integers ( low = self . min_w , high = self . max_w , size = ( n , self . dimension ) ) weights_and_profits [:, 1 :: 2 ] = self . _rng . integers ( low = self . min_p , high = self . max_p , size = ( n , self . dimension ) ) # Assume fixed capacities = np . full ( n , fill_value = self . max_capacity , dtype = np . int32 ) match self . capacity_approach : case \"evolved\" : capacities [:] = self . _rng . integers ( 1 , self . max_capacity , size = n ) case \"percentage\" : capacities [:] = ( np . sum ( weights_and_profits [:, 1 :: 2 ], axis = 1 ) * self . capacity_ratio ) . astype ( np . int32 ) return list ( Instance ( i ) for i in np . column_stack (( capacities , weights_and_profits )) )","title":"generate_instances"},{"location":"reference/domains/#domains.KnapsackDomain.generate_problems_from_instances","text":"Generates a List of Knapsack objects from the instances Parameters: instances ( Sequence [ Instance ] ) \u2013 Instances to create the problems from Returns: List ( List ) \u2013 List containing len(instances) objects of type Knapsack Source code in digneapy/domains/kp.py 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 def generate_problems_from_instances ( self , instances : Sequence [ Instance ] | np . ndarray ) -> List : \"\"\"Generates a List of Knapsack objects from the instances Args: instances (Sequence[Instance]): Instances to create the problems from Returns: List: List containing len(instances) objects of type Knapsack \"\"\" if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances ) capacities = instances [:, 0 ] . astype ( int ) weights = instances [:, 1 :: 2 ] . astype ( int ) profits = instances [:, 2 :: 2 ] . astype ( int ) # Sets the capacity according to the method if self . capacity_approach == \"percentage\" : capacities [:] = ( np . sum ( weights , axis = 1 ) * self . capacity_ratio ) . astype ( np . int32 ) elif self . capacity_approach == \"fixed\" : capacities [:] = self . max_capacity return list ( Knapsack ( profits = profits [ i ], weights = weights [ i ], capacity = capacities [ i ]) for i in range ( len ( instances )) )","title":"generate_problems_from_instances"},{"location":"reference/domains/#domains.TSP","text":"Bases: Problem Symmetric Travelling Salesman Problem Source code in digneapy/domains/tsp.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 class TSP ( Problem ): \"\"\"Symmetric Travelling Salesman Problem\"\"\" def __init__ ( self , nodes : int , coords : np . ndarray , seed : int = 42 , * args , ** kwargs , ): \"\"\"Creates a new Symmetric Travelling Salesman Problem Args: nodes (int): Number of nodes/cities in the instance to solve coords (np.ndarray(N, 2)): Coordinates of each node/city. \"\"\" self . _nodes = nodes if coords . shape [ 1 ] != 2 : raise ValueError ( f \"Expected coordinates shape to be (N, 2). Instead coords has the following shape: { coords . shape } \" ) if not isinstance ( coords , np . ndarray ): coords = np . asarray ( coords ) self . _coords = coords x_min , y_min = np . min ( self . _coords , axis = 0 ) x_max , y_max = np . max ( self . _coords , axis = 0 ) bounds = list ((( x_min , y_min ), ( x_max , y_max )) for _ in range ( self . _nodes )) super () . __init__ ( dimension = nodes , bounds = bounds , name = \"TSP\" , seed = seed ) self . _distances = np . zeros (( self . _nodes , self . _nodes )) differences = self . _coords [:, np . newaxis , :] - self . _coords [ np . newaxis , :, :] self . _distances = np . sqrt ( np . sum ( differences ** 2 , axis =- 1 )) def __evaluate_constraints ( self , individual : Sequence | Solution ) -> bool : counter = Counter ( individual ) if any ( counter [ c ] != 1 for c in counter if c != 0 ) or ( individual [ 0 ] != 0 or individual [ - 1 ] != 0 ): return False return True def evaluate ( self , individual : Sequence | Solution ) -> tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Travelling Salesmas Problem. The fitness of the solution is the fraction of the sum of the distances of the tour Args: individual (Sequence | Solution): Individual to evaluate Returns: Tuple[float]: Fitness \"\"\" if len ( individual ) != self . _nodes + 1 : msg = f \"Mismatch between individual variables ( { len ( individual ) } ) and instance variables ( { self . _nodes } ) in { self . __class__ . __name__ } . A solution for the TSP must be a sequence of len { self . _nodes + 1 } \" raise ValueError ( msg ) penalty : np . float64 = np . float64 ( 0 ) if self . __evaluate_constraints ( individual ): distance : float = 0.0 for i in range ( len ( individual ) - 2 ): distance += self . _distances [ individual [ i ]][ individual [ i + 1 ]] fitness = 1.0 / distance else : fitness = 2.938736e-39 # --> 1.0 / np.float.max penalty = np . finfo ( np . float64 ) . max if isinstance ( individual , Solution ): individual . fitness = fitness individual . objectives = ( fitness ,) individual . constraints = ( penalty ,) return ( fitness ,) def __call__ ( self , individual : Sequence | Solution ) -> tuple [ float ]: return self . evaluate ( individual ) def __repr__ ( self ): return f \"TSP<n= { self . _nodes } >\" def __len__ ( self ): return self . _nodes def __array__ ( self , dtype = np . float32 , copy : Optional [ bool ] = True ) -> npt . ArrayLike : return np . asarray ( self . _coords , dtype = dtype , copy = copy ) def create_solution ( self ) -> Solution : items = [ 0 ] + list ( range ( 1 , self . _nodes )) + [ 0 ] return Solution ( variables = items ) def to_file ( self , filename : str = \"instance.tsp\" ): with open ( filename , \"w\" ) as file : file . write ( f \" { len ( self ) } \\n\\n \" ) content = \" \\n \" . join ( f \" { x } \\t { y } \" for ( x , y ) in self . _coords ) file . write ( content ) @classmethod def from_file ( cls , filename : str ) -> Self : # TODO: Improve using np.loadtxt with open ( filename ) as f : lines = f . readlines () lines = [ line . rstrip () for line in lines ] nodes = int ( lines [ 0 ]) coords = np . zeros ( shape = ( nodes , 2 ), dtype = np . float32 ) for i , line in enumerate ( lines [ 2 :]): x , y = line . split () coords [ i ] = [ np . float32 ( x ), np . float32 ( y )] return cls ( nodes = nodes , coords = coords ) def to_instance ( self ) -> Instance : return Instance ( variables = self . _coords . flatten ())","title":"TSP"},{"location":"reference/domains/#domains.TSP.__init__","text":"Creates a new Symmetric Travelling Salesman Problem Parameters: nodes ( int ) \u2013 Number of nodes/cities in the instance to solve coords ( ndarray ( N , 2) ) \u2013 Coordinates of each node/city. Source code in digneapy/domains/tsp.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , nodes : int , coords : np . ndarray , seed : int = 42 , * args , ** kwargs , ): \"\"\"Creates a new Symmetric Travelling Salesman Problem Args: nodes (int): Number of nodes/cities in the instance to solve coords (np.ndarray(N, 2)): Coordinates of each node/city. \"\"\" self . _nodes = nodes if coords . shape [ 1 ] != 2 : raise ValueError ( f \"Expected coordinates shape to be (N, 2). Instead coords has the following shape: { coords . shape } \" ) if not isinstance ( coords , np . ndarray ): coords = np . asarray ( coords ) self . _coords = coords x_min , y_min = np . min ( self . _coords , axis = 0 ) x_max , y_max = np . max ( self . _coords , axis = 0 ) bounds = list ((( x_min , y_min ), ( x_max , y_max )) for _ in range ( self . _nodes )) super () . __init__ ( dimension = nodes , bounds = bounds , name = \"TSP\" , seed = seed ) self . _distances = np . zeros (( self . _nodes , self . _nodes )) differences = self . _coords [:, np . newaxis , :] - self . _coords [ np . newaxis , :, :] self . _distances = np . sqrt ( np . sum ( differences ** 2 , axis =- 1 ))","title":"__init__"},{"location":"reference/domains/#domains.TSP.evaluate","text":"Evaluates the candidate individual with the information of the Travelling Salesmas Problem. The fitness of the solution is the fraction of the sum of the distances of the tour Args: individual (Sequence | Solution): Individual to evaluate Returns: tuple [ float ] \u2013 Tuple[float]: Fitness Source code in digneapy/domains/tsp.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def evaluate ( self , individual : Sequence | Solution ) -> tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Travelling Salesmas Problem. The fitness of the solution is the fraction of the sum of the distances of the tour Args: individual (Sequence | Solution): Individual to evaluate Returns: Tuple[float]: Fitness \"\"\" if len ( individual ) != self . _nodes + 1 : msg = f \"Mismatch between individual variables ( { len ( individual ) } ) and instance variables ( { self . _nodes } ) in { self . __class__ . __name__ } . A solution for the TSP must be a sequence of len { self . _nodes + 1 } \" raise ValueError ( msg ) penalty : np . float64 = np . float64 ( 0 ) if self . __evaluate_constraints ( individual ): distance : float = 0.0 for i in range ( len ( individual ) - 2 ): distance += self . _distances [ individual [ i ]][ individual [ i + 1 ]] fitness = 1.0 / distance else : fitness = 2.938736e-39 # --> 1.0 / np.float.max penalty = np . finfo ( np . float64 ) . max if isinstance ( individual , Solution ): individual . fitness = fitness individual . objectives = ( fitness ,) individual . constraints = ( penalty ,) return ( fitness ,)","title":"evaluate"},{"location":"reference/domains/#domains.TSPDomain","text":"Bases: Domain Domain to generate instances for the Symmetric Travelling Salesman Problem. Source code in digneapy/domains/tsp.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 class TSPDomain ( Domain ): \"\"\"Domain to generate instances for the Symmetric Travelling Salesman Problem.\"\"\" __FEAT_NAMES = \"size,std_distances,centroid_x,centroid_y,radius,fraction_distances,area,variance_nnNds,variation_nnNds,cluster_ratio,mean_cluster_radius\" . split ( \",\" ) def __init__ ( self , dimension : int = 100 , x_range : Tuple [ int , int ] = ( 0 , 1000 ), y_range : Tuple [ int , int ] = ( 0 , 1000 ), seed : int = 42 , ): \"\"\"Creates a new TSPDomain to generate instances for the Symmetric Travelling Salesman Problem Args: dimension (int, optional): Dimension of the instances to generate. Defaults to 100. x_range (Tuple[int, int], optional): Ranges for the Xs coordinates of each node/city. Defaults to (0, 1000). y_range (Tuple[int, int], optional): Ranges for the ys coordinates of each node/city. Defaults to (0, 1000). Raises: ValueError: If dimension is < 0 ValueError: If x_range OR y_range does not have 2 dimensions each ValueError: If minimum ranges are greater than maximum ranges \"\"\" if dimension < 0 : raise ValueError ( f \"Expected dimension > 0 got { dimension } \" ) if len ( x_range ) != 2 or len ( y_range ) != 2 : raise ValueError ( f \"Expected x_range and y_range to be a tuple with only to integers. Got: x_range = { x_range } and y_range = { y_range } \" ) x_min , x_max = x_range y_min , y_max = y_range if x_min < 0 or x_max <= x_min : raise ValueError ( f \"Expected x_range to be (x_min, x_max) where x_min >= 0 and x_max > x_min. Got: x_range { x_range } \" ) if y_min < 0 or y_max <= y_min : raise ValueError ( f \"Expected y_range to be (y_min, y_max) where y_min >= 0 and y_max > y_min. Got: y_range { y_range } \" ) self . _x_range = x_range self . _y_range = y_range __bounds = [ ( x_min , x_max ) if i % 2 == 0 else ( y_min , y_max ) for i in range ( dimension * 2 ) ] super () . __init__ ( dimension = dimension , bounds = __bounds , name = \"TSP\" , seed = seed ) def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances using numpy. It can return the instances in two formats: 1. A numpy ndarray with the definition of the instances 2. A list of Instance objects created from the raw numpy generation Args: n (int, optional): Number of instances to generate. Defaults to 1. cast (bool, optional): Whether to cast the raw data to Instance objects. Defaults to False. Returns: List[Instance]: Sequence of instances \"\"\" instances = np . empty ( shape = ( n , self . dimension * 2 ), dtype = np . float32 ) instances [:, 0 :: 2 ] = self . _rng . uniform ( low = self . _x_range [ 0 ], high = self . _x_range [ 1 ], size = ( n , ( self . dimension )), ) instances [:, 1 :: 2 ] = self . _rng . uniform ( low = self . _y_range [ 0 ], high = self . _y_range [ 1 ], size = ( n , ( self . dimension )), ) return list ( Instance ( coords ) for coords in instances ) def extract_features ( self , instances : Sequence [ Instance ]) -> np . ndarray : \"\"\"Extract the features of the instance based on the TSP domain. For the TSP the features are: - Size - Standard deviation of the distances - Centroid coordinates - Radius of the instance - Fraction of distinct distances - Rectangular area - Variance of the normalised nearest neighbours distances - Coefficient of variation of the nearest neighbours distances - Cluster ratio - Mean cluster radius Args: instance (Instance): Instance to extract the features from Returns: Tuple[float]: Values of each feature \"\"\" _instances = np . asarray ( instances , copy = True ) N_INSTANCES = len ( _instances ) N_CITIES = len ( _instances [ 0 ]) // 2 # self.dimension // 2 assert _instances is not instances coords = np . asarray ( _instances , copy = True ) . reshape (( N_INSTANCES , N_CITIES , 2 )) xs = coords [:, :, 0 ] ys = coords [:, :, 1 ] areas = ( ( np . max ( xs , axis = 1 ) - np . min ( xs , axis = 1 )) * ( np . max ( ys , axis = 1 ) - np . min ( ys , axis = 1 )) ) . astype ( np . float64 ) # Compute distances for all instances distances = np . zeros (( N_INSTANCES , N_CITIES , N_CITIES )) differences = coords [:, :, np . newaxis , :] - coords [:, np . newaxis , :, :] distances = np . sqrt ( np . sum ( differences ** 2 , axis =- 1 )) mask = ~ np . eye ( N_CITIES , dtype = bool ) std_distances = np . std ( distances [:, mask ], axis = 1 ) centroids = np . mean ( coords , axis = 1 ) expanded_centroids = centroids [:, np . newaxis , :] centroids_distances = np . linalg . norm ( coords - expanded_centroids , axis =- 1 ) radius = np . mean ( centroids_distances , axis = 1 ) fractions = np . array ( [ np . unique ( d [ np . triu_indices_from ( d , k = 1 )]) . size / ( N_CITIES * ( N_CITIES - 1 ) / 2 ) for d in distances ] ) # Top five only norm_distances = np . sort ( distances , axis = 2 )[:, :, :: - 1 ][:, :, : 5 ] / np . max ( distances , axis = ( 1 , 2 ), keepdims = True ) variance_nnds = np . var ( norm_distances , axis = ( 1 , 2 )) variation_nnds = variance_nnds / np . mean ( norm_distances , axis = ( 1 , 2 )) cluster_ratio = np . empty ( shape = N_INSTANCES , dtype = np . float64 ) mean_cluster_radius = np . empty ( shape = N_INSTANCES , dtype = np . float64 ) for i in range ( N_INSTANCES ): scale = np . mean ( np . std ( coords [ i ], axis = 0 )) dbscan = DBSCAN ( eps = 0.2 * scale , min_samples = 1 ) labels = dbscan . fit_predict ( coords [ i ]) unique_labels = [ label for label in set ( labels ) if label != - 1 ] cluster_ratio [ i ] = len ( unique_labels ) / N_CITIES # Cluster radius cluster_radius = np . empty ( shape = len ( unique_labels ), dtype = np . float64 ) for j , label_id in enumerate ( unique_labels ): points_in_cluster = coords [ i ][ labels == label_id ] cluster_centroid = np . mean ( points_in_cluster , axis = 0 ) cluster_radius [ j ] = np . mean ( np . linalg . norm ( points_in_cluster - cluster_centroid , axis = 1 ) ) mean_cluster_radius [ i ] = ( np . mean ( cluster_radius ) if cluster_radius . size > 0 else 0.0 ) return np . column_stack ( [ np . full ( shape = len ( _instances ), fill_value = N_CITIES ), std_distances , centroids [:, 0 ], centroids [:, 1 ], radius , fractions , areas , variance_nnds , variation_nnds , cluster_ratio , mean_cluster_radius , ] ) . astype ( np . float64 ) def extract_features_as_dict ( self , instances : Sequence [ Instance ] ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Args: instance (Instance): Instance to extract the features from Returns: Mapping[str, float]: Dictionary with the names/values of each feature \"\"\" features = self . extract_features ( instances ) named_features : list [ dict [ str , np . float32 ]] = [{}] * len ( features ) for i , feats in enumerate ( features ): named_features [ i ] = { k : v for k , v in zip ( TSPDomain . __FEAT_NAMES , feats )} return named_features def generate_problem_from_instance ( self , instance : Instance ) -> TSP : n_nodes = len ( instance ) // 2 coords = np . array ([ * zip ( instance [:: 2 ], instance [ 1 :: 2 ])]) return TSP ( nodes = n_nodes , coords = coords ) def generate_problems_from_instances ( self , instances : Sequence [ Instance ] ) -> List [ Problem ]: if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances ) dimension = instances . shape [ 1 ] // 2 return list ( TSP ( nodes = dimension , coords = np . array ([ * zip ( instance [ 0 :: 2 ], instance [ 1 :: 2 ])]) ) for instance in instances )","title":"TSPDomain"},{"location":"reference/domains/#domains.TSPDomain.__init__","text":"Creates a new TSPDomain to generate instances for the Symmetric Travelling Salesman Problem Parameters: dimension ( int , default: 100 ) \u2013 Dimension of the instances to generate. Defaults to 100. x_range ( Tuple [ int , int ] , default: (0, 1000) ) \u2013 Ranges for the Xs coordinates of each node/city. Defaults to (0, 1000). y_range ( Tuple [ int , int ] , default: (0, 1000) ) \u2013 Ranges for the ys coordinates of each node/city. Defaults to (0, 1000). Raises: ValueError \u2013 If dimension is < 0 ValueError \u2013 If x_range OR y_range does not have 2 dimensions each ValueError \u2013 If minimum ranges are greater than maximum ranges Source code in digneapy/domains/tsp.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def __init__ ( self , dimension : int = 100 , x_range : Tuple [ int , int ] = ( 0 , 1000 ), y_range : Tuple [ int , int ] = ( 0 , 1000 ), seed : int = 42 , ): \"\"\"Creates a new TSPDomain to generate instances for the Symmetric Travelling Salesman Problem Args: dimension (int, optional): Dimension of the instances to generate. Defaults to 100. x_range (Tuple[int, int], optional): Ranges for the Xs coordinates of each node/city. Defaults to (0, 1000). y_range (Tuple[int, int], optional): Ranges for the ys coordinates of each node/city. Defaults to (0, 1000). Raises: ValueError: If dimension is < 0 ValueError: If x_range OR y_range does not have 2 dimensions each ValueError: If minimum ranges are greater than maximum ranges \"\"\" if dimension < 0 : raise ValueError ( f \"Expected dimension > 0 got { dimension } \" ) if len ( x_range ) != 2 or len ( y_range ) != 2 : raise ValueError ( f \"Expected x_range and y_range to be a tuple with only to integers. Got: x_range = { x_range } and y_range = { y_range } \" ) x_min , x_max = x_range y_min , y_max = y_range if x_min < 0 or x_max <= x_min : raise ValueError ( f \"Expected x_range to be (x_min, x_max) where x_min >= 0 and x_max > x_min. Got: x_range { x_range } \" ) if y_min < 0 or y_max <= y_min : raise ValueError ( f \"Expected y_range to be (y_min, y_max) where y_min >= 0 and y_max > y_min. Got: y_range { y_range } \" ) self . _x_range = x_range self . _y_range = y_range __bounds = [ ( x_min , x_max ) if i % 2 == 0 else ( y_min , y_max ) for i in range ( dimension * 2 ) ] super () . __init__ ( dimension = dimension , bounds = __bounds , name = \"TSP\" , seed = seed )","title":"__init__"},{"location":"reference/domains/#domains.TSPDomain.extract_features","text":"Extract the features of the instance based on the TSP domain. For the TSP the features are: - Size - Standard deviation of the distances - Centroid coordinates - Radius of the instance - Fraction of distinct distances - Rectangular area - Variance of the normalised nearest neighbours distances - Coefficient of variation of the nearest neighbours distances - Cluster ratio - Mean cluster radius Args: instance (Instance): Instance to extract the features from Returns: ndarray \u2013 Tuple[float]: Values of each feature Source code in digneapy/domains/tsp.py 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 def extract_features ( self , instances : Sequence [ Instance ]) -> np . ndarray : \"\"\"Extract the features of the instance based on the TSP domain. For the TSP the features are: - Size - Standard deviation of the distances - Centroid coordinates - Radius of the instance - Fraction of distinct distances - Rectangular area - Variance of the normalised nearest neighbours distances - Coefficient of variation of the nearest neighbours distances - Cluster ratio - Mean cluster radius Args: instance (Instance): Instance to extract the features from Returns: Tuple[float]: Values of each feature \"\"\" _instances = np . asarray ( instances , copy = True ) N_INSTANCES = len ( _instances ) N_CITIES = len ( _instances [ 0 ]) // 2 # self.dimension // 2 assert _instances is not instances coords = np . asarray ( _instances , copy = True ) . reshape (( N_INSTANCES , N_CITIES , 2 )) xs = coords [:, :, 0 ] ys = coords [:, :, 1 ] areas = ( ( np . max ( xs , axis = 1 ) - np . min ( xs , axis = 1 )) * ( np . max ( ys , axis = 1 ) - np . min ( ys , axis = 1 )) ) . astype ( np . float64 ) # Compute distances for all instances distances = np . zeros (( N_INSTANCES , N_CITIES , N_CITIES )) differences = coords [:, :, np . newaxis , :] - coords [:, np . newaxis , :, :] distances = np . sqrt ( np . sum ( differences ** 2 , axis =- 1 )) mask = ~ np . eye ( N_CITIES , dtype = bool ) std_distances = np . std ( distances [:, mask ], axis = 1 ) centroids = np . mean ( coords , axis = 1 ) expanded_centroids = centroids [:, np . newaxis , :] centroids_distances = np . linalg . norm ( coords - expanded_centroids , axis =- 1 ) radius = np . mean ( centroids_distances , axis = 1 ) fractions = np . array ( [ np . unique ( d [ np . triu_indices_from ( d , k = 1 )]) . size / ( N_CITIES * ( N_CITIES - 1 ) / 2 ) for d in distances ] ) # Top five only norm_distances = np . sort ( distances , axis = 2 )[:, :, :: - 1 ][:, :, : 5 ] / np . max ( distances , axis = ( 1 , 2 ), keepdims = True ) variance_nnds = np . var ( norm_distances , axis = ( 1 , 2 )) variation_nnds = variance_nnds / np . mean ( norm_distances , axis = ( 1 , 2 )) cluster_ratio = np . empty ( shape = N_INSTANCES , dtype = np . float64 ) mean_cluster_radius = np . empty ( shape = N_INSTANCES , dtype = np . float64 ) for i in range ( N_INSTANCES ): scale = np . mean ( np . std ( coords [ i ], axis = 0 )) dbscan = DBSCAN ( eps = 0.2 * scale , min_samples = 1 ) labels = dbscan . fit_predict ( coords [ i ]) unique_labels = [ label for label in set ( labels ) if label != - 1 ] cluster_ratio [ i ] = len ( unique_labels ) / N_CITIES # Cluster radius cluster_radius = np . empty ( shape = len ( unique_labels ), dtype = np . float64 ) for j , label_id in enumerate ( unique_labels ): points_in_cluster = coords [ i ][ labels == label_id ] cluster_centroid = np . mean ( points_in_cluster , axis = 0 ) cluster_radius [ j ] = np . mean ( np . linalg . norm ( points_in_cluster - cluster_centroid , axis = 1 ) ) mean_cluster_radius [ i ] = ( np . mean ( cluster_radius ) if cluster_radius . size > 0 else 0.0 ) return np . column_stack ( [ np . full ( shape = len ( _instances ), fill_value = N_CITIES ), std_distances , centroids [:, 0 ], centroids [:, 1 ], radius , fractions , areas , variance_nnds , variation_nnds , cluster_ratio , mean_cluster_radius , ] ) . astype ( np . float64 )","title":"extract_features"},{"location":"reference/domains/#domains.TSPDomain.extract_features_as_dict","text":"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Parameters: instance ( Instance ) \u2013 Instance to extract the features from Returns: List [ Dict [ str , float32 ]] \u2013 Mapping[str, float]: Dictionary with the names/values of each feature Source code in digneapy/domains/tsp.py 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 def extract_features_as_dict ( self , instances : Sequence [ Instance ] ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Args: instance (Instance): Instance to extract the features from Returns: Mapping[str, float]: Dictionary with the names/values of each feature \"\"\" features = self . extract_features ( instances ) named_features : list [ dict [ str , np . float32 ]] = [{}] * len ( features ) for i , feats in enumerate ( features ): named_features [ i ] = { k : v for k , v in zip ( TSPDomain . __FEAT_NAMES , feats )} return named_features","title":"extract_features_as_dict"},{"location":"reference/domains/#domains.TSPDomain.generate_instances","text":"Generates N instances using numpy. It can return the instances in two formats: 1. A numpy ndarray with the definition of the instances 2. A list of Instance objects created from the raw numpy generation Parameters: n ( int , default: 1 ) \u2013 Number of instances to generate. Defaults to 1. cast ( bool ) \u2013 Whether to cast the raw data to Instance objects. Defaults to False. Returns: List [ Instance ] \u2013 List[Instance]: Sequence of instances Source code in digneapy/domains/tsp.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances using numpy. It can return the instances in two formats: 1. A numpy ndarray with the definition of the instances 2. A list of Instance objects created from the raw numpy generation Args: n (int, optional): Number of instances to generate. Defaults to 1. cast (bool, optional): Whether to cast the raw data to Instance objects. Defaults to False. Returns: List[Instance]: Sequence of instances \"\"\" instances = np . empty ( shape = ( n , self . dimension * 2 ), dtype = np . float32 ) instances [:, 0 :: 2 ] = self . _rng . uniform ( low = self . _x_range [ 0 ], high = self . _x_range [ 1 ], size = ( n , ( self . dimension )), ) instances [:, 1 :: 2 ] = self . _rng . uniform ( low = self . _y_range [ 0 ], high = self . _y_range [ 1 ], size = ( n , ( self . dimension )), ) return list ( Instance ( coords ) for coords in instances )","title":"generate_instances"},{"location":"reference/domains/bpp/","text":"@File : bin_packing.py @Time : 2024/06/18 09:15:05 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2024, Alejandro Marrero @Desc : None BPP Bases: Problem Source code in digneapy/domains/bpp.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 class BPP ( Problem ): def __init__ ( self , items : Iterable [ int ], capacity : int , seed : int = 42 , * args , ** kwargs , ): self . _items = tuple ( items ) self . _capacity = capacity dim = len ( self . _items ) assert len ( self . _items ) > 0 assert self . _capacity > 0 bounds = list (( 0 , dim - 1 ) for _ in range ( dim )) super () . __init__ ( dimension = dim , bounds = bounds , name = \"BPP\" , seed = seed ) def evaluate ( self , individual : Sequence | Solution ) -> tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Bin Packing. The fitness of the solution is the amount of unused space, as well as the number of bins for a specific solution. Falkenauer (1998) performance metric defined as: (x) = \\\\frac{\\\\sum_{k=1}^{N} \\\\left(\\\\frac{fill_k}{C}\\\\right)^2}{N} Args: individual (Sequence | Solution): Individual to evaluate Returns: Tuple[float]: Falkenauer Fitness \"\"\" if len ( individual ) != self . _dimension : msg = f \"Mismatch between individual variables ( { len ( individual ) } ) and instance variables ( { self . _dimension } ) in { self . __class__ . __name__ } \" raise ValueError ( msg ) used_bins = np . max ( individual ) . astype ( int ) + 1 fill_i = np . zeros ( used_bins ) for item_idx , bin in enumerate ( individual ): fill_i [ bin ] += self . _items [ item_idx ] fitness = ( sum ((( f_i / self . _capacity ) * ( f_i / self . _capacity )) for f_i in fill_i ) / used_bins ) if isinstance ( individual , Solution ): individual . fitness = fitness individual . objectives = ( fitness ,) return ( fitness ,) def __call__ ( self , individual : Sequence | Solution ) -> tuple [ float ]: return self . evaluate ( individual ) def __repr__ ( self ): return f \"BPP<n= { self . _dimension } ,C= { self . _capacity } ,I= { self . _items } >\" def __len__ ( self ): return self . _dimension def __array__ ( self , dtype = np . int32 , copy : Optional [ bool ] = False ) -> npt . ArrayLike : return np . asarray ([ self . _capacity , * self . _items ], dtype = dtype , copy = copy ) def create_solution ( self ) -> Solution : items = list ( range ( self . _dimension )) return Solution ( variables = items ) def to_file ( self , filename : str = \"instance.bpp\" ): with open ( filename , \"w\" ) as file : file . write ( f \" { len ( self ) } \\t { self . _capacity } \\n\\n \" ) content = \" \\n \" . join ( str ( i ) for i in self . _items ) file . write ( content ) @classmethod def from_file ( cls , filename : str ): with open ( filename ) as f : lines = f . readlines () lines = [ line . rstrip () for line in lines ] ( _ , capacity ) = lines [ 0 ] . split () items = list ( int ( i ) for i in lines [ 2 :]) return cls ( items = items , capacity = int ( capacity )) def to_instance ( self ) -> Instance : _vars = [ self . _capacity , * self . _items ] return Instance ( variables = _vars ) evaluate ( individual ) Evaluates the candidate individual with the information of the Bin Packing. The fitness of the solution is the amount of unused space, as well as the number of bins for a specific solution. Falkenauer (1998) performance metric defined as: (x) = \\frac{\\sum_{k=1}^{N} \\left(\\frac{fill_k}{C}\\right)^2}{N} Parameters: individual ( Sequence | Solution ) \u2013 Individual to evaluate Returns: tuple [ float ] \u2013 Tuple[float]: Falkenauer Fitness Source code in digneapy/domains/bpp.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def evaluate ( self , individual : Sequence | Solution ) -> tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Bin Packing. The fitness of the solution is the amount of unused space, as well as the number of bins for a specific solution. Falkenauer (1998) performance metric defined as: (x) = \\\\frac{\\\\sum_{k=1}^{N} \\\\left(\\\\frac{fill_k}{C}\\\\right)^2}{N} Args: individual (Sequence | Solution): Individual to evaluate Returns: Tuple[float]: Falkenauer Fitness \"\"\" if len ( individual ) != self . _dimension : msg = f \"Mismatch between individual variables ( { len ( individual ) } ) and instance variables ( { self . _dimension } ) in { self . __class__ . __name__ } \" raise ValueError ( msg ) used_bins = np . max ( individual ) . astype ( int ) + 1 fill_i = np . zeros ( used_bins ) for item_idx , bin in enumerate ( individual ): fill_i [ bin ] += self . _items [ item_idx ] fitness = ( sum ((( f_i / self . _capacity ) * ( f_i / self . _capacity )) for f_i in fill_i ) / used_bins ) if isinstance ( individual , Solution ): individual . fitness = fitness individual . objectives = ( fitness ,) return ( fitness ,) BPPDomain Bases: Domain Source code in digneapy/domains/bpp.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 class BPPDomain ( Domain ): __capacity_approaches = ( \"evolved\" , \"percentage\" , \"fixed\" ) __feat_names = names = \"mean,std,median,max,min,tiny,small,medium,large,huge\" . split ( \",\" ) def __init__ ( self , dimension : int = 50 , min_i : int = 1 , max_i : int = 1000 , capacity_approach : str = \"fixed\" , max_capacity : int = 100 , capacity_ratio : float = 0.8 , seed : int = 42 , ): if dimension < 0 : raise ValueError ( f \"Expected dimension > 0 got { dimension } \" ) if min_i < 0 : raise ValueError ( f \"Expected min_i > 0 got { min_i } \" ) if max_i < 0 : raise ValueError ( f \"Expected max_i > 0 got { max_i } \" ) if min_i > max_i : raise ValueError ( f \"Expected min_i to be less than max_i got ( { min_i } , { max_i } )\" ) self . _dimension = dimension self . _min_i = min_i self . _max_i = max_i self . _max_capacity = max_capacity if capacity_ratio < 0.0 or capacity_ratio > 1.0 or not float ( capacity_ratio ): self . capacity_ratio = 0.8 # Default msg = \"The capacity ratio must be a float number in the range [0.0-1.0]. Set as 0.8 as default.\" print ( msg ) else : self . capacity_ratio = capacity_ratio if capacity_approach not in self . __capacity_approaches : msg = f \"The capacity approach { capacity_approach } is not available. Please choose between { self . __capacity_approaches } . Evolved approach set as default.\" print ( msg ) self . _capacity_approach = \"fixed\" else : self . _capacity_approach = capacity_approach bounds = [( 1.0 , self . _max_capacity )] + [ ( self . _min_i , self . _max_i ) for _ in range ( self . _dimension ) ] super () . __init__ ( dimension = dimension , bounds = bounds , name = \"BPP\" , seed = seed ) @property def capacity_approach ( self ): return self . _capacity_approach @capacity_approach . setter def capacity_approach ( self , app ): \"\"\"Setter for the Maximum capacity generator approach. It forces to update the variable to one of the specify values Args: app (str): Approach for setting the capacity. It should be fixed, evolved or percentage. \"\"\" if app not in self . __capacity_approaches : msg = f \"The capacity approach { app } is not available. Please choose between { self . __capacity_approaches } . Evolved approach set as default.\" print ( msg ) self . _capacity_approach = \"fixed\" else : self . _capacity_approach = app def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances for the domain. Args: n (int, optional): Number of instances to generate. Defaults to 1. Returns: List[Instance]: A list of Instance objects created from the raw numpy generation \"\"\" instances = np . empty ( shape = ( n , self . dimension + 1 ), dtype = np . int32 ) instances = self . _rng . integers ( low = self . _min_i , high = self . _max_i , size = ( n , self . _dimension + 1 ), dtype = int ) # Sets the capacity according to the method match self . capacity_approach : case \"evolved\" : instances [:, 0 ] = self . _rng . integers ( 1 , self . _max_capacity , size = n ) case \"percentage\" : instances [:, 0 ] = ( np . sum ( instances [:, 1 :], axis = 1 , dtype = int ) * self . capacity_ratio ) case \"fixed\" : instances [:, 0 ] = self . _max_capacity return list ( Instance ( i ) for i in instances ) def extract_features ( self , instances : Sequence [ Instance ]) -> np . ndarray : \"\"\"Extract the features of the instance based on the BPP domain. For the BPP the features are: N, Capacity, MeanWeights, MedianWeights, VarianceWeights, MaxWeight, MinWeight, Huge, Large, Medium, Small, Tiny Args: instances (Instance): Instances to extract the features from Returns: np.ndarray: Values of each feature \"\"\" if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances ) norm_variables = np . asarray ( instances , copy = True ) norm_variables [:, 1 :] = norm_variables [:, 1 :] / norm_variables [:, [ 0 ]] return np . column_stack ( [ np . mean ( norm_variables , axis = 1 ), np . std ( norm_variables , axis = 1 ), np . median ( norm_variables , axis = 1 ), np . max ( norm_variables , axis = 1 ), np . min ( norm_variables , axis = 1 ), np . mean ( norm_variables > 0.5 , axis = 1 ), # Huge np . mean ( ( 0.5 >= norm_variables ) & ( norm_variables > 0.33333333333 ), axis = 1 ), np . mean ( ( 0.33333333333 >= norm_variables ) & ( norm_variables > 0.25 ), axis = 1 ), np . mean ( 0.25 >= norm_variables , axis = 1 ), # Small np . mean ( 0.1 >= norm_variables , axis = 1 ), # Tiny ], ) . astype ( np . float32 ) def extract_features_as_dict ( self , instances : Sequence [ Instance ] ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instances. The key are the names of each feature and the values are the values extracted from instance. Args: instances (Sequence[Instance]): Instances to extract the features from. Returns: Dict[str, np.float32]: Dictionary with the names/values of each feature \"\"\" features = self . extract_features ( instances ) named_features : list [ dict [ str , np . float32 ]] = [{}] * len ( features ) for i , feats in enumerate ( features ): named_features [ i ] = { k : v for k , v in zip ( BPPDomain . __feat_names , feats )} return named_features def generate_problems_from_instances ( self , instances : Sequence [ Instance ] ) -> List [ Problem ]: if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances ) # Assume evolved capacities capacities = instances [:, 0 ] . astype ( np . int32 ) match self . capacity_approach : case \"percentage\" : capacities [:] = ( np . sum ( instances [:, 1 :], axis = 1 ) * self . capacity_ratio ) . astype ( np . int32 ) case \"fixed\" : capacities [:] = self . _max_capacity return list ( BPP ( items = instances [ i , 1 :], capacity = capacities [ i ]) for i in range ( len ( instances )) ) extract_features ( instances ) Extract the features of the instance based on the BPP domain. For the BPP the features are: N, Capacity, MeanWeights, MedianWeights, VarianceWeights, MaxWeight, MinWeight, Huge, Large, Medium, Small, Tiny Parameters: instances ( Instance ) \u2013 Instances to extract the features from Returns: ndarray \u2013 np.ndarray: Values of each feature Source code in digneapy/domains/bpp.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def extract_features ( self , instances : Sequence [ Instance ]) -> np . ndarray : \"\"\"Extract the features of the instance based on the BPP domain. For the BPP the features are: N, Capacity, MeanWeights, MedianWeights, VarianceWeights, MaxWeight, MinWeight, Huge, Large, Medium, Small, Tiny Args: instances (Instance): Instances to extract the features from Returns: np.ndarray: Values of each feature \"\"\" if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances ) norm_variables = np . asarray ( instances , copy = True ) norm_variables [:, 1 :] = norm_variables [:, 1 :] / norm_variables [:, [ 0 ]] return np . column_stack ( [ np . mean ( norm_variables , axis = 1 ), np . std ( norm_variables , axis = 1 ), np . median ( norm_variables , axis = 1 ), np . max ( norm_variables , axis = 1 ), np . min ( norm_variables , axis = 1 ), np . mean ( norm_variables > 0.5 , axis = 1 ), # Huge np . mean ( ( 0.5 >= norm_variables ) & ( norm_variables > 0.33333333333 ), axis = 1 ), np . mean ( ( 0.33333333333 >= norm_variables ) & ( norm_variables > 0.25 ), axis = 1 ), np . mean ( 0.25 >= norm_variables , axis = 1 ), # Small np . mean ( 0.1 >= norm_variables , axis = 1 ), # Tiny ], ) . astype ( np . float32 ) extract_features_as_dict ( instances ) Creates a dictionary with the features of the instances. The key are the names of each feature and the values are the values extracted from instance. Parameters: instances ( Sequence [ Instance ] ) \u2013 Instances to extract the features from. Returns: Dict[str, np.float32]: Dictionary with the names/values of each feature Source code in digneapy/domains/bpp.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 def extract_features_as_dict ( self , instances : Sequence [ Instance ] ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instances. The key are the names of each feature and the values are the values extracted from instance. Args: instances (Sequence[Instance]): Instances to extract the features from. Returns: Dict[str, np.float32]: Dictionary with the names/values of each feature \"\"\" features = self . extract_features ( instances ) named_features : list [ dict [ str , np . float32 ]] = [{}] * len ( features ) for i , feats in enumerate ( features ): named_features [ i ] = { k : v for k , v in zip ( BPPDomain . __feat_names , feats )} return named_features generate_instances ( n = 1 ) Generates N instances for the domain. Parameters: n ( int , default: 1 ) \u2013 Number of instances to generate. Defaults to 1. Returns: List [ Instance ] \u2013 List[Instance]: A list of Instance objects created from the raw numpy generation Source code in digneapy/domains/bpp.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances for the domain. Args: n (int, optional): Number of instances to generate. Defaults to 1. Returns: List[Instance]: A list of Instance objects created from the raw numpy generation \"\"\" instances = np . empty ( shape = ( n , self . dimension + 1 ), dtype = np . int32 ) instances = self . _rng . integers ( low = self . _min_i , high = self . _max_i , size = ( n , self . _dimension + 1 ), dtype = int ) # Sets the capacity according to the method match self . capacity_approach : case \"evolved\" : instances [:, 0 ] = self . _rng . integers ( 1 , self . _max_capacity , size = n ) case \"percentage\" : instances [:, 0 ] = ( np . sum ( instances [:, 1 :], axis = 1 , dtype = int ) * self . capacity_ratio ) case \"fixed\" : instances [:, 0 ] = self . _max_capacity return list ( Instance ( i ) for i in instances )","title":"Bpp"},{"location":"reference/domains/bpp/#domains.bpp.BPP","text":"Bases: Problem Source code in digneapy/domains/bpp.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 class BPP ( Problem ): def __init__ ( self , items : Iterable [ int ], capacity : int , seed : int = 42 , * args , ** kwargs , ): self . _items = tuple ( items ) self . _capacity = capacity dim = len ( self . _items ) assert len ( self . _items ) > 0 assert self . _capacity > 0 bounds = list (( 0 , dim - 1 ) for _ in range ( dim )) super () . __init__ ( dimension = dim , bounds = bounds , name = \"BPP\" , seed = seed ) def evaluate ( self , individual : Sequence | Solution ) -> tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Bin Packing. The fitness of the solution is the amount of unused space, as well as the number of bins for a specific solution. Falkenauer (1998) performance metric defined as: (x) = \\\\frac{\\\\sum_{k=1}^{N} \\\\left(\\\\frac{fill_k}{C}\\\\right)^2}{N} Args: individual (Sequence | Solution): Individual to evaluate Returns: Tuple[float]: Falkenauer Fitness \"\"\" if len ( individual ) != self . _dimension : msg = f \"Mismatch between individual variables ( { len ( individual ) } ) and instance variables ( { self . _dimension } ) in { self . __class__ . __name__ } \" raise ValueError ( msg ) used_bins = np . max ( individual ) . astype ( int ) + 1 fill_i = np . zeros ( used_bins ) for item_idx , bin in enumerate ( individual ): fill_i [ bin ] += self . _items [ item_idx ] fitness = ( sum ((( f_i / self . _capacity ) * ( f_i / self . _capacity )) for f_i in fill_i ) / used_bins ) if isinstance ( individual , Solution ): individual . fitness = fitness individual . objectives = ( fitness ,) return ( fitness ,) def __call__ ( self , individual : Sequence | Solution ) -> tuple [ float ]: return self . evaluate ( individual ) def __repr__ ( self ): return f \"BPP<n= { self . _dimension } ,C= { self . _capacity } ,I= { self . _items } >\" def __len__ ( self ): return self . _dimension def __array__ ( self , dtype = np . int32 , copy : Optional [ bool ] = False ) -> npt . ArrayLike : return np . asarray ([ self . _capacity , * self . _items ], dtype = dtype , copy = copy ) def create_solution ( self ) -> Solution : items = list ( range ( self . _dimension )) return Solution ( variables = items ) def to_file ( self , filename : str = \"instance.bpp\" ): with open ( filename , \"w\" ) as file : file . write ( f \" { len ( self ) } \\t { self . _capacity } \\n\\n \" ) content = \" \\n \" . join ( str ( i ) for i in self . _items ) file . write ( content ) @classmethod def from_file ( cls , filename : str ): with open ( filename ) as f : lines = f . readlines () lines = [ line . rstrip () for line in lines ] ( _ , capacity ) = lines [ 0 ] . split () items = list ( int ( i ) for i in lines [ 2 :]) return cls ( items = items , capacity = int ( capacity )) def to_instance ( self ) -> Instance : _vars = [ self . _capacity , * self . _items ] return Instance ( variables = _vars )","title":"BPP"},{"location":"reference/domains/bpp/#domains.bpp.BPP.evaluate","text":"Evaluates the candidate individual with the information of the Bin Packing. The fitness of the solution is the amount of unused space, as well as the number of bins for a specific solution. Falkenauer (1998) performance metric defined as: (x) = \\frac{\\sum_{k=1}^{N} \\left(\\frac{fill_k}{C}\\right)^2}{N} Parameters: individual ( Sequence | Solution ) \u2013 Individual to evaluate Returns: tuple [ float ] \u2013 Tuple[float]: Falkenauer Fitness Source code in digneapy/domains/bpp.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def evaluate ( self , individual : Sequence | Solution ) -> tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Bin Packing. The fitness of the solution is the amount of unused space, as well as the number of bins for a specific solution. Falkenauer (1998) performance metric defined as: (x) = \\\\frac{\\\\sum_{k=1}^{N} \\\\left(\\\\frac{fill_k}{C}\\\\right)^2}{N} Args: individual (Sequence | Solution): Individual to evaluate Returns: Tuple[float]: Falkenauer Fitness \"\"\" if len ( individual ) != self . _dimension : msg = f \"Mismatch between individual variables ( { len ( individual ) } ) and instance variables ( { self . _dimension } ) in { self . __class__ . __name__ } \" raise ValueError ( msg ) used_bins = np . max ( individual ) . astype ( int ) + 1 fill_i = np . zeros ( used_bins ) for item_idx , bin in enumerate ( individual ): fill_i [ bin ] += self . _items [ item_idx ] fitness = ( sum ((( f_i / self . _capacity ) * ( f_i / self . _capacity )) for f_i in fill_i ) / used_bins ) if isinstance ( individual , Solution ): individual . fitness = fitness individual . objectives = ( fitness ,) return ( fitness ,)","title":"evaluate"},{"location":"reference/domains/bpp/#domains.bpp.BPPDomain","text":"Bases: Domain Source code in digneapy/domains/bpp.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 class BPPDomain ( Domain ): __capacity_approaches = ( \"evolved\" , \"percentage\" , \"fixed\" ) __feat_names = names = \"mean,std,median,max,min,tiny,small,medium,large,huge\" . split ( \",\" ) def __init__ ( self , dimension : int = 50 , min_i : int = 1 , max_i : int = 1000 , capacity_approach : str = \"fixed\" , max_capacity : int = 100 , capacity_ratio : float = 0.8 , seed : int = 42 , ): if dimension < 0 : raise ValueError ( f \"Expected dimension > 0 got { dimension } \" ) if min_i < 0 : raise ValueError ( f \"Expected min_i > 0 got { min_i } \" ) if max_i < 0 : raise ValueError ( f \"Expected max_i > 0 got { max_i } \" ) if min_i > max_i : raise ValueError ( f \"Expected min_i to be less than max_i got ( { min_i } , { max_i } )\" ) self . _dimension = dimension self . _min_i = min_i self . _max_i = max_i self . _max_capacity = max_capacity if capacity_ratio < 0.0 or capacity_ratio > 1.0 or not float ( capacity_ratio ): self . capacity_ratio = 0.8 # Default msg = \"The capacity ratio must be a float number in the range [0.0-1.0]. Set as 0.8 as default.\" print ( msg ) else : self . capacity_ratio = capacity_ratio if capacity_approach not in self . __capacity_approaches : msg = f \"The capacity approach { capacity_approach } is not available. Please choose between { self . __capacity_approaches } . Evolved approach set as default.\" print ( msg ) self . _capacity_approach = \"fixed\" else : self . _capacity_approach = capacity_approach bounds = [( 1.0 , self . _max_capacity )] + [ ( self . _min_i , self . _max_i ) for _ in range ( self . _dimension ) ] super () . __init__ ( dimension = dimension , bounds = bounds , name = \"BPP\" , seed = seed ) @property def capacity_approach ( self ): return self . _capacity_approach @capacity_approach . setter def capacity_approach ( self , app ): \"\"\"Setter for the Maximum capacity generator approach. It forces to update the variable to one of the specify values Args: app (str): Approach for setting the capacity. It should be fixed, evolved or percentage. \"\"\" if app not in self . __capacity_approaches : msg = f \"The capacity approach { app } is not available. Please choose between { self . __capacity_approaches } . Evolved approach set as default.\" print ( msg ) self . _capacity_approach = \"fixed\" else : self . _capacity_approach = app def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances for the domain. Args: n (int, optional): Number of instances to generate. Defaults to 1. Returns: List[Instance]: A list of Instance objects created from the raw numpy generation \"\"\" instances = np . empty ( shape = ( n , self . dimension + 1 ), dtype = np . int32 ) instances = self . _rng . integers ( low = self . _min_i , high = self . _max_i , size = ( n , self . _dimension + 1 ), dtype = int ) # Sets the capacity according to the method match self . capacity_approach : case \"evolved\" : instances [:, 0 ] = self . _rng . integers ( 1 , self . _max_capacity , size = n ) case \"percentage\" : instances [:, 0 ] = ( np . sum ( instances [:, 1 :], axis = 1 , dtype = int ) * self . capacity_ratio ) case \"fixed\" : instances [:, 0 ] = self . _max_capacity return list ( Instance ( i ) for i in instances ) def extract_features ( self , instances : Sequence [ Instance ]) -> np . ndarray : \"\"\"Extract the features of the instance based on the BPP domain. For the BPP the features are: N, Capacity, MeanWeights, MedianWeights, VarianceWeights, MaxWeight, MinWeight, Huge, Large, Medium, Small, Tiny Args: instances (Instance): Instances to extract the features from Returns: np.ndarray: Values of each feature \"\"\" if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances ) norm_variables = np . asarray ( instances , copy = True ) norm_variables [:, 1 :] = norm_variables [:, 1 :] / norm_variables [:, [ 0 ]] return np . column_stack ( [ np . mean ( norm_variables , axis = 1 ), np . std ( norm_variables , axis = 1 ), np . median ( norm_variables , axis = 1 ), np . max ( norm_variables , axis = 1 ), np . min ( norm_variables , axis = 1 ), np . mean ( norm_variables > 0.5 , axis = 1 ), # Huge np . mean ( ( 0.5 >= norm_variables ) & ( norm_variables > 0.33333333333 ), axis = 1 ), np . mean ( ( 0.33333333333 >= norm_variables ) & ( norm_variables > 0.25 ), axis = 1 ), np . mean ( 0.25 >= norm_variables , axis = 1 ), # Small np . mean ( 0.1 >= norm_variables , axis = 1 ), # Tiny ], ) . astype ( np . float32 ) def extract_features_as_dict ( self , instances : Sequence [ Instance ] ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instances. The key are the names of each feature and the values are the values extracted from instance. Args: instances (Sequence[Instance]): Instances to extract the features from. Returns: Dict[str, np.float32]: Dictionary with the names/values of each feature \"\"\" features = self . extract_features ( instances ) named_features : list [ dict [ str , np . float32 ]] = [{}] * len ( features ) for i , feats in enumerate ( features ): named_features [ i ] = { k : v for k , v in zip ( BPPDomain . __feat_names , feats )} return named_features def generate_problems_from_instances ( self , instances : Sequence [ Instance ] ) -> List [ Problem ]: if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances ) # Assume evolved capacities capacities = instances [:, 0 ] . astype ( np . int32 ) match self . capacity_approach : case \"percentage\" : capacities [:] = ( np . sum ( instances [:, 1 :], axis = 1 ) * self . capacity_ratio ) . astype ( np . int32 ) case \"fixed\" : capacities [:] = self . _max_capacity return list ( BPP ( items = instances [ i , 1 :], capacity = capacities [ i ]) for i in range ( len ( instances )) )","title":"BPPDomain"},{"location":"reference/domains/bpp/#domains.bpp.BPPDomain.extract_features","text":"Extract the features of the instance based on the BPP domain. For the BPP the features are: N, Capacity, MeanWeights, MedianWeights, VarianceWeights, MaxWeight, MinWeight, Huge, Large, Medium, Small, Tiny Parameters: instances ( Instance ) \u2013 Instances to extract the features from Returns: ndarray \u2013 np.ndarray: Values of each feature Source code in digneapy/domains/bpp.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def extract_features ( self , instances : Sequence [ Instance ]) -> np . ndarray : \"\"\"Extract the features of the instance based on the BPP domain. For the BPP the features are: N, Capacity, MeanWeights, MedianWeights, VarianceWeights, MaxWeight, MinWeight, Huge, Large, Medium, Small, Tiny Args: instances (Instance): Instances to extract the features from Returns: np.ndarray: Values of each feature \"\"\" if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances ) norm_variables = np . asarray ( instances , copy = True ) norm_variables [:, 1 :] = norm_variables [:, 1 :] / norm_variables [:, [ 0 ]] return np . column_stack ( [ np . mean ( norm_variables , axis = 1 ), np . std ( norm_variables , axis = 1 ), np . median ( norm_variables , axis = 1 ), np . max ( norm_variables , axis = 1 ), np . min ( norm_variables , axis = 1 ), np . mean ( norm_variables > 0.5 , axis = 1 ), # Huge np . mean ( ( 0.5 >= norm_variables ) & ( norm_variables > 0.33333333333 ), axis = 1 ), np . mean ( ( 0.33333333333 >= norm_variables ) & ( norm_variables > 0.25 ), axis = 1 ), np . mean ( 0.25 >= norm_variables , axis = 1 ), # Small np . mean ( 0.1 >= norm_variables , axis = 1 ), # Tiny ], ) . astype ( np . float32 )","title":"extract_features"},{"location":"reference/domains/bpp/#domains.bpp.BPPDomain.extract_features_as_dict","text":"Creates a dictionary with the features of the instances. The key are the names of each feature and the values are the values extracted from instance. Parameters: instances ( Sequence [ Instance ] ) \u2013 Instances to extract the features from. Returns: Dict[str, np.float32]: Dictionary with the names/values of each feature Source code in digneapy/domains/bpp.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 def extract_features_as_dict ( self , instances : Sequence [ Instance ] ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instances. The key are the names of each feature and the values are the values extracted from instance. Args: instances (Sequence[Instance]): Instances to extract the features from. Returns: Dict[str, np.float32]: Dictionary with the names/values of each feature \"\"\" features = self . extract_features ( instances ) named_features : list [ dict [ str , np . float32 ]] = [{}] * len ( features ) for i , feats in enumerate ( features ): named_features [ i ] = { k : v for k , v in zip ( BPPDomain . __feat_names , feats )} return named_features","title":"extract_features_as_dict"},{"location":"reference/domains/bpp/#domains.bpp.BPPDomain.generate_instances","text":"Generates N instances for the domain. Parameters: n ( int , default: 1 ) \u2013 Number of instances to generate. Defaults to 1. Returns: List [ Instance ] \u2013 List[Instance]: A list of Instance objects created from the raw numpy generation Source code in digneapy/domains/bpp.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances for the domain. Args: n (int, optional): Number of instances to generate. Defaults to 1. Returns: List[Instance]: A list of Instance objects created from the raw numpy generation \"\"\" instances = np . empty ( shape = ( n , self . dimension + 1 ), dtype = np . int32 ) instances = self . _rng . integers ( low = self . _min_i , high = self . _max_i , size = ( n , self . _dimension + 1 ), dtype = int ) # Sets the capacity according to the method match self . capacity_approach : case \"evolved\" : instances [:, 0 ] = self . _rng . integers ( 1 , self . _max_capacity , size = n ) case \"percentage\" : instances [:, 0 ] = ( np . sum ( instances [:, 1 :], axis = 1 , dtype = int ) * self . capacity_ratio ) case \"fixed\" : instances [:, 0 ] = self . _max_capacity return list ( Instance ( i ) for i in instances )","title":"generate_instances"},{"location":"reference/domains/kp/","text":"@File : knapsack.py @Time : 2023/10/30 12:18:44 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2023, Alejandro Marrero @Desc : None Knapsack Bases: Problem Source code in digneapy/domains/kp.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 class Knapsack ( Problem ): def __init__ ( self , profits : Sequence [ int ], weights : Sequence [ int ], capacity : int = 0 , seed : int = 42 , * args , ** kwargs , ): if len ( profits ) != len ( weights ): raise ValueError ( f \"The number of weights and profits is different in Knapsack. Got { len ( weights ) } weights and { len ( profits ) } profits\" ) if capacity <= 0 : raise ValueError ( f \"Capacity must be a positive integer. Got { capacity } \" ) super () . __init__ ( dimension = len ( profits ), bounds = [], name = \"KP\" , seed = seed ) self . weights = weights self . profits = profits self . capacity = capacity self . penalty_factor = 100.0 def get_bounds_at ( self , i : int ) -> tuple : if i < 0 or i > self . _dimension : raise ValueError ( f \"Index { i } out-of-range. The bounds are 0- { self . _dimension } \" ) return ( 0 , 1 ) @property def bounds ( self ): return list (( 0 , 1 ) for _ in range ( self . _dimension )) def evaluate ( self , individual : Sequence | Solution | np . ndarray ) -> Tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Knapsack Args: individual (Sequence | Solution): Individual to evaluate Raises: ValueError: Raises an error if the len(individual) != len(profits or weights) Returns: Tuple[float]: Profit \"\"\" if len ( individual ) != self . _dimension : msg = f \"Mismatch between individual variables and instance variables in { self . __class__ . __name__ } \" raise ValueError ( msg ) profit = np . dot ( individual , self . profits ) packed = np . dot ( individual , self . weights ) difference = max ( 0 , packed - self . capacity ) penalty = self . penalty_factor * difference profit -= penalty return ( profit ,) def __call__ ( self , individual : Sequence | Solution | np . ndarray ) -> Tuple [ float ]: return self . evaluate ( individual ) def __array__ ( self , dtype = np . int32 , copy : Optional [ bool ] = None ) -> npt . ArrayLike : \"\"\"Creates a numpy array from the Knapsack instance description. Returns: npt.ArrayLike: 1d numpy array of size 1 + (2 * dimension) \"\"\" return np . asarray ( [ self . capacity , * list ( itertools . chain . from_iterable ([ * zip ( self . weights , self . profits )]) ), ], dtype = dtype , copy = copy , ) def __repr__ ( self ): return f \"KP<n= { len ( self . profits ) } ,C= { self . capacity } >\" def __len__ ( self ): return len ( self . weights ) def create_solution ( self ) -> Solution : chromosome = self . _rng . integers ( low = 0 , high = 1 , size = self . _dimension ) return Solution ( variables = chromosome ) def to_file ( self , filename : str = \"instance.kp\" ): with open ( filename , \"w\" ) as file : file . write ( f \" { len ( self ) } \\t { self . capacity } \\n\\n \" ) content = \" \\n \" . join ( f \" { w_i } \\t { p_i } \" for w_i , p_i in zip ( self . weights , self . profits ) ) file . write ( content ) @classmethod def from_file ( cls , filename : str ): content = np . loadtxt ( filename , dtype = int ) capacity = content [ 0 ][ 1 ] weights , profits = content [ 1 :, 0 ], content [ 1 :, 1 ] return cls ( profits = profits , weights = weights , capacity = capacity ) def to_instance ( self ) -> Instance : _vars = [ self . capacity ] + list ( itertools . chain . from_iterable ([ * zip ( self . weights , self . profits )]) ) return Instance ( variables = _vars ) __array__ ( dtype = np . int32 , copy = None ) Creates a numpy array from the Knapsack instance description. Returns: ArrayLike \u2013 npt.ArrayLike: 1d numpy array of size 1 + (2 * dimension) Source code in digneapy/domains/kp.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def __array__ ( self , dtype = np . int32 , copy : Optional [ bool ] = None ) -> npt . ArrayLike : \"\"\"Creates a numpy array from the Knapsack instance description. Returns: npt.ArrayLike: 1d numpy array of size 1 + (2 * dimension) \"\"\" return np . asarray ( [ self . capacity , * list ( itertools . chain . from_iterable ([ * zip ( self . weights , self . profits )]) ), ], dtype = dtype , copy = copy , ) evaluate ( individual ) Evaluates the candidate individual with the information of the Knapsack Parameters: individual ( Sequence | Solution ) \u2013 Individual to evaluate Raises: ValueError \u2013 Raises an error if the len(individual) != len(profits or weights) Returns: Tuple [ float ] \u2013 Tuple[float]: Profit Source code in digneapy/domains/kp.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def evaluate ( self , individual : Sequence | Solution | np . ndarray ) -> Tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Knapsack Args: individual (Sequence | Solution): Individual to evaluate Raises: ValueError: Raises an error if the len(individual) != len(profits or weights) Returns: Tuple[float]: Profit \"\"\" if len ( individual ) != self . _dimension : msg = f \"Mismatch between individual variables and instance variables in { self . __class__ . __name__ } \" raise ValueError ( msg ) profit = np . dot ( individual , self . profits ) packed = np . dot ( individual , self . weights ) difference = max ( 0 , packed - self . capacity ) penalty = self . penalty_factor * difference profit -= penalty return ( profit ,) KnapsackDomain Bases: Domain Source code in digneapy/domains/kp.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 class KnapsackDomain ( Domain ): __capacity_approaches = ( \"evolved\" , \"percentage\" , \"fixed\" ) def __init__ ( self , dimension : int = 50 , min_p : int = 1 , min_w : int = 1 , max_p : int = 1_000 , max_w : int = 1_000 , capacity_approach : str = \"evolved\" , max_capacity : int = int ( 1e5 ), capacity_ratio : float = 0.8 , seed : Optional [ int ] = None , ): self . min_p = min_p self . min_w = min_w self . max_p = max_p self . max_w = max_w self . max_capacity = max_capacity if capacity_ratio < 0.0 or capacity_ratio > 1.0 or not float ( capacity_ratio ): self . capacity_ratio = 0.8 # Default msg = \"The capacity ratio must be a float number in the range [0.0-1.0]. Set as 0.8 as default.\" print ( msg ) else : self . capacity_ratio = capacity_ratio if capacity_approach not in self . __capacity_approaches : msg = f \"The capacity approach { capacity_approach } is not available. Please choose between { self . __capacity_approaches } . Evolved approach set as default.\" print ( msg ) self . _capacity_approach = \"evolved\" else : self . _capacity_approach = capacity_approach bounds = [( 1.0 , self . max_capacity )] + [ ( min_w , max_w ) if i % 2 == 0 else ( min_p , max_p ) for i in range ( 2 * dimension ) ] super () . __init__ ( dimension = dimension , bounds = bounds , name = \"KP\" , feat_names = \"capacity,max_p,max_w,min_p,min_w,avg_eff,mean,std\" . split ( \",\" ), seed = seed , ) @property def capacity_approach ( self ): return self . _capacity_approach @capacity_approach . setter def capacity_approach ( self , app ): \"\"\"Setter for the Maximum capacity generator approach. It forces to update the variable to one of the specify values Args: app (str): Approach for setting the capacity. It should be fixed, evolved or percentage. \"\"\" if app not in self . __capacity_approaches : msg = f \"The capacity approach { app } is not available. Please choose between { self . __capacity_approaches } . Evolved approach set as default.\" print ( msg ) self . _capacity_approach = \"evolved\" else : self . _capacity_approach = app def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances for the domain. Args: n (int, optional): Number of instances to generate. Defaults to 1. Returns: List[Instance]: A list of Instance objects created from the raw numpy generation \"\"\" weights_and_profits = np . empty ( shape = ( n , self . dimension * 2 ), dtype = np . int32 ) weights_and_profits [:, 0 :: 2 ] = self . _rng . integers ( low = self . min_w , high = self . max_w , size = ( n , self . dimension ) ) weights_and_profits [:, 1 :: 2 ] = self . _rng . integers ( low = self . min_p , high = self . max_p , size = ( n , self . dimension ) ) # Assume fixed capacities = np . full ( n , fill_value = self . max_capacity , dtype = np . int32 ) match self . capacity_approach : case \"evolved\" : capacities [:] = self . _rng . integers ( 1 , self . max_capacity , size = n ) case \"percentage\" : capacities [:] = ( np . sum ( weights_and_profits [:, 1 :: 2 ], axis = 1 ) * self . capacity_ratio ) . astype ( np . int32 ) return list ( Instance ( i ) for i in np . column_stack (( capacities , weights_and_profits )) ) def extract_features ( self , instances : Sequence [ Instance ] | np . ndarray ) -> np . ndarray : \"\"\"Extract the features of the instance based on the domain Args: instances (Sequence[Instance]): Instances to extract the features from. Returns: ArrayLike: 2d array with the features of each instance \"\"\" if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances , copy = True ) features = np . empty ( shape = ( len ( instances ), 8 ), dtype = np . float32 ) weights = instances [:, 1 :: 2 ] profits = instances [:, 2 :: 2 ] features [:, 0 ] = instances [:, 0 ] # Qs features [:, 1 ] = np . max ( profits , axis = 1 ) features [:, 2 ] = np . max ( weights , axis = 1 ) features [:, 3 ] = np . min ( profits , axis = 1 ) features [:, 4 ] = np . min ( weights , axis = 1 ) features [:, 5 ] = np . mean ( profits / weights ) features [:, 6 ] = np . mean ( instances [:, 1 :], axis = 1 ) features [:, 7 ] = np . std ( instances [:, 1 :], axis = 1 ) return features def extract_features_as_dict ( self , instances : Sequence [ Instance ] | np . ndarray ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Args: instances (Sequence[Instance]): Instances to extract the features from. They should in the an array form. Returns: Dict[str, float]: Dictionary with the names/values of each feature \"\"\" features = self . extract_features ( instances ) named_features : list [ dict [ str , np . float32 ]] = [{}] * len ( features ) for i , feats in enumerate ( features ): named_features [ i ] = { k : v for k , v in zip ( self . feat_names , feats )} return named_features def generate_problems_from_instances ( self , instances : Sequence [ Instance ] | np . ndarray ) -> List : \"\"\"Generates a List of Knapsack objects from the instances Args: instances (Sequence[Instance]): Instances to create the problems from Returns: List: List containing len(instances) objects of type Knapsack \"\"\" if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances ) capacities = instances [:, 0 ] . astype ( int ) weights = instances [:, 1 :: 2 ] . astype ( int ) profits = instances [:, 2 :: 2 ] . astype ( int ) # Sets the capacity according to the method if self . capacity_approach == \"percentage\" : capacities [:] = ( np . sum ( weights , axis = 1 ) * self . capacity_ratio ) . astype ( np . int32 ) elif self . capacity_approach == \"fixed\" : capacities [:] = self . max_capacity return list ( Knapsack ( profits = profits [ i ], weights = weights [ i ], capacity = capacities [ i ]) for i in range ( len ( instances )) ) extract_features ( instances ) Extract the features of the instance based on the domain Parameters: instances ( Sequence [ Instance ] ) \u2013 Instances to extract the features from. Returns: ArrayLike ( ndarray ) \u2013 2d array with the features of each instance Source code in digneapy/domains/kp.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 def extract_features ( self , instances : Sequence [ Instance ] | np . ndarray ) -> np . ndarray : \"\"\"Extract the features of the instance based on the domain Args: instances (Sequence[Instance]): Instances to extract the features from. Returns: ArrayLike: 2d array with the features of each instance \"\"\" if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances , copy = True ) features = np . empty ( shape = ( len ( instances ), 8 ), dtype = np . float32 ) weights = instances [:, 1 :: 2 ] profits = instances [:, 2 :: 2 ] features [:, 0 ] = instances [:, 0 ] # Qs features [:, 1 ] = np . max ( profits , axis = 1 ) features [:, 2 ] = np . max ( weights , axis = 1 ) features [:, 3 ] = np . min ( profits , axis = 1 ) features [:, 4 ] = np . min ( weights , axis = 1 ) features [:, 5 ] = np . mean ( profits / weights ) features [:, 6 ] = np . mean ( instances [:, 1 :], axis = 1 ) features [:, 7 ] = np . std ( instances [:, 1 :], axis = 1 ) return features extract_features_as_dict ( instances ) Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Parameters: instances ( Sequence [ Instance ] ) \u2013 Instances to extract the features from. They should in the an array form. Returns: List [ Dict [ str , float32 ]] \u2013 Dict[str, float]: Dictionary with the names/values of each feature Source code in digneapy/domains/kp.py 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def extract_features_as_dict ( self , instances : Sequence [ Instance ] | np . ndarray ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Args: instances (Sequence[Instance]): Instances to extract the features from. They should in the an array form. Returns: Dict[str, float]: Dictionary with the names/values of each feature \"\"\" features = self . extract_features ( instances ) named_features : list [ dict [ str , np . float32 ]] = [{}] * len ( features ) for i , feats in enumerate ( features ): named_features [ i ] = { k : v for k , v in zip ( self . feat_names , feats )} return named_features generate_instances ( n = 1 ) Generates N instances for the domain. Parameters: n ( int , default: 1 ) \u2013 Number of instances to generate. Defaults to 1. Returns: List [ Instance ] \u2013 List[Instance]: A list of Instance objects created from the raw numpy generation Source code in digneapy/domains/kp.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances for the domain. Args: n (int, optional): Number of instances to generate. Defaults to 1. Returns: List[Instance]: A list of Instance objects created from the raw numpy generation \"\"\" weights_and_profits = np . empty ( shape = ( n , self . dimension * 2 ), dtype = np . int32 ) weights_and_profits [:, 0 :: 2 ] = self . _rng . integers ( low = self . min_w , high = self . max_w , size = ( n , self . dimension ) ) weights_and_profits [:, 1 :: 2 ] = self . _rng . integers ( low = self . min_p , high = self . max_p , size = ( n , self . dimension ) ) # Assume fixed capacities = np . full ( n , fill_value = self . max_capacity , dtype = np . int32 ) match self . capacity_approach : case \"evolved\" : capacities [:] = self . _rng . integers ( 1 , self . max_capacity , size = n ) case \"percentage\" : capacities [:] = ( np . sum ( weights_and_profits [:, 1 :: 2 ], axis = 1 ) * self . capacity_ratio ) . astype ( np . int32 ) return list ( Instance ( i ) for i in np . column_stack (( capacities , weights_and_profits )) ) generate_problems_from_instances ( instances ) Generates a List of Knapsack objects from the instances Parameters: instances ( Sequence [ Instance ] ) \u2013 Instances to create the problems from Returns: List ( List ) \u2013 List containing len(instances) objects of type Knapsack Source code in digneapy/domains/kp.py 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 def generate_problems_from_instances ( self , instances : Sequence [ Instance ] | np . ndarray ) -> List : \"\"\"Generates a List of Knapsack objects from the instances Args: instances (Sequence[Instance]): Instances to create the problems from Returns: List: List containing len(instances) objects of type Knapsack \"\"\" if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances ) capacities = instances [:, 0 ] . astype ( int ) weights = instances [:, 1 :: 2 ] . astype ( int ) profits = instances [:, 2 :: 2 ] . astype ( int ) # Sets the capacity according to the method if self . capacity_approach == \"percentage\" : capacities [:] = ( np . sum ( weights , axis = 1 ) * self . capacity_ratio ) . astype ( np . int32 ) elif self . capacity_approach == \"fixed\" : capacities [:] = self . max_capacity return list ( Knapsack ( profits = profits [ i ], weights = weights [ i ], capacity = capacities [ i ]) for i in range ( len ( instances )) )","title":"Kp"},{"location":"reference/domains/kp/#domains.kp.Knapsack","text":"Bases: Problem Source code in digneapy/domains/kp.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 class Knapsack ( Problem ): def __init__ ( self , profits : Sequence [ int ], weights : Sequence [ int ], capacity : int = 0 , seed : int = 42 , * args , ** kwargs , ): if len ( profits ) != len ( weights ): raise ValueError ( f \"The number of weights and profits is different in Knapsack. Got { len ( weights ) } weights and { len ( profits ) } profits\" ) if capacity <= 0 : raise ValueError ( f \"Capacity must be a positive integer. Got { capacity } \" ) super () . __init__ ( dimension = len ( profits ), bounds = [], name = \"KP\" , seed = seed ) self . weights = weights self . profits = profits self . capacity = capacity self . penalty_factor = 100.0 def get_bounds_at ( self , i : int ) -> tuple : if i < 0 or i > self . _dimension : raise ValueError ( f \"Index { i } out-of-range. The bounds are 0- { self . _dimension } \" ) return ( 0 , 1 ) @property def bounds ( self ): return list (( 0 , 1 ) for _ in range ( self . _dimension )) def evaluate ( self , individual : Sequence | Solution | np . ndarray ) -> Tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Knapsack Args: individual (Sequence | Solution): Individual to evaluate Raises: ValueError: Raises an error if the len(individual) != len(profits or weights) Returns: Tuple[float]: Profit \"\"\" if len ( individual ) != self . _dimension : msg = f \"Mismatch between individual variables and instance variables in { self . __class__ . __name__ } \" raise ValueError ( msg ) profit = np . dot ( individual , self . profits ) packed = np . dot ( individual , self . weights ) difference = max ( 0 , packed - self . capacity ) penalty = self . penalty_factor * difference profit -= penalty return ( profit ,) def __call__ ( self , individual : Sequence | Solution | np . ndarray ) -> Tuple [ float ]: return self . evaluate ( individual ) def __array__ ( self , dtype = np . int32 , copy : Optional [ bool ] = None ) -> npt . ArrayLike : \"\"\"Creates a numpy array from the Knapsack instance description. Returns: npt.ArrayLike: 1d numpy array of size 1 + (2 * dimension) \"\"\" return np . asarray ( [ self . capacity , * list ( itertools . chain . from_iterable ([ * zip ( self . weights , self . profits )]) ), ], dtype = dtype , copy = copy , ) def __repr__ ( self ): return f \"KP<n= { len ( self . profits ) } ,C= { self . capacity } >\" def __len__ ( self ): return len ( self . weights ) def create_solution ( self ) -> Solution : chromosome = self . _rng . integers ( low = 0 , high = 1 , size = self . _dimension ) return Solution ( variables = chromosome ) def to_file ( self , filename : str = \"instance.kp\" ): with open ( filename , \"w\" ) as file : file . write ( f \" { len ( self ) } \\t { self . capacity } \\n\\n \" ) content = \" \\n \" . join ( f \" { w_i } \\t { p_i } \" for w_i , p_i in zip ( self . weights , self . profits ) ) file . write ( content ) @classmethod def from_file ( cls , filename : str ): content = np . loadtxt ( filename , dtype = int ) capacity = content [ 0 ][ 1 ] weights , profits = content [ 1 :, 0 ], content [ 1 :, 1 ] return cls ( profits = profits , weights = weights , capacity = capacity ) def to_instance ( self ) -> Instance : _vars = [ self . capacity ] + list ( itertools . chain . from_iterable ([ * zip ( self . weights , self . profits )]) ) return Instance ( variables = _vars )","title":"Knapsack"},{"location":"reference/domains/kp/#domains.kp.Knapsack.__array__","text":"Creates a numpy array from the Knapsack instance description. Returns: ArrayLike \u2013 npt.ArrayLike: 1d numpy array of size 1 + (2 * dimension) Source code in digneapy/domains/kp.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def __array__ ( self , dtype = np . int32 , copy : Optional [ bool ] = None ) -> npt . ArrayLike : \"\"\"Creates a numpy array from the Knapsack instance description. Returns: npt.ArrayLike: 1d numpy array of size 1 + (2 * dimension) \"\"\" return np . asarray ( [ self . capacity , * list ( itertools . chain . from_iterable ([ * zip ( self . weights , self . profits )]) ), ], dtype = dtype , copy = copy , )","title":"__array__"},{"location":"reference/domains/kp/#domains.kp.Knapsack.evaluate","text":"Evaluates the candidate individual with the information of the Knapsack Parameters: individual ( Sequence | Solution ) \u2013 Individual to evaluate Raises: ValueError \u2013 Raises an error if the len(individual) != len(profits or weights) Returns: Tuple [ float ] \u2013 Tuple[float]: Profit Source code in digneapy/domains/kp.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def evaluate ( self , individual : Sequence | Solution | np . ndarray ) -> Tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Knapsack Args: individual (Sequence | Solution): Individual to evaluate Raises: ValueError: Raises an error if the len(individual) != len(profits or weights) Returns: Tuple[float]: Profit \"\"\" if len ( individual ) != self . _dimension : msg = f \"Mismatch between individual variables and instance variables in { self . __class__ . __name__ } \" raise ValueError ( msg ) profit = np . dot ( individual , self . profits ) packed = np . dot ( individual , self . weights ) difference = max ( 0 , packed - self . capacity ) penalty = self . penalty_factor * difference profit -= penalty return ( profit ,)","title":"evaluate"},{"location":"reference/domains/kp/#domains.kp.KnapsackDomain","text":"Bases: Domain Source code in digneapy/domains/kp.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 class KnapsackDomain ( Domain ): __capacity_approaches = ( \"evolved\" , \"percentage\" , \"fixed\" ) def __init__ ( self , dimension : int = 50 , min_p : int = 1 , min_w : int = 1 , max_p : int = 1_000 , max_w : int = 1_000 , capacity_approach : str = \"evolved\" , max_capacity : int = int ( 1e5 ), capacity_ratio : float = 0.8 , seed : Optional [ int ] = None , ): self . min_p = min_p self . min_w = min_w self . max_p = max_p self . max_w = max_w self . max_capacity = max_capacity if capacity_ratio < 0.0 or capacity_ratio > 1.0 or not float ( capacity_ratio ): self . capacity_ratio = 0.8 # Default msg = \"The capacity ratio must be a float number in the range [0.0-1.0]. Set as 0.8 as default.\" print ( msg ) else : self . capacity_ratio = capacity_ratio if capacity_approach not in self . __capacity_approaches : msg = f \"The capacity approach { capacity_approach } is not available. Please choose between { self . __capacity_approaches } . Evolved approach set as default.\" print ( msg ) self . _capacity_approach = \"evolved\" else : self . _capacity_approach = capacity_approach bounds = [( 1.0 , self . max_capacity )] + [ ( min_w , max_w ) if i % 2 == 0 else ( min_p , max_p ) for i in range ( 2 * dimension ) ] super () . __init__ ( dimension = dimension , bounds = bounds , name = \"KP\" , feat_names = \"capacity,max_p,max_w,min_p,min_w,avg_eff,mean,std\" . split ( \",\" ), seed = seed , ) @property def capacity_approach ( self ): return self . _capacity_approach @capacity_approach . setter def capacity_approach ( self , app ): \"\"\"Setter for the Maximum capacity generator approach. It forces to update the variable to one of the specify values Args: app (str): Approach for setting the capacity. It should be fixed, evolved or percentage. \"\"\" if app not in self . __capacity_approaches : msg = f \"The capacity approach { app } is not available. Please choose between { self . __capacity_approaches } . Evolved approach set as default.\" print ( msg ) self . _capacity_approach = \"evolved\" else : self . _capacity_approach = app def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances for the domain. Args: n (int, optional): Number of instances to generate. Defaults to 1. Returns: List[Instance]: A list of Instance objects created from the raw numpy generation \"\"\" weights_and_profits = np . empty ( shape = ( n , self . dimension * 2 ), dtype = np . int32 ) weights_and_profits [:, 0 :: 2 ] = self . _rng . integers ( low = self . min_w , high = self . max_w , size = ( n , self . dimension ) ) weights_and_profits [:, 1 :: 2 ] = self . _rng . integers ( low = self . min_p , high = self . max_p , size = ( n , self . dimension ) ) # Assume fixed capacities = np . full ( n , fill_value = self . max_capacity , dtype = np . int32 ) match self . capacity_approach : case \"evolved\" : capacities [:] = self . _rng . integers ( 1 , self . max_capacity , size = n ) case \"percentage\" : capacities [:] = ( np . sum ( weights_and_profits [:, 1 :: 2 ], axis = 1 ) * self . capacity_ratio ) . astype ( np . int32 ) return list ( Instance ( i ) for i in np . column_stack (( capacities , weights_and_profits )) ) def extract_features ( self , instances : Sequence [ Instance ] | np . ndarray ) -> np . ndarray : \"\"\"Extract the features of the instance based on the domain Args: instances (Sequence[Instance]): Instances to extract the features from. Returns: ArrayLike: 2d array with the features of each instance \"\"\" if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances , copy = True ) features = np . empty ( shape = ( len ( instances ), 8 ), dtype = np . float32 ) weights = instances [:, 1 :: 2 ] profits = instances [:, 2 :: 2 ] features [:, 0 ] = instances [:, 0 ] # Qs features [:, 1 ] = np . max ( profits , axis = 1 ) features [:, 2 ] = np . max ( weights , axis = 1 ) features [:, 3 ] = np . min ( profits , axis = 1 ) features [:, 4 ] = np . min ( weights , axis = 1 ) features [:, 5 ] = np . mean ( profits / weights ) features [:, 6 ] = np . mean ( instances [:, 1 :], axis = 1 ) features [:, 7 ] = np . std ( instances [:, 1 :], axis = 1 ) return features def extract_features_as_dict ( self , instances : Sequence [ Instance ] | np . ndarray ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Args: instances (Sequence[Instance]): Instances to extract the features from. They should in the an array form. Returns: Dict[str, float]: Dictionary with the names/values of each feature \"\"\" features = self . extract_features ( instances ) named_features : list [ dict [ str , np . float32 ]] = [{}] * len ( features ) for i , feats in enumerate ( features ): named_features [ i ] = { k : v for k , v in zip ( self . feat_names , feats )} return named_features def generate_problems_from_instances ( self , instances : Sequence [ Instance ] | np . ndarray ) -> List : \"\"\"Generates a List of Knapsack objects from the instances Args: instances (Sequence[Instance]): Instances to create the problems from Returns: List: List containing len(instances) objects of type Knapsack \"\"\" if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances ) capacities = instances [:, 0 ] . astype ( int ) weights = instances [:, 1 :: 2 ] . astype ( int ) profits = instances [:, 2 :: 2 ] . astype ( int ) # Sets the capacity according to the method if self . capacity_approach == \"percentage\" : capacities [:] = ( np . sum ( weights , axis = 1 ) * self . capacity_ratio ) . astype ( np . int32 ) elif self . capacity_approach == \"fixed\" : capacities [:] = self . max_capacity return list ( Knapsack ( profits = profits [ i ], weights = weights [ i ], capacity = capacities [ i ]) for i in range ( len ( instances )) )","title":"KnapsackDomain"},{"location":"reference/domains/kp/#domains.kp.KnapsackDomain.extract_features","text":"Extract the features of the instance based on the domain Parameters: instances ( Sequence [ Instance ] ) \u2013 Instances to extract the features from. Returns: ArrayLike ( ndarray ) \u2013 2d array with the features of each instance Source code in digneapy/domains/kp.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 def extract_features ( self , instances : Sequence [ Instance ] | np . ndarray ) -> np . ndarray : \"\"\"Extract the features of the instance based on the domain Args: instances (Sequence[Instance]): Instances to extract the features from. Returns: ArrayLike: 2d array with the features of each instance \"\"\" if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances , copy = True ) features = np . empty ( shape = ( len ( instances ), 8 ), dtype = np . float32 ) weights = instances [:, 1 :: 2 ] profits = instances [:, 2 :: 2 ] features [:, 0 ] = instances [:, 0 ] # Qs features [:, 1 ] = np . max ( profits , axis = 1 ) features [:, 2 ] = np . max ( weights , axis = 1 ) features [:, 3 ] = np . min ( profits , axis = 1 ) features [:, 4 ] = np . min ( weights , axis = 1 ) features [:, 5 ] = np . mean ( profits / weights ) features [:, 6 ] = np . mean ( instances [:, 1 :], axis = 1 ) features [:, 7 ] = np . std ( instances [:, 1 :], axis = 1 ) return features","title":"extract_features"},{"location":"reference/domains/kp/#domains.kp.KnapsackDomain.extract_features_as_dict","text":"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Parameters: instances ( Sequence [ Instance ] ) \u2013 Instances to extract the features from. They should in the an array form. Returns: List [ Dict [ str , float32 ]] \u2013 Dict[str, float]: Dictionary with the names/values of each feature Source code in digneapy/domains/kp.py 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def extract_features_as_dict ( self , instances : Sequence [ Instance ] | np . ndarray ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Args: instances (Sequence[Instance]): Instances to extract the features from. They should in the an array form. Returns: Dict[str, float]: Dictionary with the names/values of each feature \"\"\" features = self . extract_features ( instances ) named_features : list [ dict [ str , np . float32 ]] = [{}] * len ( features ) for i , feats in enumerate ( features ): named_features [ i ] = { k : v for k , v in zip ( self . feat_names , feats )} return named_features","title":"extract_features_as_dict"},{"location":"reference/domains/kp/#domains.kp.KnapsackDomain.generate_instances","text":"Generates N instances for the domain. Parameters: n ( int , default: 1 ) \u2013 Number of instances to generate. Defaults to 1. Returns: List [ Instance ] \u2013 List[Instance]: A list of Instance objects created from the raw numpy generation Source code in digneapy/domains/kp.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances for the domain. Args: n (int, optional): Number of instances to generate. Defaults to 1. Returns: List[Instance]: A list of Instance objects created from the raw numpy generation \"\"\" weights_and_profits = np . empty ( shape = ( n , self . dimension * 2 ), dtype = np . int32 ) weights_and_profits [:, 0 :: 2 ] = self . _rng . integers ( low = self . min_w , high = self . max_w , size = ( n , self . dimension ) ) weights_and_profits [:, 1 :: 2 ] = self . _rng . integers ( low = self . min_p , high = self . max_p , size = ( n , self . dimension ) ) # Assume fixed capacities = np . full ( n , fill_value = self . max_capacity , dtype = np . int32 ) match self . capacity_approach : case \"evolved\" : capacities [:] = self . _rng . integers ( 1 , self . max_capacity , size = n ) case \"percentage\" : capacities [:] = ( np . sum ( weights_and_profits [:, 1 :: 2 ], axis = 1 ) * self . capacity_ratio ) . astype ( np . int32 ) return list ( Instance ( i ) for i in np . column_stack (( capacities , weights_and_profits )) )","title":"generate_instances"},{"location":"reference/domains/kp/#domains.kp.KnapsackDomain.generate_problems_from_instances","text":"Generates a List of Knapsack objects from the instances Parameters: instances ( Sequence [ Instance ] ) \u2013 Instances to create the problems from Returns: List ( List ) \u2013 List containing len(instances) objects of type Knapsack Source code in digneapy/domains/kp.py 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 def generate_problems_from_instances ( self , instances : Sequence [ Instance ] | np . ndarray ) -> List : \"\"\"Generates a List of Knapsack objects from the instances Args: instances (Sequence[Instance]): Instances to create the problems from Returns: List: List containing len(instances) objects of type Knapsack \"\"\" if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances ) capacities = instances [:, 0 ] . astype ( int ) weights = instances [:, 1 :: 2 ] . astype ( int ) profits = instances [:, 2 :: 2 ] . astype ( int ) # Sets the capacity according to the method if self . capacity_approach == \"percentage\" : capacities [:] = ( np . sum ( weights , axis = 1 ) * self . capacity_ratio ) . astype ( np . int32 ) elif self . capacity_approach == \"fixed\" : capacities [:] = self . max_capacity return list ( Knapsack ( profits = profits [ i ], weights = weights [ i ], capacity = capacities [ i ]) for i in range ( len ( instances )) )","title":"generate_problems_from_instances"},{"location":"reference/domains/tsp/","text":"@File : tsp.py @Time : 2025/02/21 10:47:31 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2025, Alejandro Marrero @Desc : None TSP Bases: Problem Symmetric Travelling Salesman Problem Source code in digneapy/domains/tsp.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 class TSP ( Problem ): \"\"\"Symmetric Travelling Salesman Problem\"\"\" def __init__ ( self , nodes : int , coords : np . ndarray , seed : int = 42 , * args , ** kwargs , ): \"\"\"Creates a new Symmetric Travelling Salesman Problem Args: nodes (int): Number of nodes/cities in the instance to solve coords (np.ndarray(N, 2)): Coordinates of each node/city. \"\"\" self . _nodes = nodes if coords . shape [ 1 ] != 2 : raise ValueError ( f \"Expected coordinates shape to be (N, 2). Instead coords has the following shape: { coords . shape } \" ) if not isinstance ( coords , np . ndarray ): coords = np . asarray ( coords ) self . _coords = coords x_min , y_min = np . min ( self . _coords , axis = 0 ) x_max , y_max = np . max ( self . _coords , axis = 0 ) bounds = list ((( x_min , y_min ), ( x_max , y_max )) for _ in range ( self . _nodes )) super () . __init__ ( dimension = nodes , bounds = bounds , name = \"TSP\" , seed = seed ) self . _distances = np . zeros (( self . _nodes , self . _nodes )) differences = self . _coords [:, np . newaxis , :] - self . _coords [ np . newaxis , :, :] self . _distances = np . sqrt ( np . sum ( differences ** 2 , axis =- 1 )) def __evaluate_constraints ( self , individual : Sequence | Solution ) -> bool : counter = Counter ( individual ) if any ( counter [ c ] != 1 for c in counter if c != 0 ) or ( individual [ 0 ] != 0 or individual [ - 1 ] != 0 ): return False return True def evaluate ( self , individual : Sequence | Solution ) -> tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Travelling Salesmas Problem. The fitness of the solution is the fraction of the sum of the distances of the tour Args: individual (Sequence | Solution): Individual to evaluate Returns: Tuple[float]: Fitness \"\"\" if len ( individual ) != self . _nodes + 1 : msg = f \"Mismatch between individual variables ( { len ( individual ) } ) and instance variables ( { self . _nodes } ) in { self . __class__ . __name__ } . A solution for the TSP must be a sequence of len { self . _nodes + 1 } \" raise ValueError ( msg ) penalty : np . float64 = np . float64 ( 0 ) if self . __evaluate_constraints ( individual ): distance : float = 0.0 for i in range ( len ( individual ) - 2 ): distance += self . _distances [ individual [ i ]][ individual [ i + 1 ]] fitness = 1.0 / distance else : fitness = 2.938736e-39 # --> 1.0 / np.float.max penalty = np . finfo ( np . float64 ) . max if isinstance ( individual , Solution ): individual . fitness = fitness individual . objectives = ( fitness ,) individual . constraints = ( penalty ,) return ( fitness ,) def __call__ ( self , individual : Sequence | Solution ) -> tuple [ float ]: return self . evaluate ( individual ) def __repr__ ( self ): return f \"TSP<n= { self . _nodes } >\" def __len__ ( self ): return self . _nodes def __array__ ( self , dtype = np . float32 , copy : Optional [ bool ] = True ) -> npt . ArrayLike : return np . asarray ( self . _coords , dtype = dtype , copy = copy ) def create_solution ( self ) -> Solution : items = [ 0 ] + list ( range ( 1 , self . _nodes )) + [ 0 ] return Solution ( variables = items ) def to_file ( self , filename : str = \"instance.tsp\" ): with open ( filename , \"w\" ) as file : file . write ( f \" { len ( self ) } \\n\\n \" ) content = \" \\n \" . join ( f \" { x } \\t { y } \" for ( x , y ) in self . _coords ) file . write ( content ) @classmethod def from_file ( cls , filename : str ) -> Self : # TODO: Improve using np.loadtxt with open ( filename ) as f : lines = f . readlines () lines = [ line . rstrip () for line in lines ] nodes = int ( lines [ 0 ]) coords = np . zeros ( shape = ( nodes , 2 ), dtype = np . float32 ) for i , line in enumerate ( lines [ 2 :]): x , y = line . split () coords [ i ] = [ np . float32 ( x ), np . float32 ( y )] return cls ( nodes = nodes , coords = coords ) def to_instance ( self ) -> Instance : return Instance ( variables = self . _coords . flatten ()) __init__ ( nodes , coords , seed = 42 , * args , ** kwargs ) Creates a new Symmetric Travelling Salesman Problem Parameters: nodes ( int ) \u2013 Number of nodes/cities in the instance to solve coords ( ndarray ( N , 2) ) \u2013 Coordinates of each node/city. Source code in digneapy/domains/tsp.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , nodes : int , coords : np . ndarray , seed : int = 42 , * args , ** kwargs , ): \"\"\"Creates a new Symmetric Travelling Salesman Problem Args: nodes (int): Number of nodes/cities in the instance to solve coords (np.ndarray(N, 2)): Coordinates of each node/city. \"\"\" self . _nodes = nodes if coords . shape [ 1 ] != 2 : raise ValueError ( f \"Expected coordinates shape to be (N, 2). Instead coords has the following shape: { coords . shape } \" ) if not isinstance ( coords , np . ndarray ): coords = np . asarray ( coords ) self . _coords = coords x_min , y_min = np . min ( self . _coords , axis = 0 ) x_max , y_max = np . max ( self . _coords , axis = 0 ) bounds = list ((( x_min , y_min ), ( x_max , y_max )) for _ in range ( self . _nodes )) super () . __init__ ( dimension = nodes , bounds = bounds , name = \"TSP\" , seed = seed ) self . _distances = np . zeros (( self . _nodes , self . _nodes )) differences = self . _coords [:, np . newaxis , :] - self . _coords [ np . newaxis , :, :] self . _distances = np . sqrt ( np . sum ( differences ** 2 , axis =- 1 )) evaluate ( individual ) Evaluates the candidate individual with the information of the Travelling Salesmas Problem. The fitness of the solution is the fraction of the sum of the distances of the tour Args: individual (Sequence | Solution): Individual to evaluate Returns: tuple [ float ] \u2013 Tuple[float]: Fitness Source code in digneapy/domains/tsp.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def evaluate ( self , individual : Sequence | Solution ) -> tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Travelling Salesmas Problem. The fitness of the solution is the fraction of the sum of the distances of the tour Args: individual (Sequence | Solution): Individual to evaluate Returns: Tuple[float]: Fitness \"\"\" if len ( individual ) != self . _nodes + 1 : msg = f \"Mismatch between individual variables ( { len ( individual ) } ) and instance variables ( { self . _nodes } ) in { self . __class__ . __name__ } . A solution for the TSP must be a sequence of len { self . _nodes + 1 } \" raise ValueError ( msg ) penalty : np . float64 = np . float64 ( 0 ) if self . __evaluate_constraints ( individual ): distance : float = 0.0 for i in range ( len ( individual ) - 2 ): distance += self . _distances [ individual [ i ]][ individual [ i + 1 ]] fitness = 1.0 / distance else : fitness = 2.938736e-39 # --> 1.0 / np.float.max penalty = np . finfo ( np . float64 ) . max if isinstance ( individual , Solution ): individual . fitness = fitness individual . objectives = ( fitness ,) individual . constraints = ( penalty ,) return ( fitness ,) TSPDomain Bases: Domain Domain to generate instances for the Symmetric Travelling Salesman Problem. Source code in digneapy/domains/tsp.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 class TSPDomain ( Domain ): \"\"\"Domain to generate instances for the Symmetric Travelling Salesman Problem.\"\"\" __FEAT_NAMES = \"size,std_distances,centroid_x,centroid_y,radius,fraction_distances,area,variance_nnNds,variation_nnNds,cluster_ratio,mean_cluster_radius\" . split ( \",\" ) def __init__ ( self , dimension : int = 100 , x_range : Tuple [ int , int ] = ( 0 , 1000 ), y_range : Tuple [ int , int ] = ( 0 , 1000 ), seed : int = 42 , ): \"\"\"Creates a new TSPDomain to generate instances for the Symmetric Travelling Salesman Problem Args: dimension (int, optional): Dimension of the instances to generate. Defaults to 100. x_range (Tuple[int, int], optional): Ranges for the Xs coordinates of each node/city. Defaults to (0, 1000). y_range (Tuple[int, int], optional): Ranges for the ys coordinates of each node/city. Defaults to (0, 1000). Raises: ValueError: If dimension is < 0 ValueError: If x_range OR y_range does not have 2 dimensions each ValueError: If minimum ranges are greater than maximum ranges \"\"\" if dimension < 0 : raise ValueError ( f \"Expected dimension > 0 got { dimension } \" ) if len ( x_range ) != 2 or len ( y_range ) != 2 : raise ValueError ( f \"Expected x_range and y_range to be a tuple with only to integers. Got: x_range = { x_range } and y_range = { y_range } \" ) x_min , x_max = x_range y_min , y_max = y_range if x_min < 0 or x_max <= x_min : raise ValueError ( f \"Expected x_range to be (x_min, x_max) where x_min >= 0 and x_max > x_min. Got: x_range { x_range } \" ) if y_min < 0 or y_max <= y_min : raise ValueError ( f \"Expected y_range to be (y_min, y_max) where y_min >= 0 and y_max > y_min. Got: y_range { y_range } \" ) self . _x_range = x_range self . _y_range = y_range __bounds = [ ( x_min , x_max ) if i % 2 == 0 else ( y_min , y_max ) for i in range ( dimension * 2 ) ] super () . __init__ ( dimension = dimension , bounds = __bounds , name = \"TSP\" , seed = seed ) def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances using numpy. It can return the instances in two formats: 1. A numpy ndarray with the definition of the instances 2. A list of Instance objects created from the raw numpy generation Args: n (int, optional): Number of instances to generate. Defaults to 1. cast (bool, optional): Whether to cast the raw data to Instance objects. Defaults to False. Returns: List[Instance]: Sequence of instances \"\"\" instances = np . empty ( shape = ( n , self . dimension * 2 ), dtype = np . float32 ) instances [:, 0 :: 2 ] = self . _rng . uniform ( low = self . _x_range [ 0 ], high = self . _x_range [ 1 ], size = ( n , ( self . dimension )), ) instances [:, 1 :: 2 ] = self . _rng . uniform ( low = self . _y_range [ 0 ], high = self . _y_range [ 1 ], size = ( n , ( self . dimension )), ) return list ( Instance ( coords ) for coords in instances ) def extract_features ( self , instances : Sequence [ Instance ]) -> np . ndarray : \"\"\"Extract the features of the instance based on the TSP domain. For the TSP the features are: - Size - Standard deviation of the distances - Centroid coordinates - Radius of the instance - Fraction of distinct distances - Rectangular area - Variance of the normalised nearest neighbours distances - Coefficient of variation of the nearest neighbours distances - Cluster ratio - Mean cluster radius Args: instance (Instance): Instance to extract the features from Returns: Tuple[float]: Values of each feature \"\"\" _instances = np . asarray ( instances , copy = True ) N_INSTANCES = len ( _instances ) N_CITIES = len ( _instances [ 0 ]) // 2 # self.dimension // 2 assert _instances is not instances coords = np . asarray ( _instances , copy = True ) . reshape (( N_INSTANCES , N_CITIES , 2 )) xs = coords [:, :, 0 ] ys = coords [:, :, 1 ] areas = ( ( np . max ( xs , axis = 1 ) - np . min ( xs , axis = 1 )) * ( np . max ( ys , axis = 1 ) - np . min ( ys , axis = 1 )) ) . astype ( np . float64 ) # Compute distances for all instances distances = np . zeros (( N_INSTANCES , N_CITIES , N_CITIES )) differences = coords [:, :, np . newaxis , :] - coords [:, np . newaxis , :, :] distances = np . sqrt ( np . sum ( differences ** 2 , axis =- 1 )) mask = ~ np . eye ( N_CITIES , dtype = bool ) std_distances = np . std ( distances [:, mask ], axis = 1 ) centroids = np . mean ( coords , axis = 1 ) expanded_centroids = centroids [:, np . newaxis , :] centroids_distances = np . linalg . norm ( coords - expanded_centroids , axis =- 1 ) radius = np . mean ( centroids_distances , axis = 1 ) fractions = np . array ( [ np . unique ( d [ np . triu_indices_from ( d , k = 1 )]) . size / ( N_CITIES * ( N_CITIES - 1 ) / 2 ) for d in distances ] ) # Top five only norm_distances = np . sort ( distances , axis = 2 )[:, :, :: - 1 ][:, :, : 5 ] / np . max ( distances , axis = ( 1 , 2 ), keepdims = True ) variance_nnds = np . var ( norm_distances , axis = ( 1 , 2 )) variation_nnds = variance_nnds / np . mean ( norm_distances , axis = ( 1 , 2 )) cluster_ratio = np . empty ( shape = N_INSTANCES , dtype = np . float64 ) mean_cluster_radius = np . empty ( shape = N_INSTANCES , dtype = np . float64 ) for i in range ( N_INSTANCES ): scale = np . mean ( np . std ( coords [ i ], axis = 0 )) dbscan = DBSCAN ( eps = 0.2 * scale , min_samples = 1 ) labels = dbscan . fit_predict ( coords [ i ]) unique_labels = [ label for label in set ( labels ) if label != - 1 ] cluster_ratio [ i ] = len ( unique_labels ) / N_CITIES # Cluster radius cluster_radius = np . empty ( shape = len ( unique_labels ), dtype = np . float64 ) for j , label_id in enumerate ( unique_labels ): points_in_cluster = coords [ i ][ labels == label_id ] cluster_centroid = np . mean ( points_in_cluster , axis = 0 ) cluster_radius [ j ] = np . mean ( np . linalg . norm ( points_in_cluster - cluster_centroid , axis = 1 ) ) mean_cluster_radius [ i ] = ( np . mean ( cluster_radius ) if cluster_radius . size > 0 else 0.0 ) return np . column_stack ( [ np . full ( shape = len ( _instances ), fill_value = N_CITIES ), std_distances , centroids [:, 0 ], centroids [:, 1 ], radius , fractions , areas , variance_nnds , variation_nnds , cluster_ratio , mean_cluster_radius , ] ) . astype ( np . float64 ) def extract_features_as_dict ( self , instances : Sequence [ Instance ] ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Args: instance (Instance): Instance to extract the features from Returns: Mapping[str, float]: Dictionary with the names/values of each feature \"\"\" features = self . extract_features ( instances ) named_features : list [ dict [ str , np . float32 ]] = [{}] * len ( features ) for i , feats in enumerate ( features ): named_features [ i ] = { k : v for k , v in zip ( TSPDomain . __FEAT_NAMES , feats )} return named_features def generate_problem_from_instance ( self , instance : Instance ) -> TSP : n_nodes = len ( instance ) // 2 coords = np . array ([ * zip ( instance [:: 2 ], instance [ 1 :: 2 ])]) return TSP ( nodes = n_nodes , coords = coords ) def generate_problems_from_instances ( self , instances : Sequence [ Instance ] ) -> List [ Problem ]: if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances ) dimension = instances . shape [ 1 ] // 2 return list ( TSP ( nodes = dimension , coords = np . array ([ * zip ( instance [ 0 :: 2 ], instance [ 1 :: 2 ])]) ) for instance in instances ) __init__ ( dimension = 100 , x_range = ( 0 , 1000 ), y_range = ( 0 , 1000 ), seed = 42 ) Creates a new TSPDomain to generate instances for the Symmetric Travelling Salesman Problem Parameters: dimension ( int , default: 100 ) \u2013 Dimension of the instances to generate. Defaults to 100. x_range ( Tuple [ int , int ] , default: (0, 1000) ) \u2013 Ranges for the Xs coordinates of each node/city. Defaults to (0, 1000). y_range ( Tuple [ int , int ] , default: (0, 1000) ) \u2013 Ranges for the ys coordinates of each node/city. Defaults to (0, 1000). Raises: ValueError \u2013 If dimension is < 0 ValueError \u2013 If x_range OR y_range does not have 2 dimensions each ValueError \u2013 If minimum ranges are greater than maximum ranges Source code in digneapy/domains/tsp.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def __init__ ( self , dimension : int = 100 , x_range : Tuple [ int , int ] = ( 0 , 1000 ), y_range : Tuple [ int , int ] = ( 0 , 1000 ), seed : int = 42 , ): \"\"\"Creates a new TSPDomain to generate instances for the Symmetric Travelling Salesman Problem Args: dimension (int, optional): Dimension of the instances to generate. Defaults to 100. x_range (Tuple[int, int], optional): Ranges for the Xs coordinates of each node/city. Defaults to (0, 1000). y_range (Tuple[int, int], optional): Ranges for the ys coordinates of each node/city. Defaults to (0, 1000). Raises: ValueError: If dimension is < 0 ValueError: If x_range OR y_range does not have 2 dimensions each ValueError: If minimum ranges are greater than maximum ranges \"\"\" if dimension < 0 : raise ValueError ( f \"Expected dimension > 0 got { dimension } \" ) if len ( x_range ) != 2 or len ( y_range ) != 2 : raise ValueError ( f \"Expected x_range and y_range to be a tuple with only to integers. Got: x_range = { x_range } and y_range = { y_range } \" ) x_min , x_max = x_range y_min , y_max = y_range if x_min < 0 or x_max <= x_min : raise ValueError ( f \"Expected x_range to be (x_min, x_max) where x_min >= 0 and x_max > x_min. Got: x_range { x_range } \" ) if y_min < 0 or y_max <= y_min : raise ValueError ( f \"Expected y_range to be (y_min, y_max) where y_min >= 0 and y_max > y_min. Got: y_range { y_range } \" ) self . _x_range = x_range self . _y_range = y_range __bounds = [ ( x_min , x_max ) if i % 2 == 0 else ( y_min , y_max ) for i in range ( dimension * 2 ) ] super () . __init__ ( dimension = dimension , bounds = __bounds , name = \"TSP\" , seed = seed ) extract_features ( instances ) Extract the features of the instance based on the TSP domain. For the TSP the features are: - Size - Standard deviation of the distances - Centroid coordinates - Radius of the instance - Fraction of distinct distances - Rectangular area - Variance of the normalised nearest neighbours distances - Coefficient of variation of the nearest neighbours distances - Cluster ratio - Mean cluster radius Args: instance (Instance): Instance to extract the features from Returns: ndarray \u2013 Tuple[float]: Values of each feature Source code in digneapy/domains/tsp.py 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 def extract_features ( self , instances : Sequence [ Instance ]) -> np . ndarray : \"\"\"Extract the features of the instance based on the TSP domain. For the TSP the features are: - Size - Standard deviation of the distances - Centroid coordinates - Radius of the instance - Fraction of distinct distances - Rectangular area - Variance of the normalised nearest neighbours distances - Coefficient of variation of the nearest neighbours distances - Cluster ratio - Mean cluster radius Args: instance (Instance): Instance to extract the features from Returns: Tuple[float]: Values of each feature \"\"\" _instances = np . asarray ( instances , copy = True ) N_INSTANCES = len ( _instances ) N_CITIES = len ( _instances [ 0 ]) // 2 # self.dimension // 2 assert _instances is not instances coords = np . asarray ( _instances , copy = True ) . reshape (( N_INSTANCES , N_CITIES , 2 )) xs = coords [:, :, 0 ] ys = coords [:, :, 1 ] areas = ( ( np . max ( xs , axis = 1 ) - np . min ( xs , axis = 1 )) * ( np . max ( ys , axis = 1 ) - np . min ( ys , axis = 1 )) ) . astype ( np . float64 ) # Compute distances for all instances distances = np . zeros (( N_INSTANCES , N_CITIES , N_CITIES )) differences = coords [:, :, np . newaxis , :] - coords [:, np . newaxis , :, :] distances = np . sqrt ( np . sum ( differences ** 2 , axis =- 1 )) mask = ~ np . eye ( N_CITIES , dtype = bool ) std_distances = np . std ( distances [:, mask ], axis = 1 ) centroids = np . mean ( coords , axis = 1 ) expanded_centroids = centroids [:, np . newaxis , :] centroids_distances = np . linalg . norm ( coords - expanded_centroids , axis =- 1 ) radius = np . mean ( centroids_distances , axis = 1 ) fractions = np . array ( [ np . unique ( d [ np . triu_indices_from ( d , k = 1 )]) . size / ( N_CITIES * ( N_CITIES - 1 ) / 2 ) for d in distances ] ) # Top five only norm_distances = np . sort ( distances , axis = 2 )[:, :, :: - 1 ][:, :, : 5 ] / np . max ( distances , axis = ( 1 , 2 ), keepdims = True ) variance_nnds = np . var ( norm_distances , axis = ( 1 , 2 )) variation_nnds = variance_nnds / np . mean ( norm_distances , axis = ( 1 , 2 )) cluster_ratio = np . empty ( shape = N_INSTANCES , dtype = np . float64 ) mean_cluster_radius = np . empty ( shape = N_INSTANCES , dtype = np . float64 ) for i in range ( N_INSTANCES ): scale = np . mean ( np . std ( coords [ i ], axis = 0 )) dbscan = DBSCAN ( eps = 0.2 * scale , min_samples = 1 ) labels = dbscan . fit_predict ( coords [ i ]) unique_labels = [ label for label in set ( labels ) if label != - 1 ] cluster_ratio [ i ] = len ( unique_labels ) / N_CITIES # Cluster radius cluster_radius = np . empty ( shape = len ( unique_labels ), dtype = np . float64 ) for j , label_id in enumerate ( unique_labels ): points_in_cluster = coords [ i ][ labels == label_id ] cluster_centroid = np . mean ( points_in_cluster , axis = 0 ) cluster_radius [ j ] = np . mean ( np . linalg . norm ( points_in_cluster - cluster_centroid , axis = 1 ) ) mean_cluster_radius [ i ] = ( np . mean ( cluster_radius ) if cluster_radius . size > 0 else 0.0 ) return np . column_stack ( [ np . full ( shape = len ( _instances ), fill_value = N_CITIES ), std_distances , centroids [:, 0 ], centroids [:, 1 ], radius , fractions , areas , variance_nnds , variation_nnds , cluster_ratio , mean_cluster_radius , ] ) . astype ( np . float64 ) extract_features_as_dict ( instances ) Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Parameters: instance ( Instance ) \u2013 Instance to extract the features from Returns: List [ Dict [ str , float32 ]] \u2013 Mapping[str, float]: Dictionary with the names/values of each feature Source code in digneapy/domains/tsp.py 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 def extract_features_as_dict ( self , instances : Sequence [ Instance ] ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Args: instance (Instance): Instance to extract the features from Returns: Mapping[str, float]: Dictionary with the names/values of each feature \"\"\" features = self . extract_features ( instances ) named_features : list [ dict [ str , np . float32 ]] = [{}] * len ( features ) for i , feats in enumerate ( features ): named_features [ i ] = { k : v for k , v in zip ( TSPDomain . __FEAT_NAMES , feats )} return named_features generate_instances ( n = 1 ) Generates N instances using numpy. It can return the instances in two formats: 1. A numpy ndarray with the definition of the instances 2. A list of Instance objects created from the raw numpy generation Parameters: n ( int , default: 1 ) \u2013 Number of instances to generate. Defaults to 1. cast ( bool ) \u2013 Whether to cast the raw data to Instance objects. Defaults to False. Returns: List [ Instance ] \u2013 List[Instance]: Sequence of instances Source code in digneapy/domains/tsp.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances using numpy. It can return the instances in two formats: 1. A numpy ndarray with the definition of the instances 2. A list of Instance objects created from the raw numpy generation Args: n (int, optional): Number of instances to generate. Defaults to 1. cast (bool, optional): Whether to cast the raw data to Instance objects. Defaults to False. Returns: List[Instance]: Sequence of instances \"\"\" instances = np . empty ( shape = ( n , self . dimension * 2 ), dtype = np . float32 ) instances [:, 0 :: 2 ] = self . _rng . uniform ( low = self . _x_range [ 0 ], high = self . _x_range [ 1 ], size = ( n , ( self . dimension )), ) instances [:, 1 :: 2 ] = self . _rng . uniform ( low = self . _y_range [ 0 ], high = self . _y_range [ 1 ], size = ( n , ( self . dimension )), ) return list ( Instance ( coords ) for coords in instances )","title":"Tsp"},{"location":"reference/domains/tsp/#domains.tsp.TSP","text":"Bases: Problem Symmetric Travelling Salesman Problem Source code in digneapy/domains/tsp.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 class TSP ( Problem ): \"\"\"Symmetric Travelling Salesman Problem\"\"\" def __init__ ( self , nodes : int , coords : np . ndarray , seed : int = 42 , * args , ** kwargs , ): \"\"\"Creates a new Symmetric Travelling Salesman Problem Args: nodes (int): Number of nodes/cities in the instance to solve coords (np.ndarray(N, 2)): Coordinates of each node/city. \"\"\" self . _nodes = nodes if coords . shape [ 1 ] != 2 : raise ValueError ( f \"Expected coordinates shape to be (N, 2). Instead coords has the following shape: { coords . shape } \" ) if not isinstance ( coords , np . ndarray ): coords = np . asarray ( coords ) self . _coords = coords x_min , y_min = np . min ( self . _coords , axis = 0 ) x_max , y_max = np . max ( self . _coords , axis = 0 ) bounds = list ((( x_min , y_min ), ( x_max , y_max )) for _ in range ( self . _nodes )) super () . __init__ ( dimension = nodes , bounds = bounds , name = \"TSP\" , seed = seed ) self . _distances = np . zeros (( self . _nodes , self . _nodes )) differences = self . _coords [:, np . newaxis , :] - self . _coords [ np . newaxis , :, :] self . _distances = np . sqrt ( np . sum ( differences ** 2 , axis =- 1 )) def __evaluate_constraints ( self , individual : Sequence | Solution ) -> bool : counter = Counter ( individual ) if any ( counter [ c ] != 1 for c in counter if c != 0 ) or ( individual [ 0 ] != 0 or individual [ - 1 ] != 0 ): return False return True def evaluate ( self , individual : Sequence | Solution ) -> tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Travelling Salesmas Problem. The fitness of the solution is the fraction of the sum of the distances of the tour Args: individual (Sequence | Solution): Individual to evaluate Returns: Tuple[float]: Fitness \"\"\" if len ( individual ) != self . _nodes + 1 : msg = f \"Mismatch between individual variables ( { len ( individual ) } ) and instance variables ( { self . _nodes } ) in { self . __class__ . __name__ } . A solution for the TSP must be a sequence of len { self . _nodes + 1 } \" raise ValueError ( msg ) penalty : np . float64 = np . float64 ( 0 ) if self . __evaluate_constraints ( individual ): distance : float = 0.0 for i in range ( len ( individual ) - 2 ): distance += self . _distances [ individual [ i ]][ individual [ i + 1 ]] fitness = 1.0 / distance else : fitness = 2.938736e-39 # --> 1.0 / np.float.max penalty = np . finfo ( np . float64 ) . max if isinstance ( individual , Solution ): individual . fitness = fitness individual . objectives = ( fitness ,) individual . constraints = ( penalty ,) return ( fitness ,) def __call__ ( self , individual : Sequence | Solution ) -> tuple [ float ]: return self . evaluate ( individual ) def __repr__ ( self ): return f \"TSP<n= { self . _nodes } >\" def __len__ ( self ): return self . _nodes def __array__ ( self , dtype = np . float32 , copy : Optional [ bool ] = True ) -> npt . ArrayLike : return np . asarray ( self . _coords , dtype = dtype , copy = copy ) def create_solution ( self ) -> Solution : items = [ 0 ] + list ( range ( 1 , self . _nodes )) + [ 0 ] return Solution ( variables = items ) def to_file ( self , filename : str = \"instance.tsp\" ): with open ( filename , \"w\" ) as file : file . write ( f \" { len ( self ) } \\n\\n \" ) content = \" \\n \" . join ( f \" { x } \\t { y } \" for ( x , y ) in self . _coords ) file . write ( content ) @classmethod def from_file ( cls , filename : str ) -> Self : # TODO: Improve using np.loadtxt with open ( filename ) as f : lines = f . readlines () lines = [ line . rstrip () for line in lines ] nodes = int ( lines [ 0 ]) coords = np . zeros ( shape = ( nodes , 2 ), dtype = np . float32 ) for i , line in enumerate ( lines [ 2 :]): x , y = line . split () coords [ i ] = [ np . float32 ( x ), np . float32 ( y )] return cls ( nodes = nodes , coords = coords ) def to_instance ( self ) -> Instance : return Instance ( variables = self . _coords . flatten ())","title":"TSP"},{"location":"reference/domains/tsp/#domains.tsp.TSP.__init__","text":"Creates a new Symmetric Travelling Salesman Problem Parameters: nodes ( int ) \u2013 Number of nodes/cities in the instance to solve coords ( ndarray ( N , 2) ) \u2013 Coordinates of each node/city. Source code in digneapy/domains/tsp.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , nodes : int , coords : np . ndarray , seed : int = 42 , * args , ** kwargs , ): \"\"\"Creates a new Symmetric Travelling Salesman Problem Args: nodes (int): Number of nodes/cities in the instance to solve coords (np.ndarray(N, 2)): Coordinates of each node/city. \"\"\" self . _nodes = nodes if coords . shape [ 1 ] != 2 : raise ValueError ( f \"Expected coordinates shape to be (N, 2). Instead coords has the following shape: { coords . shape } \" ) if not isinstance ( coords , np . ndarray ): coords = np . asarray ( coords ) self . _coords = coords x_min , y_min = np . min ( self . _coords , axis = 0 ) x_max , y_max = np . max ( self . _coords , axis = 0 ) bounds = list ((( x_min , y_min ), ( x_max , y_max )) for _ in range ( self . _nodes )) super () . __init__ ( dimension = nodes , bounds = bounds , name = \"TSP\" , seed = seed ) self . _distances = np . zeros (( self . _nodes , self . _nodes )) differences = self . _coords [:, np . newaxis , :] - self . _coords [ np . newaxis , :, :] self . _distances = np . sqrt ( np . sum ( differences ** 2 , axis =- 1 ))","title":"__init__"},{"location":"reference/domains/tsp/#domains.tsp.TSP.evaluate","text":"Evaluates the candidate individual with the information of the Travelling Salesmas Problem. The fitness of the solution is the fraction of the sum of the distances of the tour Args: individual (Sequence | Solution): Individual to evaluate Returns: tuple [ float ] \u2013 Tuple[float]: Fitness Source code in digneapy/domains/tsp.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def evaluate ( self , individual : Sequence | Solution ) -> tuple [ float ]: \"\"\"Evaluates the candidate individual with the information of the Travelling Salesmas Problem. The fitness of the solution is the fraction of the sum of the distances of the tour Args: individual (Sequence | Solution): Individual to evaluate Returns: Tuple[float]: Fitness \"\"\" if len ( individual ) != self . _nodes + 1 : msg = f \"Mismatch between individual variables ( { len ( individual ) } ) and instance variables ( { self . _nodes } ) in { self . __class__ . __name__ } . A solution for the TSP must be a sequence of len { self . _nodes + 1 } \" raise ValueError ( msg ) penalty : np . float64 = np . float64 ( 0 ) if self . __evaluate_constraints ( individual ): distance : float = 0.0 for i in range ( len ( individual ) - 2 ): distance += self . _distances [ individual [ i ]][ individual [ i + 1 ]] fitness = 1.0 / distance else : fitness = 2.938736e-39 # --> 1.0 / np.float.max penalty = np . finfo ( np . float64 ) . max if isinstance ( individual , Solution ): individual . fitness = fitness individual . objectives = ( fitness ,) individual . constraints = ( penalty ,) return ( fitness ,)","title":"evaluate"},{"location":"reference/domains/tsp/#domains.tsp.TSPDomain","text":"Bases: Domain Domain to generate instances for the Symmetric Travelling Salesman Problem. Source code in digneapy/domains/tsp.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 class TSPDomain ( Domain ): \"\"\"Domain to generate instances for the Symmetric Travelling Salesman Problem.\"\"\" __FEAT_NAMES = \"size,std_distances,centroid_x,centroid_y,radius,fraction_distances,area,variance_nnNds,variation_nnNds,cluster_ratio,mean_cluster_radius\" . split ( \",\" ) def __init__ ( self , dimension : int = 100 , x_range : Tuple [ int , int ] = ( 0 , 1000 ), y_range : Tuple [ int , int ] = ( 0 , 1000 ), seed : int = 42 , ): \"\"\"Creates a new TSPDomain to generate instances for the Symmetric Travelling Salesman Problem Args: dimension (int, optional): Dimension of the instances to generate. Defaults to 100. x_range (Tuple[int, int], optional): Ranges for the Xs coordinates of each node/city. Defaults to (0, 1000). y_range (Tuple[int, int], optional): Ranges for the ys coordinates of each node/city. Defaults to (0, 1000). Raises: ValueError: If dimension is < 0 ValueError: If x_range OR y_range does not have 2 dimensions each ValueError: If minimum ranges are greater than maximum ranges \"\"\" if dimension < 0 : raise ValueError ( f \"Expected dimension > 0 got { dimension } \" ) if len ( x_range ) != 2 or len ( y_range ) != 2 : raise ValueError ( f \"Expected x_range and y_range to be a tuple with only to integers. Got: x_range = { x_range } and y_range = { y_range } \" ) x_min , x_max = x_range y_min , y_max = y_range if x_min < 0 or x_max <= x_min : raise ValueError ( f \"Expected x_range to be (x_min, x_max) where x_min >= 0 and x_max > x_min. Got: x_range { x_range } \" ) if y_min < 0 or y_max <= y_min : raise ValueError ( f \"Expected y_range to be (y_min, y_max) where y_min >= 0 and y_max > y_min. Got: y_range { y_range } \" ) self . _x_range = x_range self . _y_range = y_range __bounds = [ ( x_min , x_max ) if i % 2 == 0 else ( y_min , y_max ) for i in range ( dimension * 2 ) ] super () . __init__ ( dimension = dimension , bounds = __bounds , name = \"TSP\" , seed = seed ) def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances using numpy. It can return the instances in two formats: 1. A numpy ndarray with the definition of the instances 2. A list of Instance objects created from the raw numpy generation Args: n (int, optional): Number of instances to generate. Defaults to 1. cast (bool, optional): Whether to cast the raw data to Instance objects. Defaults to False. Returns: List[Instance]: Sequence of instances \"\"\" instances = np . empty ( shape = ( n , self . dimension * 2 ), dtype = np . float32 ) instances [:, 0 :: 2 ] = self . _rng . uniform ( low = self . _x_range [ 0 ], high = self . _x_range [ 1 ], size = ( n , ( self . dimension )), ) instances [:, 1 :: 2 ] = self . _rng . uniform ( low = self . _y_range [ 0 ], high = self . _y_range [ 1 ], size = ( n , ( self . dimension )), ) return list ( Instance ( coords ) for coords in instances ) def extract_features ( self , instances : Sequence [ Instance ]) -> np . ndarray : \"\"\"Extract the features of the instance based on the TSP domain. For the TSP the features are: - Size - Standard deviation of the distances - Centroid coordinates - Radius of the instance - Fraction of distinct distances - Rectangular area - Variance of the normalised nearest neighbours distances - Coefficient of variation of the nearest neighbours distances - Cluster ratio - Mean cluster radius Args: instance (Instance): Instance to extract the features from Returns: Tuple[float]: Values of each feature \"\"\" _instances = np . asarray ( instances , copy = True ) N_INSTANCES = len ( _instances ) N_CITIES = len ( _instances [ 0 ]) // 2 # self.dimension // 2 assert _instances is not instances coords = np . asarray ( _instances , copy = True ) . reshape (( N_INSTANCES , N_CITIES , 2 )) xs = coords [:, :, 0 ] ys = coords [:, :, 1 ] areas = ( ( np . max ( xs , axis = 1 ) - np . min ( xs , axis = 1 )) * ( np . max ( ys , axis = 1 ) - np . min ( ys , axis = 1 )) ) . astype ( np . float64 ) # Compute distances for all instances distances = np . zeros (( N_INSTANCES , N_CITIES , N_CITIES )) differences = coords [:, :, np . newaxis , :] - coords [:, np . newaxis , :, :] distances = np . sqrt ( np . sum ( differences ** 2 , axis =- 1 )) mask = ~ np . eye ( N_CITIES , dtype = bool ) std_distances = np . std ( distances [:, mask ], axis = 1 ) centroids = np . mean ( coords , axis = 1 ) expanded_centroids = centroids [:, np . newaxis , :] centroids_distances = np . linalg . norm ( coords - expanded_centroids , axis =- 1 ) radius = np . mean ( centroids_distances , axis = 1 ) fractions = np . array ( [ np . unique ( d [ np . triu_indices_from ( d , k = 1 )]) . size / ( N_CITIES * ( N_CITIES - 1 ) / 2 ) for d in distances ] ) # Top five only norm_distances = np . sort ( distances , axis = 2 )[:, :, :: - 1 ][:, :, : 5 ] / np . max ( distances , axis = ( 1 , 2 ), keepdims = True ) variance_nnds = np . var ( norm_distances , axis = ( 1 , 2 )) variation_nnds = variance_nnds / np . mean ( norm_distances , axis = ( 1 , 2 )) cluster_ratio = np . empty ( shape = N_INSTANCES , dtype = np . float64 ) mean_cluster_radius = np . empty ( shape = N_INSTANCES , dtype = np . float64 ) for i in range ( N_INSTANCES ): scale = np . mean ( np . std ( coords [ i ], axis = 0 )) dbscan = DBSCAN ( eps = 0.2 * scale , min_samples = 1 ) labels = dbscan . fit_predict ( coords [ i ]) unique_labels = [ label for label in set ( labels ) if label != - 1 ] cluster_ratio [ i ] = len ( unique_labels ) / N_CITIES # Cluster radius cluster_radius = np . empty ( shape = len ( unique_labels ), dtype = np . float64 ) for j , label_id in enumerate ( unique_labels ): points_in_cluster = coords [ i ][ labels == label_id ] cluster_centroid = np . mean ( points_in_cluster , axis = 0 ) cluster_radius [ j ] = np . mean ( np . linalg . norm ( points_in_cluster - cluster_centroid , axis = 1 ) ) mean_cluster_radius [ i ] = ( np . mean ( cluster_radius ) if cluster_radius . size > 0 else 0.0 ) return np . column_stack ( [ np . full ( shape = len ( _instances ), fill_value = N_CITIES ), std_distances , centroids [:, 0 ], centroids [:, 1 ], radius , fractions , areas , variance_nnds , variation_nnds , cluster_ratio , mean_cluster_radius , ] ) . astype ( np . float64 ) def extract_features_as_dict ( self , instances : Sequence [ Instance ] ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Args: instance (Instance): Instance to extract the features from Returns: Mapping[str, float]: Dictionary with the names/values of each feature \"\"\" features = self . extract_features ( instances ) named_features : list [ dict [ str , np . float32 ]] = [{}] * len ( features ) for i , feats in enumerate ( features ): named_features [ i ] = { k : v for k , v in zip ( TSPDomain . __FEAT_NAMES , feats )} return named_features def generate_problem_from_instance ( self , instance : Instance ) -> TSP : n_nodes = len ( instance ) // 2 coords = np . array ([ * zip ( instance [:: 2 ], instance [ 1 :: 2 ])]) return TSP ( nodes = n_nodes , coords = coords ) def generate_problems_from_instances ( self , instances : Sequence [ Instance ] ) -> List [ Problem ]: if not isinstance ( instances , np . ndarray ): instances = np . asarray ( instances ) dimension = instances . shape [ 1 ] // 2 return list ( TSP ( nodes = dimension , coords = np . array ([ * zip ( instance [ 0 :: 2 ], instance [ 1 :: 2 ])]) ) for instance in instances )","title":"TSPDomain"},{"location":"reference/domains/tsp/#domains.tsp.TSPDomain.__init__","text":"Creates a new TSPDomain to generate instances for the Symmetric Travelling Salesman Problem Parameters: dimension ( int , default: 100 ) \u2013 Dimension of the instances to generate. Defaults to 100. x_range ( Tuple [ int , int ] , default: (0, 1000) ) \u2013 Ranges for the Xs coordinates of each node/city. Defaults to (0, 1000). y_range ( Tuple [ int , int ] , default: (0, 1000) ) \u2013 Ranges for the ys coordinates of each node/city. Defaults to (0, 1000). Raises: ValueError \u2013 If dimension is < 0 ValueError \u2013 If x_range OR y_range does not have 2 dimensions each ValueError \u2013 If minimum ranges are greater than maximum ranges Source code in digneapy/domains/tsp.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def __init__ ( self , dimension : int = 100 , x_range : Tuple [ int , int ] = ( 0 , 1000 ), y_range : Tuple [ int , int ] = ( 0 , 1000 ), seed : int = 42 , ): \"\"\"Creates a new TSPDomain to generate instances for the Symmetric Travelling Salesman Problem Args: dimension (int, optional): Dimension of the instances to generate. Defaults to 100. x_range (Tuple[int, int], optional): Ranges for the Xs coordinates of each node/city. Defaults to (0, 1000). y_range (Tuple[int, int], optional): Ranges for the ys coordinates of each node/city. Defaults to (0, 1000). Raises: ValueError: If dimension is < 0 ValueError: If x_range OR y_range does not have 2 dimensions each ValueError: If minimum ranges are greater than maximum ranges \"\"\" if dimension < 0 : raise ValueError ( f \"Expected dimension > 0 got { dimension } \" ) if len ( x_range ) != 2 or len ( y_range ) != 2 : raise ValueError ( f \"Expected x_range and y_range to be a tuple with only to integers. Got: x_range = { x_range } and y_range = { y_range } \" ) x_min , x_max = x_range y_min , y_max = y_range if x_min < 0 or x_max <= x_min : raise ValueError ( f \"Expected x_range to be (x_min, x_max) where x_min >= 0 and x_max > x_min. Got: x_range { x_range } \" ) if y_min < 0 or y_max <= y_min : raise ValueError ( f \"Expected y_range to be (y_min, y_max) where y_min >= 0 and y_max > y_min. Got: y_range { y_range } \" ) self . _x_range = x_range self . _y_range = y_range __bounds = [ ( x_min , x_max ) if i % 2 == 0 else ( y_min , y_max ) for i in range ( dimension * 2 ) ] super () . __init__ ( dimension = dimension , bounds = __bounds , name = \"TSP\" , seed = seed )","title":"__init__"},{"location":"reference/domains/tsp/#domains.tsp.TSPDomain.extract_features","text":"Extract the features of the instance based on the TSP domain. For the TSP the features are: - Size - Standard deviation of the distances - Centroid coordinates - Radius of the instance - Fraction of distinct distances - Rectangular area - Variance of the normalised nearest neighbours distances - Coefficient of variation of the nearest neighbours distances - Cluster ratio - Mean cluster radius Args: instance (Instance): Instance to extract the features from Returns: ndarray \u2013 Tuple[float]: Values of each feature Source code in digneapy/domains/tsp.py 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 def extract_features ( self , instances : Sequence [ Instance ]) -> np . ndarray : \"\"\"Extract the features of the instance based on the TSP domain. For the TSP the features are: - Size - Standard deviation of the distances - Centroid coordinates - Radius of the instance - Fraction of distinct distances - Rectangular area - Variance of the normalised nearest neighbours distances - Coefficient of variation of the nearest neighbours distances - Cluster ratio - Mean cluster radius Args: instance (Instance): Instance to extract the features from Returns: Tuple[float]: Values of each feature \"\"\" _instances = np . asarray ( instances , copy = True ) N_INSTANCES = len ( _instances ) N_CITIES = len ( _instances [ 0 ]) // 2 # self.dimension // 2 assert _instances is not instances coords = np . asarray ( _instances , copy = True ) . reshape (( N_INSTANCES , N_CITIES , 2 )) xs = coords [:, :, 0 ] ys = coords [:, :, 1 ] areas = ( ( np . max ( xs , axis = 1 ) - np . min ( xs , axis = 1 )) * ( np . max ( ys , axis = 1 ) - np . min ( ys , axis = 1 )) ) . astype ( np . float64 ) # Compute distances for all instances distances = np . zeros (( N_INSTANCES , N_CITIES , N_CITIES )) differences = coords [:, :, np . newaxis , :] - coords [:, np . newaxis , :, :] distances = np . sqrt ( np . sum ( differences ** 2 , axis =- 1 )) mask = ~ np . eye ( N_CITIES , dtype = bool ) std_distances = np . std ( distances [:, mask ], axis = 1 ) centroids = np . mean ( coords , axis = 1 ) expanded_centroids = centroids [:, np . newaxis , :] centroids_distances = np . linalg . norm ( coords - expanded_centroids , axis =- 1 ) radius = np . mean ( centroids_distances , axis = 1 ) fractions = np . array ( [ np . unique ( d [ np . triu_indices_from ( d , k = 1 )]) . size / ( N_CITIES * ( N_CITIES - 1 ) / 2 ) for d in distances ] ) # Top five only norm_distances = np . sort ( distances , axis = 2 )[:, :, :: - 1 ][:, :, : 5 ] / np . max ( distances , axis = ( 1 , 2 ), keepdims = True ) variance_nnds = np . var ( norm_distances , axis = ( 1 , 2 )) variation_nnds = variance_nnds / np . mean ( norm_distances , axis = ( 1 , 2 )) cluster_ratio = np . empty ( shape = N_INSTANCES , dtype = np . float64 ) mean_cluster_radius = np . empty ( shape = N_INSTANCES , dtype = np . float64 ) for i in range ( N_INSTANCES ): scale = np . mean ( np . std ( coords [ i ], axis = 0 )) dbscan = DBSCAN ( eps = 0.2 * scale , min_samples = 1 ) labels = dbscan . fit_predict ( coords [ i ]) unique_labels = [ label for label in set ( labels ) if label != - 1 ] cluster_ratio [ i ] = len ( unique_labels ) / N_CITIES # Cluster radius cluster_radius = np . empty ( shape = len ( unique_labels ), dtype = np . float64 ) for j , label_id in enumerate ( unique_labels ): points_in_cluster = coords [ i ][ labels == label_id ] cluster_centroid = np . mean ( points_in_cluster , axis = 0 ) cluster_radius [ j ] = np . mean ( np . linalg . norm ( points_in_cluster - cluster_centroid , axis = 1 ) ) mean_cluster_radius [ i ] = ( np . mean ( cluster_radius ) if cluster_radius . size > 0 else 0.0 ) return np . column_stack ( [ np . full ( shape = len ( _instances ), fill_value = N_CITIES ), std_distances , centroids [:, 0 ], centroids [:, 1 ], radius , fractions , areas , variance_nnds , variation_nnds , cluster_ratio , mean_cluster_radius , ] ) . astype ( np . float64 )","title":"extract_features"},{"location":"reference/domains/tsp/#domains.tsp.TSPDomain.extract_features_as_dict","text":"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Parameters: instance ( Instance ) \u2013 Instance to extract the features from Returns: List [ Dict [ str , float32 ]] \u2013 Mapping[str, float]: Dictionary with the names/values of each feature Source code in digneapy/domains/tsp.py 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 def extract_features_as_dict ( self , instances : Sequence [ Instance ] ) -> List [ Dict [ str , np . float32 ]]: \"\"\"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Args: instance (Instance): Instance to extract the features from Returns: Mapping[str, float]: Dictionary with the names/values of each feature \"\"\" features = self . extract_features ( instances ) named_features : list [ dict [ str , np . float32 ]] = [{}] * len ( features ) for i , feats in enumerate ( features ): named_features [ i ] = { k : v for k , v in zip ( TSPDomain . __FEAT_NAMES , feats )} return named_features","title":"extract_features_as_dict"},{"location":"reference/domains/tsp/#domains.tsp.TSPDomain.generate_instances","text":"Generates N instances using numpy. It can return the instances in two formats: 1. A numpy ndarray with the definition of the instances 2. A list of Instance objects created from the raw numpy generation Parameters: n ( int , default: 1 ) \u2013 Number of instances to generate. Defaults to 1. cast ( bool ) \u2013 Whether to cast the raw data to Instance objects. Defaults to False. Returns: List [ Instance ] \u2013 List[Instance]: Sequence of instances Source code in digneapy/domains/tsp.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 def generate_instances ( self , n : int = 1 ) -> List [ Instance ]: \"\"\"Generates N instances using numpy. It can return the instances in two formats: 1. A numpy ndarray with the definition of the instances 2. A list of Instance objects created from the raw numpy generation Args: n (int, optional): Number of instances to generate. Defaults to 1. cast (bool, optional): Whether to cast the raw data to Instance objects. Defaults to False. Returns: List[Instance]: Sequence of instances \"\"\" instances = np . empty ( shape = ( n , self . dimension * 2 ), dtype = np . float32 ) instances [:, 0 :: 2 ] = self . _rng . uniform ( low = self . _x_range [ 0 ], high = self . _x_range [ 1 ], size = ( n , ( self . dimension )), ) instances [:, 1 :: 2 ] = self . _rng . uniform ( low = self . _y_range [ 0 ], high = self . _y_range [ 1 ], size = ( n , ( self . dimension )), ) return list ( Instance ( coords ) for coords in instances )","title":"generate_instances"},{"location":"reference/operators/","text":"@File : init .py @Time : 2023/11/03 10:33:37 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2023, Alejandro Marrero @Desc : None batch_uniform_one_mutation ( population , lb , ub ) Performs uniform one mutation in batches Parameters: population ( ndarray ) \u2013 Batch of individuals to mutate lb ( ndarray ) \u2013 Lower bound for each dimension up ( ndarray ) \u2013 Upper bound for each dimension Raises: ValueError \u2013 If the dimension of the individuals do not match the bounds Returns: ndarray \u2013 np.ndarray: mutated population Source code in digneapy/operators/_mutation.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def batch_uniform_one_mutation ( population : np . ndarray , lb : np . ndarray , ub : np . ndarray ) -> np . ndarray : \"\"\"Performs uniform one mutation in batches Args: population (np.ndarray): Batch of individuals to mutate lb (np.ndarray): Lower bound for each dimension up (np.ndarray): Upper bound for each dimension Raises: ValueError: If the dimension of the individuals do not match the bounds Returns: np.ndarray: mutated population \"\"\" dimension = len ( population [ 0 ]) n_individuals = len ( population ) if len ( lb ) != len ( ub ) or dimension != len ( lb ): msg = f \"The size of individuals ( { dimension } ) and bounds { len ( lb ) } is different in uniform_one_mutation\" raise ValueError ( msg ) rng = np . random . default_rng ( 84793258734753 ) mutation_points = rng . integers ( low = 0 , high = dimension , size = n_individuals ) new_values = rng . uniform ( low = lb [ mutation_points ], high = ub [ mutation_points ], size = n_individuals ) population [ np . arange ( n_individuals ), mutation_points ] = new_values return population binary_tournament_selection ( population ) Binary Tournament Selection Operator Parameters: population ( Sequence ) \u2013 Population of individuals to select a parent from Raises: RuntimeError \u2013 If the population is empty Returns: IndType \u2013 Instance or Solution: New parent Source code in digneapy/operators/_selection.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def binary_tournament_selection ( population : Sequence [ IndType ]) -> IndType : \"\"\"Binary Tournament Selection Operator Args: population (Sequence): Population of individuals to select a parent from Raises: RuntimeError: If the population is empty Returns: Instance or Solution: New parent \"\"\" if not population : msg = \"Trying to selection individuals in an empty population.\" raise ValueError ( msg ) elif len ( population ) == 1 : return population [ 0 ] else : idx1 , idx2 = np . random . default_rng () . integers ( low = 0 , high = len ( population ), size = 2 ) return max ( population [ idx1 ], population [ idx2 ], key = attrgetter ( \"fitness\" )) elitist_replacement ( current_population , offspring , hof = 1 ) Returns a new population constructed using the Elitist approach. HoF number of individuals from the current + offspring populations are kept in the new population. The remaining individuals are selected from the offspring population. Parameters: current_population Sequence[IndType], \u2013 Current population in the algorithm offspring Sequence[IndType], \u2013 Offspring population hof ( int , default: 1 ) \u2013 description . Defaults to 1. Raises: ValueError \u2013 Raises if the sizes of the population are different Returns: list [ IndType ] \u2013 list[IndType]: Source code in digneapy/operators/_replacement.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def elitist_replacement ( current_population : Sequence [ IndType ], offspring : Sequence [ IndType ], hof : int = 1 , ) -> list [ IndType ]: \"\"\"Returns a new population constructed using the Elitist approach. HoF number of individuals from the current + offspring populations are kept in the new population. The remaining individuals are selected from the offspring population. Args: current_population Sequence[IndType],: Current population in the algorithm offspring Sequence[IndType],: Offspring population hof (int, optional): _description_. Defaults to 1. Raises: ValueError: Raises if the sizes of the population are different Returns: list[IndType]: \"\"\" if len ( current_population ) != len ( offspring ): msg = f \"The size of the current population ( { len ( current_population ) } ) != size of the offspring ( { len ( offspring ) } ) in elitist_replacement\" raise ValueError ( msg ) combined_population = sorted ( itertools . chain ( current_population , offspring ), key = attrgetter ( \"fitness\" ), reverse = True , ) top = combined_population [: hof ] return list ( top + offspring [ 1 :]) first_improve_replacement ( current_population , offspring ) Returns a new population produced by a greedy operator. Each individual in the current population is compared with its analogous in the offspring population and the best survives Parameters: current_population ( Sequence [ IndType ] ) \u2013 Current population in the algorithm offspring ( Sequence [ IndType ] ) \u2013 Offspring population Raises: ValueError \u2013 Raises if the sizes of the population are different Returns: Sequence [ IndType ] \u2013 Sequence[IndType]: New population Source code in digneapy/operators/_replacement.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def first_improve_replacement ( current_population : Sequence [ IndType ], offspring : Sequence [ IndType ], ) -> Sequence [ IndType ]: \"\"\"Returns a new population produced by a greedy operator. Each individual in the current population is compared with its analogous in the offspring population and the best survives Args: current_population (Sequence[IndType]): Current population in the algorithm offspring (Sequence[IndType]): Offspring population Raises: ValueError: Raises if the sizes of the population are different Returns: Sequence[IndType]: New population \"\"\" if len ( current_population ) != len ( offspring ): msg = f \"The size of the current population ( { len ( current_population ) } ) != size of the offspring ( { len ( offspring ) } ) in first_improve_replacement\" raise ValueError ( msg ) return [ a if a > b else b for a , b in zip ( current_population , offspring )] generational_replacement ( current_population , offspring ) Returns the offspring population as the new current population Parameters: current_population ( Sequence [ IndType ] ) \u2013 Current population in the algorithm offspring ( Sequence [ IndType ] ) \u2013 Offspring population Raises: ValueError \u2013 Raises if the sizes of the population are different Returns: Sequence [ IndType ] \u2013 Sequence[IndType]: New population Source code in digneapy/operators/_replacement.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def generational_replacement ( current_population : Sequence [ IndType ], offspring : Sequence [ IndType ], ) -> Sequence [ IndType ]: \"\"\"Returns the offspring population as the new current population Args: current_population (Sequence[IndType]): Current population in the algorithm offspring (Sequence[IndType]): Offspring population Raises: ValueError: Raises if the sizes of the population are different Returns: Sequence[IndType]: New population \"\"\" if len ( current_population ) != len ( offspring ): msg = f \"The size of the current population ( { len ( current_population ) } ) != size of the offspring ( { len ( offspring ) } ) in generational replacement\" raise ValueError ( msg ) return offspring [:] one_point_crossover ( individual , other ) One point crossover Parameters: individual Instance or Solution \u2013 First individual to apply crossover. Returned object other Instance or Solution \u2013 Second individual to apply crossover Raises: ValueError \u2013 When the len(ind_1) != len(ind_2) Returns: IndType \u2013 Instance or Solution: New individual Source code in digneapy/operators/_crossover.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def one_point_crossover ( individual : IndType , other : IndType ) -> IndType : \"\"\"One point crossover Args: individual Instance or Solution: First individual to apply crossover. Returned object other Instance or Solution: Second individual to apply crossover Raises: ValueError: When the len(ind_1) != len(ind_2) Returns: Instance or Solution: New individual \"\"\" if len ( individual ) != len ( other ): msg = f \"Individual of different length in uniform_crossover. len(ind) = { len ( individual ) } != len(other) = { len ( other ) } \" raise ValueError ( msg ) offspring = individual . clone () cross_point = np . random . default_rng () . integers ( low = 0 , high = len ( individual )) offspring [ cross_point :] = other [ cross_point :] return offspring uniform_crossover ( individual , other , cxpb = np . float64 ( 0.5 )) Uniform Crossover Operator for Instances and Solutions Parameters: individual ( IndType ) \u2013 First individual to apply crossover. Returned object. other ( IndType ) \u2013 Second individual to apply crossover cxpb ( float64 , default: float64 (0.5) ) \u2013 Crossover probability. Defaults to 0.5. Raises: ValueError \u2013 When the len(ind_1) != len(ind_2) Returns: ndarray ( IndType ) \u2013 New individual Source code in digneapy/operators/_crossover.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def uniform_crossover ( individual : IndType , other : IndType , cxpb : np . float64 = np . float64 ( 0.5 ) ) -> IndType : \"\"\"Uniform Crossover Operator for Instances and Solutions Args: individual (IndType): First individual to apply crossover. Returned object. other (IndType): Second individual to apply crossover cxpb (float64, optional): Crossover probability. Defaults to 0.5. Raises: ValueError: When the len(ind_1) != len(ind_2) Returns: ndarray: New individual \"\"\" if len ( individual ) != len ( other ): msg = f \"Individual of different length in uniform_crossover. len(ind) = { len ( individual ) } != len(other) = { len ( other ) } \" raise ValueError ( msg ) probs = np . random . default_rng () . random ( size = len ( individual )) genotype = np . empty_like ( individual ) genotype = np . where ( probs <= cxpb , individual , other ) return individual . clone_with ( variables = genotype )","title":"Index"},{"location":"reference/operators/#operators.batch_uniform_one_mutation","text":"Performs uniform one mutation in batches Parameters: population ( ndarray ) \u2013 Batch of individuals to mutate lb ( ndarray ) \u2013 Lower bound for each dimension up ( ndarray ) \u2013 Upper bound for each dimension Raises: ValueError \u2013 If the dimension of the individuals do not match the bounds Returns: ndarray \u2013 np.ndarray: mutated population Source code in digneapy/operators/_mutation.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def batch_uniform_one_mutation ( population : np . ndarray , lb : np . ndarray , ub : np . ndarray ) -> np . ndarray : \"\"\"Performs uniform one mutation in batches Args: population (np.ndarray): Batch of individuals to mutate lb (np.ndarray): Lower bound for each dimension up (np.ndarray): Upper bound for each dimension Raises: ValueError: If the dimension of the individuals do not match the bounds Returns: np.ndarray: mutated population \"\"\" dimension = len ( population [ 0 ]) n_individuals = len ( population ) if len ( lb ) != len ( ub ) or dimension != len ( lb ): msg = f \"The size of individuals ( { dimension } ) and bounds { len ( lb ) } is different in uniform_one_mutation\" raise ValueError ( msg ) rng = np . random . default_rng ( 84793258734753 ) mutation_points = rng . integers ( low = 0 , high = dimension , size = n_individuals ) new_values = rng . uniform ( low = lb [ mutation_points ], high = ub [ mutation_points ], size = n_individuals ) population [ np . arange ( n_individuals ), mutation_points ] = new_values return population","title":"batch_uniform_one_mutation"},{"location":"reference/operators/#operators.binary_tournament_selection","text":"Binary Tournament Selection Operator Parameters: population ( Sequence ) \u2013 Population of individuals to select a parent from Raises: RuntimeError \u2013 If the population is empty Returns: IndType \u2013 Instance or Solution: New parent Source code in digneapy/operators/_selection.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def binary_tournament_selection ( population : Sequence [ IndType ]) -> IndType : \"\"\"Binary Tournament Selection Operator Args: population (Sequence): Population of individuals to select a parent from Raises: RuntimeError: If the population is empty Returns: Instance or Solution: New parent \"\"\" if not population : msg = \"Trying to selection individuals in an empty population.\" raise ValueError ( msg ) elif len ( population ) == 1 : return population [ 0 ] else : idx1 , idx2 = np . random . default_rng () . integers ( low = 0 , high = len ( population ), size = 2 ) return max ( population [ idx1 ], population [ idx2 ], key = attrgetter ( \"fitness\" ))","title":"binary_tournament_selection"},{"location":"reference/operators/#operators.elitist_replacement","text":"Returns a new population constructed using the Elitist approach. HoF number of individuals from the current + offspring populations are kept in the new population. The remaining individuals are selected from the offspring population. Parameters: current_population Sequence[IndType], \u2013 Current population in the algorithm offspring Sequence[IndType], \u2013 Offspring population hof ( int , default: 1 ) \u2013 description . Defaults to 1. Raises: ValueError \u2013 Raises if the sizes of the population are different Returns: list [ IndType ] \u2013 list[IndType]: Source code in digneapy/operators/_replacement.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def elitist_replacement ( current_population : Sequence [ IndType ], offspring : Sequence [ IndType ], hof : int = 1 , ) -> list [ IndType ]: \"\"\"Returns a new population constructed using the Elitist approach. HoF number of individuals from the current + offspring populations are kept in the new population. The remaining individuals are selected from the offspring population. Args: current_population Sequence[IndType],: Current population in the algorithm offspring Sequence[IndType],: Offspring population hof (int, optional): _description_. Defaults to 1. Raises: ValueError: Raises if the sizes of the population are different Returns: list[IndType]: \"\"\" if len ( current_population ) != len ( offspring ): msg = f \"The size of the current population ( { len ( current_population ) } ) != size of the offspring ( { len ( offspring ) } ) in elitist_replacement\" raise ValueError ( msg ) combined_population = sorted ( itertools . chain ( current_population , offspring ), key = attrgetter ( \"fitness\" ), reverse = True , ) top = combined_population [: hof ] return list ( top + offspring [ 1 :])","title":"elitist_replacement"},{"location":"reference/operators/#operators.first_improve_replacement","text":"Returns a new population produced by a greedy operator. Each individual in the current population is compared with its analogous in the offspring population and the best survives Parameters: current_population ( Sequence [ IndType ] ) \u2013 Current population in the algorithm offspring ( Sequence [ IndType ] ) \u2013 Offspring population Raises: ValueError \u2013 Raises if the sizes of the population are different Returns: Sequence [ IndType ] \u2013 Sequence[IndType]: New population Source code in digneapy/operators/_replacement.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def first_improve_replacement ( current_population : Sequence [ IndType ], offspring : Sequence [ IndType ], ) -> Sequence [ IndType ]: \"\"\"Returns a new population produced by a greedy operator. Each individual in the current population is compared with its analogous in the offspring population and the best survives Args: current_population (Sequence[IndType]): Current population in the algorithm offspring (Sequence[IndType]): Offspring population Raises: ValueError: Raises if the sizes of the population are different Returns: Sequence[IndType]: New population \"\"\" if len ( current_population ) != len ( offspring ): msg = f \"The size of the current population ( { len ( current_population ) } ) != size of the offspring ( { len ( offspring ) } ) in first_improve_replacement\" raise ValueError ( msg ) return [ a if a > b else b for a , b in zip ( current_population , offspring )]","title":"first_improve_replacement"},{"location":"reference/operators/#operators.generational_replacement","text":"Returns the offspring population as the new current population Parameters: current_population ( Sequence [ IndType ] ) \u2013 Current population in the algorithm offspring ( Sequence [ IndType ] ) \u2013 Offspring population Raises: ValueError \u2013 Raises if the sizes of the population are different Returns: Sequence [ IndType ] \u2013 Sequence[IndType]: New population Source code in digneapy/operators/_replacement.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def generational_replacement ( current_population : Sequence [ IndType ], offspring : Sequence [ IndType ], ) -> Sequence [ IndType ]: \"\"\"Returns the offspring population as the new current population Args: current_population (Sequence[IndType]): Current population in the algorithm offspring (Sequence[IndType]): Offspring population Raises: ValueError: Raises if the sizes of the population are different Returns: Sequence[IndType]: New population \"\"\" if len ( current_population ) != len ( offspring ): msg = f \"The size of the current population ( { len ( current_population ) } ) != size of the offspring ( { len ( offspring ) } ) in generational replacement\" raise ValueError ( msg ) return offspring [:]","title":"generational_replacement"},{"location":"reference/operators/#operators.one_point_crossover","text":"One point crossover Parameters: individual Instance or Solution \u2013 First individual to apply crossover. Returned object other Instance or Solution \u2013 Second individual to apply crossover Raises: ValueError \u2013 When the len(ind_1) != len(ind_2) Returns: IndType \u2013 Instance or Solution: New individual Source code in digneapy/operators/_crossover.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def one_point_crossover ( individual : IndType , other : IndType ) -> IndType : \"\"\"One point crossover Args: individual Instance or Solution: First individual to apply crossover. Returned object other Instance or Solution: Second individual to apply crossover Raises: ValueError: When the len(ind_1) != len(ind_2) Returns: Instance or Solution: New individual \"\"\" if len ( individual ) != len ( other ): msg = f \"Individual of different length in uniform_crossover. len(ind) = { len ( individual ) } != len(other) = { len ( other ) } \" raise ValueError ( msg ) offspring = individual . clone () cross_point = np . random . default_rng () . integers ( low = 0 , high = len ( individual )) offspring [ cross_point :] = other [ cross_point :] return offspring","title":"one_point_crossover"},{"location":"reference/operators/#operators.uniform_crossover","text":"Uniform Crossover Operator for Instances and Solutions Parameters: individual ( IndType ) \u2013 First individual to apply crossover. Returned object. other ( IndType ) \u2013 Second individual to apply crossover cxpb ( float64 , default: float64 (0.5) ) \u2013 Crossover probability. Defaults to 0.5. Raises: ValueError \u2013 When the len(ind_1) != len(ind_2) Returns: ndarray ( IndType ) \u2013 New individual Source code in digneapy/operators/_crossover.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def uniform_crossover ( individual : IndType , other : IndType , cxpb : np . float64 = np . float64 ( 0.5 ) ) -> IndType : \"\"\"Uniform Crossover Operator for Instances and Solutions Args: individual (IndType): First individual to apply crossover. Returned object. other (IndType): Second individual to apply crossover cxpb (float64, optional): Crossover probability. Defaults to 0.5. Raises: ValueError: When the len(ind_1) != len(ind_2) Returns: ndarray: New individual \"\"\" if len ( individual ) != len ( other ): msg = f \"Individual of different length in uniform_crossover. len(ind) = { len ( individual ) } != len(other) = { len ( other ) } \" raise ValueError ( msg ) probs = np . random . default_rng () . random ( size = len ( individual )) genotype = np . empty_like ( individual ) genotype = np . where ( probs <= cxpb , individual , other ) return individual . clone_with ( variables = genotype )","title":"uniform_crossover"},{"location":"reference/operators/_crossover/","text":"@File : crossover.py @Time : 2023/11/03 10:33:32 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2023, Alejandro Marrero @Desc : None one_point_crossover ( individual , other ) One point crossover Parameters: individual Instance or Solution \u2013 First individual to apply crossover. Returned object other Instance or Solution \u2013 Second individual to apply crossover Raises: ValueError \u2013 When the len(ind_1) != len(ind_2) Returns: IndType \u2013 Instance or Solution: New individual Source code in digneapy/operators/_crossover.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def one_point_crossover ( individual : IndType , other : IndType ) -> IndType : \"\"\"One point crossover Args: individual Instance or Solution: First individual to apply crossover. Returned object other Instance or Solution: Second individual to apply crossover Raises: ValueError: When the len(ind_1) != len(ind_2) Returns: Instance or Solution: New individual \"\"\" if len ( individual ) != len ( other ): msg = f \"Individual of different length in uniform_crossover. len(ind) = { len ( individual ) } != len(other) = { len ( other ) } \" raise ValueError ( msg ) offspring = individual . clone () cross_point = np . random . default_rng () . integers ( low = 0 , high = len ( individual )) offspring [ cross_point :] = other [ cross_point :] return offspring uniform_crossover ( individual , other , cxpb = np . float64 ( 0.5 )) Uniform Crossover Operator for Instances and Solutions Parameters: individual ( IndType ) \u2013 First individual to apply crossover. Returned object. other ( IndType ) \u2013 Second individual to apply crossover cxpb ( float64 , default: float64 (0.5) ) \u2013 Crossover probability. Defaults to 0.5. Raises: ValueError \u2013 When the len(ind_1) != len(ind_2) Returns: ndarray ( IndType ) \u2013 New individual Source code in digneapy/operators/_crossover.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def uniform_crossover ( individual : IndType , other : IndType , cxpb : np . float64 = np . float64 ( 0.5 ) ) -> IndType : \"\"\"Uniform Crossover Operator for Instances and Solutions Args: individual (IndType): First individual to apply crossover. Returned object. other (IndType): Second individual to apply crossover cxpb (float64, optional): Crossover probability. Defaults to 0.5. Raises: ValueError: When the len(ind_1) != len(ind_2) Returns: ndarray: New individual \"\"\" if len ( individual ) != len ( other ): msg = f \"Individual of different length in uniform_crossover. len(ind) = { len ( individual ) } != len(other) = { len ( other ) } \" raise ValueError ( msg ) probs = np . random . default_rng () . random ( size = len ( individual )) genotype = np . empty_like ( individual ) genotype = np . where ( probs <= cxpb , individual , other ) return individual . clone_with ( variables = genotype )","title":" crossover"},{"location":"reference/operators/_crossover/#operators._crossover.one_point_crossover","text":"One point crossover Parameters: individual Instance or Solution \u2013 First individual to apply crossover. Returned object other Instance or Solution \u2013 Second individual to apply crossover Raises: ValueError \u2013 When the len(ind_1) != len(ind_2) Returns: IndType \u2013 Instance or Solution: New individual Source code in digneapy/operators/_crossover.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def one_point_crossover ( individual : IndType , other : IndType ) -> IndType : \"\"\"One point crossover Args: individual Instance or Solution: First individual to apply crossover. Returned object other Instance or Solution: Second individual to apply crossover Raises: ValueError: When the len(ind_1) != len(ind_2) Returns: Instance or Solution: New individual \"\"\" if len ( individual ) != len ( other ): msg = f \"Individual of different length in uniform_crossover. len(ind) = { len ( individual ) } != len(other) = { len ( other ) } \" raise ValueError ( msg ) offspring = individual . clone () cross_point = np . random . default_rng () . integers ( low = 0 , high = len ( individual )) offspring [ cross_point :] = other [ cross_point :] return offspring","title":"one_point_crossover"},{"location":"reference/operators/_crossover/#operators._crossover.uniform_crossover","text":"Uniform Crossover Operator for Instances and Solutions Parameters: individual ( IndType ) \u2013 First individual to apply crossover. Returned object. other ( IndType ) \u2013 Second individual to apply crossover cxpb ( float64 , default: float64 (0.5) ) \u2013 Crossover probability. Defaults to 0.5. Raises: ValueError \u2013 When the len(ind_1) != len(ind_2) Returns: ndarray ( IndType ) \u2013 New individual Source code in digneapy/operators/_crossover.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def uniform_crossover ( individual : IndType , other : IndType , cxpb : np . float64 = np . float64 ( 0.5 ) ) -> IndType : \"\"\"Uniform Crossover Operator for Instances and Solutions Args: individual (IndType): First individual to apply crossover. Returned object. other (IndType): Second individual to apply crossover cxpb (float64, optional): Crossover probability. Defaults to 0.5. Raises: ValueError: When the len(ind_1) != len(ind_2) Returns: ndarray: New individual \"\"\" if len ( individual ) != len ( other ): msg = f \"Individual of different length in uniform_crossover. len(ind) = { len ( individual ) } != len(other) = { len ( other ) } \" raise ValueError ( msg ) probs = np . random . default_rng () . random ( size = len ( individual )) genotype = np . empty_like ( individual ) genotype = np . where ( probs <= cxpb , individual , other ) return individual . clone_with ( variables = genotype )","title":"uniform_crossover"},{"location":"reference/operators/_mutation/","text":"@File : mutation.py @Time : 2023/11/03 10:33:30 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2023, Alejandro Marrero @Desc : None batch_uniform_one_mutation ( population , lb , ub ) Performs uniform one mutation in batches Parameters: population ( ndarray ) \u2013 Batch of individuals to mutate lb ( ndarray ) \u2013 Lower bound for each dimension up ( ndarray ) \u2013 Upper bound for each dimension Raises: ValueError \u2013 If the dimension of the individuals do not match the bounds Returns: ndarray \u2013 np.ndarray: mutated population Source code in digneapy/operators/_mutation.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def batch_uniform_one_mutation ( population : np . ndarray , lb : np . ndarray , ub : np . ndarray ) -> np . ndarray : \"\"\"Performs uniform one mutation in batches Args: population (np.ndarray): Batch of individuals to mutate lb (np.ndarray): Lower bound for each dimension up (np.ndarray): Upper bound for each dimension Raises: ValueError: If the dimension of the individuals do not match the bounds Returns: np.ndarray: mutated population \"\"\" dimension = len ( population [ 0 ]) n_individuals = len ( population ) if len ( lb ) != len ( ub ) or dimension != len ( lb ): msg = f \"The size of individuals ( { dimension } ) and bounds { len ( lb ) } is different in uniform_one_mutation\" raise ValueError ( msg ) rng = np . random . default_rng ( 84793258734753 ) mutation_points = rng . integers ( low = 0 , high = dimension , size = n_individuals ) new_values = rng . uniform ( low = lb [ mutation_points ], high = ub [ mutation_points ], size = n_individuals ) population [ np . arange ( n_individuals ), mutation_points ] = new_values return population","title":" mutation"},{"location":"reference/operators/_mutation/#operators._mutation.batch_uniform_one_mutation","text":"Performs uniform one mutation in batches Parameters: population ( ndarray ) \u2013 Batch of individuals to mutate lb ( ndarray ) \u2013 Lower bound for each dimension up ( ndarray ) \u2013 Upper bound for each dimension Raises: ValueError \u2013 If the dimension of the individuals do not match the bounds Returns: ndarray \u2013 np.ndarray: mutated population Source code in digneapy/operators/_mutation.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def batch_uniform_one_mutation ( population : np . ndarray , lb : np . ndarray , ub : np . ndarray ) -> np . ndarray : \"\"\"Performs uniform one mutation in batches Args: population (np.ndarray): Batch of individuals to mutate lb (np.ndarray): Lower bound for each dimension up (np.ndarray): Upper bound for each dimension Raises: ValueError: If the dimension of the individuals do not match the bounds Returns: np.ndarray: mutated population \"\"\" dimension = len ( population [ 0 ]) n_individuals = len ( population ) if len ( lb ) != len ( ub ) or dimension != len ( lb ): msg = f \"The size of individuals ( { dimension } ) and bounds { len ( lb ) } is different in uniform_one_mutation\" raise ValueError ( msg ) rng = np . random . default_rng ( 84793258734753 ) mutation_points = rng . integers ( low = 0 , high = dimension , size = n_individuals ) new_values = rng . uniform ( low = lb [ mutation_points ], high = ub [ mutation_points ], size = n_individuals ) population [ np . arange ( n_individuals ), mutation_points ] = new_values return population","title":"batch_uniform_one_mutation"},{"location":"reference/operators/_replacement/","text":"@File : replacement.py @Time : 2023/11/03 10:33:22 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2023, Alejandro Marrero @Desc : None elitist_replacement ( current_population , offspring , hof = 1 ) Returns a new population constructed using the Elitist approach. HoF number of individuals from the current + offspring populations are kept in the new population. The remaining individuals are selected from the offspring population. Parameters: current_population Sequence[IndType], \u2013 Current population in the algorithm offspring Sequence[IndType], \u2013 Offspring population hof ( int , default: 1 ) \u2013 description . Defaults to 1. Raises: ValueError \u2013 Raises if the sizes of the population are different Returns: list [ IndType ] \u2013 list[IndType]: Source code in digneapy/operators/_replacement.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def elitist_replacement ( current_population : Sequence [ IndType ], offspring : Sequence [ IndType ], hof : int = 1 , ) -> list [ IndType ]: \"\"\"Returns a new population constructed using the Elitist approach. HoF number of individuals from the current + offspring populations are kept in the new population. The remaining individuals are selected from the offspring population. Args: current_population Sequence[IndType],: Current population in the algorithm offspring Sequence[IndType],: Offspring population hof (int, optional): _description_. Defaults to 1. Raises: ValueError: Raises if the sizes of the population are different Returns: list[IndType]: \"\"\" if len ( current_population ) != len ( offspring ): msg = f \"The size of the current population ( { len ( current_population ) } ) != size of the offspring ( { len ( offspring ) } ) in elitist_replacement\" raise ValueError ( msg ) combined_population = sorted ( itertools . chain ( current_population , offspring ), key = attrgetter ( \"fitness\" ), reverse = True , ) top = combined_population [: hof ] return list ( top + offspring [ 1 :]) first_improve_replacement ( current_population , offspring ) Returns a new population produced by a greedy operator. Each individual in the current population is compared with its analogous in the offspring population and the best survives Parameters: current_population ( Sequence [ IndType ] ) \u2013 Current population in the algorithm offspring ( Sequence [ IndType ] ) \u2013 Offspring population Raises: ValueError \u2013 Raises if the sizes of the population are different Returns: Sequence [ IndType ] \u2013 Sequence[IndType]: New population Source code in digneapy/operators/_replacement.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def first_improve_replacement ( current_population : Sequence [ IndType ], offspring : Sequence [ IndType ], ) -> Sequence [ IndType ]: \"\"\"Returns a new population produced by a greedy operator. Each individual in the current population is compared with its analogous in the offspring population and the best survives Args: current_population (Sequence[IndType]): Current population in the algorithm offspring (Sequence[IndType]): Offspring population Raises: ValueError: Raises if the sizes of the population are different Returns: Sequence[IndType]: New population \"\"\" if len ( current_population ) != len ( offspring ): msg = f \"The size of the current population ( { len ( current_population ) } ) != size of the offspring ( { len ( offspring ) } ) in first_improve_replacement\" raise ValueError ( msg ) return [ a if a > b else b for a , b in zip ( current_population , offspring )] generational_replacement ( current_population , offspring ) Returns the offspring population as the new current population Parameters: current_population ( Sequence [ IndType ] ) \u2013 Current population in the algorithm offspring ( Sequence [ IndType ] ) \u2013 Offspring population Raises: ValueError \u2013 Raises if the sizes of the population are different Returns: Sequence [ IndType ] \u2013 Sequence[IndType]: New population Source code in digneapy/operators/_replacement.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def generational_replacement ( current_population : Sequence [ IndType ], offspring : Sequence [ IndType ], ) -> Sequence [ IndType ]: \"\"\"Returns the offspring population as the new current population Args: current_population (Sequence[IndType]): Current population in the algorithm offspring (Sequence[IndType]): Offspring population Raises: ValueError: Raises if the sizes of the population are different Returns: Sequence[IndType]: New population \"\"\" if len ( current_population ) != len ( offspring ): msg = f \"The size of the current population ( { len ( current_population ) } ) != size of the offspring ( { len ( offspring ) } ) in generational replacement\" raise ValueError ( msg ) return offspring [:]","title":" replacement"},{"location":"reference/operators/_replacement/#operators._replacement.elitist_replacement","text":"Returns a new population constructed using the Elitist approach. HoF number of individuals from the current + offspring populations are kept in the new population. The remaining individuals are selected from the offspring population. Parameters: current_population Sequence[IndType], \u2013 Current population in the algorithm offspring Sequence[IndType], \u2013 Offspring population hof ( int , default: 1 ) \u2013 description . Defaults to 1. Raises: ValueError \u2013 Raises if the sizes of the population are different Returns: list [ IndType ] \u2013 list[IndType]: Source code in digneapy/operators/_replacement.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def elitist_replacement ( current_population : Sequence [ IndType ], offspring : Sequence [ IndType ], hof : int = 1 , ) -> list [ IndType ]: \"\"\"Returns a new population constructed using the Elitist approach. HoF number of individuals from the current + offspring populations are kept in the new population. The remaining individuals are selected from the offspring population. Args: current_population Sequence[IndType],: Current population in the algorithm offspring Sequence[IndType],: Offspring population hof (int, optional): _description_. Defaults to 1. Raises: ValueError: Raises if the sizes of the population are different Returns: list[IndType]: \"\"\" if len ( current_population ) != len ( offspring ): msg = f \"The size of the current population ( { len ( current_population ) } ) != size of the offspring ( { len ( offspring ) } ) in elitist_replacement\" raise ValueError ( msg ) combined_population = sorted ( itertools . chain ( current_population , offspring ), key = attrgetter ( \"fitness\" ), reverse = True , ) top = combined_population [: hof ] return list ( top + offspring [ 1 :])","title":"elitist_replacement"},{"location":"reference/operators/_replacement/#operators._replacement.first_improve_replacement","text":"Returns a new population produced by a greedy operator. Each individual in the current population is compared with its analogous in the offspring population and the best survives Parameters: current_population ( Sequence [ IndType ] ) \u2013 Current population in the algorithm offspring ( Sequence [ IndType ] ) \u2013 Offspring population Raises: ValueError \u2013 Raises if the sizes of the population are different Returns: Sequence [ IndType ] \u2013 Sequence[IndType]: New population Source code in digneapy/operators/_replacement.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def first_improve_replacement ( current_population : Sequence [ IndType ], offspring : Sequence [ IndType ], ) -> Sequence [ IndType ]: \"\"\"Returns a new population produced by a greedy operator. Each individual in the current population is compared with its analogous in the offspring population and the best survives Args: current_population (Sequence[IndType]): Current population in the algorithm offspring (Sequence[IndType]): Offspring population Raises: ValueError: Raises if the sizes of the population are different Returns: Sequence[IndType]: New population \"\"\" if len ( current_population ) != len ( offspring ): msg = f \"The size of the current population ( { len ( current_population ) } ) != size of the offspring ( { len ( offspring ) } ) in first_improve_replacement\" raise ValueError ( msg ) return [ a if a > b else b for a , b in zip ( current_population , offspring )]","title":"first_improve_replacement"},{"location":"reference/operators/_replacement/#operators._replacement.generational_replacement","text":"Returns the offspring population as the new current population Parameters: current_population ( Sequence [ IndType ] ) \u2013 Current population in the algorithm offspring ( Sequence [ IndType ] ) \u2013 Offspring population Raises: ValueError \u2013 Raises if the sizes of the population are different Returns: Sequence [ IndType ] \u2013 Sequence[IndType]: New population Source code in digneapy/operators/_replacement.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def generational_replacement ( current_population : Sequence [ IndType ], offspring : Sequence [ IndType ], ) -> Sequence [ IndType ]: \"\"\"Returns the offspring population as the new current population Args: current_population (Sequence[IndType]): Current population in the algorithm offspring (Sequence[IndType]): Offspring population Raises: ValueError: Raises if the sizes of the population are different Returns: Sequence[IndType]: New population \"\"\" if len ( current_population ) != len ( offspring ): msg = f \"The size of the current population ( { len ( current_population ) } ) != size of the offspring ( { len ( offspring ) } ) in generational replacement\" raise ValueError ( msg ) return offspring [:]","title":"generational_replacement"},{"location":"reference/operators/_selection/","text":"@File : selection.py @Time : 2023/11/03 10:33:26 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2023, Alejandro Marrero @Desc : None binary_tournament_selection ( population ) Binary Tournament Selection Operator Parameters: population ( Sequence ) \u2013 Population of individuals to select a parent from Raises: RuntimeError \u2013 If the population is empty Returns: IndType \u2013 Instance or Solution: New parent Source code in digneapy/operators/_selection.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def binary_tournament_selection ( population : Sequence [ IndType ]) -> IndType : \"\"\"Binary Tournament Selection Operator Args: population (Sequence): Population of individuals to select a parent from Raises: RuntimeError: If the population is empty Returns: Instance or Solution: New parent \"\"\" if not population : msg = \"Trying to selection individuals in an empty population.\" raise ValueError ( msg ) elif len ( population ) == 1 : return population [ 0 ] else : idx1 , idx2 = np . random . default_rng () . integers ( low = 0 , high = len ( population ), size = 2 ) return max ( population [ idx1 ], population [ idx2 ], key = attrgetter ( \"fitness\" ))","title":" selection"},{"location":"reference/operators/_selection/#operators._selection.binary_tournament_selection","text":"Binary Tournament Selection Operator Parameters: population ( Sequence ) \u2013 Population of individuals to select a parent from Raises: RuntimeError \u2013 If the population is empty Returns: IndType \u2013 Instance or Solution: New parent Source code in digneapy/operators/_selection.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def binary_tournament_selection ( population : Sequence [ IndType ]) -> IndType : \"\"\"Binary Tournament Selection Operator Args: population (Sequence): Population of individuals to select a parent from Raises: RuntimeError: If the population is empty Returns: Instance or Solution: New parent \"\"\" if not population : msg = \"Trying to selection individuals in an empty population.\" raise ValueError ( msg ) elif len ( population ) == 1 : return population [ 0 ] else : idx1 , idx2 = np . random . default_rng () . integers ( low = 0 , high = len ( population ), size = 2 ) return max ( population [ idx1 ], population [ idx2 ], key = attrgetter ( \"fitness\" ))","title":"binary_tournament_selection"},{"location":"reference/solvers/","text":"@File : init .py @Time : 2023/11/02 12:06:59 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2023, Alejandro Marrero @Desc : None","title":"Index"},{"location":"reference/solvers/bpp/","text":"@File : _bpp_heuristics.py @Time : 2024/06/18 11:54:17 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2024, Alejandro Marrero @Desc : None","title":"Bpp"},{"location":"reference/solvers/evolutionary/","text":"@File : heuristics.py @Time : 2024/4/11 11:14:36 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2024, Alejandro Marrero @Desc : None EA Bases: Solver , SupportsSolve [ P ] , RNG Evolutionary Algorithm from DEAP for digneapy Source code in digneapy/solvers/evolutionary.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 class EA ( Solver , SupportsSolve [ P ], RNG ): \"\"\"Evolutionary Algorithm from DEAP for digneapy\"\"\" def __init__ ( self , direction : Direction , dim : int , min_g : int | float , max_g : int | float , cx = tools . cxUniform , mut = tools . mutUniformInt , pop_size : int = 10 , cxpb : float = 0.6 , mutpb : float = 0.3 , generations : int = 500 , n_cores : int = 1 , seed : int = 42 , ): \"\"\"Creates a new EA instance with the given parameters. Args: dir (str): Direction of the evolution process. Min (minimisation) or Max (maximisation). dim (int): Number of variables of the problem to solve. min_g (int | float): Minimum value of the genome of the solutions. max_g (int | float): Maximum value of the genome of the solutions. pop_size (int, optional): Population size of the evolutionary algorithm. Defaults to 10. cxpb (float, optional): Crossover probability. Defaults to 0.6. mutpb (float, optional): Mutation probability. Defaults to 0.3. generations (int, optional): Number of generations to perform. Defaults to 500. Raises: TypeError: If direction is not in digneapy.solvers.DIRECTIONS \"\"\" if not isinstance ( direction , Direction ): raise TypeError ( f \"Direction not allowed. Please use a value of the class Direction( { Direction . values () } )\" ) self . direction = direction self . _cx = cx self . _mut = mut self . _pop_size = pop_size self . _cxpb = cxpb self . _mutpb = mutpb self . _generations = generations self . _n_cores = n_cores if n_cores > 1 else 1 self . _toolbox = base . Toolbox () self . initialize_rng ( seed = seed ) if direction == Direction . MINIMISE : self . _toolbox . register ( \"individual\" , _gen_dignea_ind , creator . IndMin , self . _rng , dim , min_g , max_g , ) else : self . _toolbox . register ( \"individual\" , _gen_dignea_ind , creator . IndMax , self . _rng , dim , min_g , max_g , ) self . _toolbox . register ( \"population\" , tools . initRepeat , list , self . _toolbox . individual ) self . _toolbox . register ( \"mate\" , cx , indpb = 0.5 ) self . _toolbox . register ( \"mutate\" , mut , low = min_g , up = max_g , indpb = ( 1.0 / dim )) self . _toolbox . register ( \"select\" , tools . selTournament , tournsize = 2 ) self . _stats = tools . Statistics ( key = lambda ind : ind . fitness . values ) self . _stats . register ( \"avg\" , np . mean ) self . _stats . register ( \"std\" , np . std ) self . _stats . register ( \"min\" , np . min ) self . _stats . register ( \"max\" , np . max ) self . _logbook = None self . _best_found = Solution () self . _name = f \"EA_PS_ { self . _pop_size } _CXPB_ { self . _cxpb } _MUTPB_ { self . _mutpb } \" self . __name__ = self . _name if self . _n_cores > 1 : self . _pool = ThreadPoolExecutor ( max_workers = self . _n_cores ) self . _toolbox . register ( \"map\" , self . _pool . map ) def __call__ ( self , problem : P , * args , ** kwargs ) -> list [ Solution ]: \"\"\"Call method of the EA solver. It runs the EA to solve the OptProblem given. Returns: Population (list[Solution]): Final population of the algorithm with the best individual found. \"\"\" if problem is None : msg = \"Problem is None in EA.__call__()\" raise ValueError ( msg ) self . _toolbox . register ( \"evaluate\" , problem ) # Reset the algorithm self . _population = self . _toolbox . population ( n = self . _pop_size ) self . _hof = tools . HallOfFame ( 1 ) self . _logbook = None self . _population , self . _logbook = algorithms . eaSimple ( self . _population , self . _toolbox , cxpb = self . _cxpb , mutpb = self . _mutpb , ngen = self . _generations , halloffame = self . _hof , stats = self . _stats , verbose = False , ) # Convert to Solution class cast_pop = [ Solution ( variables = i , objectives = ( i . fitness . values [ 0 ],), fitness = i . fitness . values [ 0 ], ) for i in self . _population ] self . _population = cast_pop self . _best_found = Solution ( variables = self . _hof [ 0 ], objectives = ( self . _hof [ 0 ] . fitness . values [ 0 ],), fitness = self . _hof [ 0 ] . fitness . values [ 0 ], ) return [ self . _best_found , * cast_pop ] __call__ ( problem , * args , ** kwargs ) Call method of the EA solver. It runs the EA to solve the OptProblem given. Returns: Population ( list [ Solution ] ) \u2013 Final population of the algorithm with the best individual found. Source code in digneapy/solvers/evolutionary.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def __call__ ( self , problem : P , * args , ** kwargs ) -> list [ Solution ]: \"\"\"Call method of the EA solver. It runs the EA to solve the OptProblem given. Returns: Population (list[Solution]): Final population of the algorithm with the best individual found. \"\"\" if problem is None : msg = \"Problem is None in EA.__call__()\" raise ValueError ( msg ) self . _toolbox . register ( \"evaluate\" , problem ) # Reset the algorithm self . _population = self . _toolbox . population ( n = self . _pop_size ) self . _hof = tools . HallOfFame ( 1 ) self . _logbook = None self . _population , self . _logbook = algorithms . eaSimple ( self . _population , self . _toolbox , cxpb = self . _cxpb , mutpb = self . _mutpb , ngen = self . _generations , halloffame = self . _hof , stats = self . _stats , verbose = False , ) # Convert to Solution class cast_pop = [ Solution ( variables = i , objectives = ( i . fitness . values [ 0 ],), fitness = i . fitness . values [ 0 ], ) for i in self . _population ] self . _population = cast_pop self . _best_found = Solution ( variables = self . _hof [ 0 ], objectives = ( self . _hof [ 0 ] . fitness . values [ 0 ],), fitness = self . _hof [ 0 ] . fitness . values [ 0 ], ) return [ self . _best_found , * cast_pop ] __init__ ( direction , dim , min_g , max_g , cx = tools . cxUniform , mut = tools . mutUniformInt , pop_size = 10 , cxpb = 0.6 , mutpb = 0.3 , generations = 500 , n_cores = 1 , seed = 42 ) Creates a new EA instance with the given parameters. Args: dir (str): Direction of the evolution process. Min (minimisation) or Max (maximisation). dim (int): Number of variables of the problem to solve. min_g (int | float): Minimum value of the genome of the solutions. max_g (int | float): Maximum value of the genome of the solutions. pop_size (int, optional): Population size of the evolutionary algorithm. Defaults to 10. cxpb (float, optional): Crossover probability. Defaults to 0.6. mutpb (float, optional): Mutation probability. Defaults to 0.3. generations (int, optional): Number of generations to perform. Defaults to 500. Raises: TypeError \u2013 If direction is not in digneapy.solvers.DIRECTIONS Source code in digneapy/solvers/evolutionary.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def __init__ ( self , direction : Direction , dim : int , min_g : int | float , max_g : int | float , cx = tools . cxUniform , mut = tools . mutUniformInt , pop_size : int = 10 , cxpb : float = 0.6 , mutpb : float = 0.3 , generations : int = 500 , n_cores : int = 1 , seed : int = 42 , ): \"\"\"Creates a new EA instance with the given parameters. Args: dir (str): Direction of the evolution process. Min (minimisation) or Max (maximisation). dim (int): Number of variables of the problem to solve. min_g (int | float): Minimum value of the genome of the solutions. max_g (int | float): Maximum value of the genome of the solutions. pop_size (int, optional): Population size of the evolutionary algorithm. Defaults to 10. cxpb (float, optional): Crossover probability. Defaults to 0.6. mutpb (float, optional): Mutation probability. Defaults to 0.3. generations (int, optional): Number of generations to perform. Defaults to 500. Raises: TypeError: If direction is not in digneapy.solvers.DIRECTIONS \"\"\" if not isinstance ( direction , Direction ): raise TypeError ( f \"Direction not allowed. Please use a value of the class Direction( { Direction . values () } )\" ) self . direction = direction self . _cx = cx self . _mut = mut self . _pop_size = pop_size self . _cxpb = cxpb self . _mutpb = mutpb self . _generations = generations self . _n_cores = n_cores if n_cores > 1 else 1 self . _toolbox = base . Toolbox () self . initialize_rng ( seed = seed ) if direction == Direction . MINIMISE : self . _toolbox . register ( \"individual\" , _gen_dignea_ind , creator . IndMin , self . _rng , dim , min_g , max_g , ) else : self . _toolbox . register ( \"individual\" , _gen_dignea_ind , creator . IndMax , self . _rng , dim , min_g , max_g , ) self . _toolbox . register ( \"population\" , tools . initRepeat , list , self . _toolbox . individual ) self . _toolbox . register ( \"mate\" , cx , indpb = 0.5 ) self . _toolbox . register ( \"mutate\" , mut , low = min_g , up = max_g , indpb = ( 1.0 / dim )) self . _toolbox . register ( \"select\" , tools . selTournament , tournsize = 2 ) self . _stats = tools . Statistics ( key = lambda ind : ind . fitness . values ) self . _stats . register ( \"avg\" , np . mean ) self . _stats . register ( \"std\" , np . std ) self . _stats . register ( \"min\" , np . min ) self . _stats . register ( \"max\" , np . max ) self . _logbook = None self . _best_found = Solution () self . _name = f \"EA_PS_ { self . _pop_size } _CXPB_ { self . _cxpb } _MUTPB_ { self . _mutpb } \" self . __name__ = self . _name if self . _n_cores > 1 : self . _pool = ThreadPoolExecutor ( max_workers = self . _n_cores ) self . _toolbox . register ( \"map\" , self . _pool . map )","title":"Evolutionary"},{"location":"reference/solvers/evolutionary/#solvers.evolutionary.EA","text":"Bases: Solver , SupportsSolve [ P ] , RNG Evolutionary Algorithm from DEAP for digneapy Source code in digneapy/solvers/evolutionary.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 class EA ( Solver , SupportsSolve [ P ], RNG ): \"\"\"Evolutionary Algorithm from DEAP for digneapy\"\"\" def __init__ ( self , direction : Direction , dim : int , min_g : int | float , max_g : int | float , cx = tools . cxUniform , mut = tools . mutUniformInt , pop_size : int = 10 , cxpb : float = 0.6 , mutpb : float = 0.3 , generations : int = 500 , n_cores : int = 1 , seed : int = 42 , ): \"\"\"Creates a new EA instance with the given parameters. Args: dir (str): Direction of the evolution process. Min (minimisation) or Max (maximisation). dim (int): Number of variables of the problem to solve. min_g (int | float): Minimum value of the genome of the solutions. max_g (int | float): Maximum value of the genome of the solutions. pop_size (int, optional): Population size of the evolutionary algorithm. Defaults to 10. cxpb (float, optional): Crossover probability. Defaults to 0.6. mutpb (float, optional): Mutation probability. Defaults to 0.3. generations (int, optional): Number of generations to perform. Defaults to 500. Raises: TypeError: If direction is not in digneapy.solvers.DIRECTIONS \"\"\" if not isinstance ( direction , Direction ): raise TypeError ( f \"Direction not allowed. Please use a value of the class Direction( { Direction . values () } )\" ) self . direction = direction self . _cx = cx self . _mut = mut self . _pop_size = pop_size self . _cxpb = cxpb self . _mutpb = mutpb self . _generations = generations self . _n_cores = n_cores if n_cores > 1 else 1 self . _toolbox = base . Toolbox () self . initialize_rng ( seed = seed ) if direction == Direction . MINIMISE : self . _toolbox . register ( \"individual\" , _gen_dignea_ind , creator . IndMin , self . _rng , dim , min_g , max_g , ) else : self . _toolbox . register ( \"individual\" , _gen_dignea_ind , creator . IndMax , self . _rng , dim , min_g , max_g , ) self . _toolbox . register ( \"population\" , tools . initRepeat , list , self . _toolbox . individual ) self . _toolbox . register ( \"mate\" , cx , indpb = 0.5 ) self . _toolbox . register ( \"mutate\" , mut , low = min_g , up = max_g , indpb = ( 1.0 / dim )) self . _toolbox . register ( \"select\" , tools . selTournament , tournsize = 2 ) self . _stats = tools . Statistics ( key = lambda ind : ind . fitness . values ) self . _stats . register ( \"avg\" , np . mean ) self . _stats . register ( \"std\" , np . std ) self . _stats . register ( \"min\" , np . min ) self . _stats . register ( \"max\" , np . max ) self . _logbook = None self . _best_found = Solution () self . _name = f \"EA_PS_ { self . _pop_size } _CXPB_ { self . _cxpb } _MUTPB_ { self . _mutpb } \" self . __name__ = self . _name if self . _n_cores > 1 : self . _pool = ThreadPoolExecutor ( max_workers = self . _n_cores ) self . _toolbox . register ( \"map\" , self . _pool . map ) def __call__ ( self , problem : P , * args , ** kwargs ) -> list [ Solution ]: \"\"\"Call method of the EA solver. It runs the EA to solve the OptProblem given. Returns: Population (list[Solution]): Final population of the algorithm with the best individual found. \"\"\" if problem is None : msg = \"Problem is None in EA.__call__()\" raise ValueError ( msg ) self . _toolbox . register ( \"evaluate\" , problem ) # Reset the algorithm self . _population = self . _toolbox . population ( n = self . _pop_size ) self . _hof = tools . HallOfFame ( 1 ) self . _logbook = None self . _population , self . _logbook = algorithms . eaSimple ( self . _population , self . _toolbox , cxpb = self . _cxpb , mutpb = self . _mutpb , ngen = self . _generations , halloffame = self . _hof , stats = self . _stats , verbose = False , ) # Convert to Solution class cast_pop = [ Solution ( variables = i , objectives = ( i . fitness . values [ 0 ],), fitness = i . fitness . values [ 0 ], ) for i in self . _population ] self . _population = cast_pop self . _best_found = Solution ( variables = self . _hof [ 0 ], objectives = ( self . _hof [ 0 ] . fitness . values [ 0 ],), fitness = self . _hof [ 0 ] . fitness . values [ 0 ], ) return [ self . _best_found , * cast_pop ]","title":"EA"},{"location":"reference/solvers/evolutionary/#solvers.evolutionary.EA.__call__","text":"Call method of the EA solver. It runs the EA to solve the OptProblem given. Returns: Population ( list [ Solution ] ) \u2013 Final population of the algorithm with the best individual found. Source code in digneapy/solvers/evolutionary.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def __call__ ( self , problem : P , * args , ** kwargs ) -> list [ Solution ]: \"\"\"Call method of the EA solver. It runs the EA to solve the OptProblem given. Returns: Population (list[Solution]): Final population of the algorithm with the best individual found. \"\"\" if problem is None : msg = \"Problem is None in EA.__call__()\" raise ValueError ( msg ) self . _toolbox . register ( \"evaluate\" , problem ) # Reset the algorithm self . _population = self . _toolbox . population ( n = self . _pop_size ) self . _hof = tools . HallOfFame ( 1 ) self . _logbook = None self . _population , self . _logbook = algorithms . eaSimple ( self . _population , self . _toolbox , cxpb = self . _cxpb , mutpb = self . _mutpb , ngen = self . _generations , halloffame = self . _hof , stats = self . _stats , verbose = False , ) # Convert to Solution class cast_pop = [ Solution ( variables = i , objectives = ( i . fitness . values [ 0 ],), fitness = i . fitness . values [ 0 ], ) for i in self . _population ] self . _population = cast_pop self . _best_found = Solution ( variables = self . _hof [ 0 ], objectives = ( self . _hof [ 0 ] . fitness . values [ 0 ],), fitness = self . _hof [ 0 ] . fitness . values [ 0 ], ) return [ self . _best_found , * cast_pop ]","title":"__call__"},{"location":"reference/solvers/evolutionary/#solvers.evolutionary.EA.__init__","text":"Creates a new EA instance with the given parameters. Args: dir (str): Direction of the evolution process. Min (minimisation) or Max (maximisation). dim (int): Number of variables of the problem to solve. min_g (int | float): Minimum value of the genome of the solutions. max_g (int | float): Maximum value of the genome of the solutions. pop_size (int, optional): Population size of the evolutionary algorithm. Defaults to 10. cxpb (float, optional): Crossover probability. Defaults to 0.6. mutpb (float, optional): Mutation probability. Defaults to 0.3. generations (int, optional): Number of generations to perform. Defaults to 500. Raises: TypeError \u2013 If direction is not in digneapy.solvers.DIRECTIONS Source code in digneapy/solvers/evolutionary.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def __init__ ( self , direction : Direction , dim : int , min_g : int | float , max_g : int | float , cx = tools . cxUniform , mut = tools . mutUniformInt , pop_size : int = 10 , cxpb : float = 0.6 , mutpb : float = 0.3 , generations : int = 500 , n_cores : int = 1 , seed : int = 42 , ): \"\"\"Creates a new EA instance with the given parameters. Args: dir (str): Direction of the evolution process. Min (minimisation) or Max (maximisation). dim (int): Number of variables of the problem to solve. min_g (int | float): Minimum value of the genome of the solutions. max_g (int | float): Maximum value of the genome of the solutions. pop_size (int, optional): Population size of the evolutionary algorithm. Defaults to 10. cxpb (float, optional): Crossover probability. Defaults to 0.6. mutpb (float, optional): Mutation probability. Defaults to 0.3. generations (int, optional): Number of generations to perform. Defaults to 500. Raises: TypeError: If direction is not in digneapy.solvers.DIRECTIONS \"\"\" if not isinstance ( direction , Direction ): raise TypeError ( f \"Direction not allowed. Please use a value of the class Direction( { Direction . values () } )\" ) self . direction = direction self . _cx = cx self . _mut = mut self . _pop_size = pop_size self . _cxpb = cxpb self . _mutpb = mutpb self . _generations = generations self . _n_cores = n_cores if n_cores > 1 else 1 self . _toolbox = base . Toolbox () self . initialize_rng ( seed = seed ) if direction == Direction . MINIMISE : self . _toolbox . register ( \"individual\" , _gen_dignea_ind , creator . IndMin , self . _rng , dim , min_g , max_g , ) else : self . _toolbox . register ( \"individual\" , _gen_dignea_ind , creator . IndMax , self . _rng , dim , min_g , max_g , ) self . _toolbox . register ( \"population\" , tools . initRepeat , list , self . _toolbox . individual ) self . _toolbox . register ( \"mate\" , cx , indpb = 0.5 ) self . _toolbox . register ( \"mutate\" , mut , low = min_g , up = max_g , indpb = ( 1.0 / dim )) self . _toolbox . register ( \"select\" , tools . selTournament , tournsize = 2 ) self . _stats = tools . Statistics ( key = lambda ind : ind . fitness . values ) self . _stats . register ( \"avg\" , np . mean ) self . _stats . register ( \"std\" , np . std ) self . _stats . register ( \"min\" , np . min ) self . _stats . register ( \"max\" , np . max ) self . _logbook = None self . _best_found = Solution () self . _name = f \"EA_PS_ { self . _pop_size } _CXPB_ { self . _cxpb } _MUTPB_ { self . _mutpb } \" self . __name__ = self . _name if self . _n_cores > 1 : self . _pool = ThreadPoolExecutor ( max_workers = self . _n_cores ) self . _toolbox . register ( \"map\" , self . _pool . map )","title":"__init__"},{"location":"reference/solvers/pisinger/","text":"@File : pisinger.py @Time : 2024/09/17 12:40:27 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2024, Alejandro Marrero @Desc : None","title":"Pisinger"},{"location":"reference/solvers/tsp/","text":"@File : tsp.py @Time : 2025/03/05 09:32:55 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2025, Alejandro Marrero @Desc : None greedy ( problem , * args , ** kwargs ) The Greedy heuristic gradually constructs a tour by repeatedly selecting the shortest edge and adding it to the tour as long as it doesn\u2019t create a cycle with less than N edges, or increases the degree of any node to more than 2. We must not add the same edge twice of course. Args: problem (TSP): Problem to solve Raises: ValueError: If problem is None Returns: list[Solution]: Collection of solutions for the given problem Source code in digneapy/solvers/tsp.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def greedy ( problem : TSP , * args , ** kwargs ) -> list [ Solution ]: \"\"\"The Greedy heuristic gradually constructs a tour by repeatedly selecting the shortest edge and adding it to the tour as long as it doesn\u2019t create a cycle with less than N edges, or increases the degree of any node to more than 2. We must not add the same edge twice of course. Args: problem (TSP): Problem to solve Raises: ValueError: If problem is None Returns: list[Solution]: Collection of solutions for the given problem \"\"\" if problem is None : raise ValueError ( \"No problem found in two_opt heuristic\" ) N = problem . dimension distances = problem . _distances counter = Counter () selected : set [ tuple [ int , int ]] = set () ordered_edges = sorted ( [( distances [ i ][ j ], i , j ) for i in range ( N ) for j in range ( i + 1 , N )] ) length = 0.0 for dist , i , j in ordered_edges : if ( i , j ) in selected or ( j , i ) in selected : continue if counter [ i ] >= 2 or counter [ j ] >= 2 : continue selected . add (( i , j )) counter [ i ] += 1 counter [ j ] += 1 length += dist if len ( selected ) == N : break _fitness = 1.0 / length return [ Solution ( variables = list ( range ( N + 1 )), objectives = ( _fitness ,), fitness = _fitness ) ] nneighbour ( problem , * args , ** kwargs ) Nearest-Neighbour Heuristic for the Travelling Salesman Problem Parameters: problem ( TSP ) \u2013 Problem to solve Raises: ValueError \u2013 If problem is None Returns: list [ Solution ] \u2013 list[Solution]: Collection of solutions to the problem. Source code in digneapy/solvers/tsp.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def nneighbour ( problem : TSP , * args , ** kwargs ) -> list [ Solution ]: \"\"\"Nearest-Neighbour Heuristic for the Travelling Salesman Problem Args: problem (TSP): Problem to solve Raises: ValueError: If problem is None Returns: list[Solution]: Collection of solutions to the problem. \"\"\" if problem is None : raise ValueError ( \"No problem found in nneighbour heuristic\" ) distances = problem . _distances current_node = 0 visited_nodes : set [ int ] = set ([ current_node ]) tour = np . zeros ( problem . dimension + 1 ) length = np . float32 ( 0 ) idx = 1 while len ( visited_nodes ) != problem . dimension : next_node = 0 min_distance = np . finfo ( np . float32 ) . max for j in range ( problem . dimension ): if j not in visited_nodes and distances [ current_node ][ j ] < min_distance : min_distance = distances [ current_node ][ j ] next_node = j visited_nodes . add ( next_node ) tour [ idx ] = next_node idx += 1 length += min_distance current_node = next_node length += distances [ current_node ][ 0 ] length = 1.0 / length return [ Solution ( variables = tour , objectives = ( length ,), fitness = length )] three_opt ( problem , * args , ** kwargs ) 3-Opt Heuristic for the Travelling Salesman Problem Parameters: problem ( TSP ) \u2013 Problem to solve Raises: ValueError \u2013 If problem is None Returns: list [ Solution ] \u2013 list[Solution]: Collection of solutions to the problem Source code in digneapy/solvers/tsp.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def three_opt ( problem : TSP , * args , ** kwargs ) -> list [ Solution ]: \"\"\"3-Opt Heuristic for the Travelling Salesman Problem Args: problem (TSP): Problem to solve Raises: ValueError: If problem is None Returns: list[Solution]: Collection of solutions to the problem \"\"\" if problem is None : raise ValueError ( \"No problem found in three_opt heuristic\" ) distances = problem . _distances N = problem . dimension tour = np . arange ( start = 0 , stop = N + 1 , step = 1 , dtype = int ) tour [ - 1 ] = 0 improve = True while improve : improve = False for i in range ( 1 , N - 2 ): for j in range ( i + 2 , N - 1 ): for k in range ( j + 2 , N ): new_tour = np . concatenate ( ( tour [: i ], tour [ j : k ][:: - 1 ], tour [ i : j ], tour [ k :]) ) current = ( distances [ tour [ i - 1 ]][ tour [ i ]] + distances [ tour [ j - 1 ]][ tour [ j ]] + distances [ tour [ k - 1 ]][ tour [ k ]] ) newer = ( distances [ new_tour [ - 2 ]][ new_tour [ - 1 ]] + distances [ new_tour [ 0 ]][ new_tour [ 1 ]] ) if newer < current : tour = new_tour improve = True fitness = problem . evaluate ( tour )[ 0 ] return [ Solution ( variables = tour , objectives = ( fitness ,), fitness = fitness )]","title":"Tsp"},{"location":"reference/solvers/tsp/#solvers.tsp.greedy","text":"The Greedy heuristic gradually constructs a tour by repeatedly selecting the shortest edge and adding it to the tour as long as it doesn\u2019t create a cycle with less than N edges, or increases the degree of any node to more than 2. We must not add the same edge twice of course. Args: problem (TSP): Problem to solve Raises: ValueError: If problem is None Returns: list[Solution]: Collection of solutions for the given problem Source code in digneapy/solvers/tsp.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def greedy ( problem : TSP , * args , ** kwargs ) -> list [ Solution ]: \"\"\"The Greedy heuristic gradually constructs a tour by repeatedly selecting the shortest edge and adding it to the tour as long as it doesn\u2019t create a cycle with less than N edges, or increases the degree of any node to more than 2. We must not add the same edge twice of course. Args: problem (TSP): Problem to solve Raises: ValueError: If problem is None Returns: list[Solution]: Collection of solutions for the given problem \"\"\" if problem is None : raise ValueError ( \"No problem found in two_opt heuristic\" ) N = problem . dimension distances = problem . _distances counter = Counter () selected : set [ tuple [ int , int ]] = set () ordered_edges = sorted ( [( distances [ i ][ j ], i , j ) for i in range ( N ) for j in range ( i + 1 , N )] ) length = 0.0 for dist , i , j in ordered_edges : if ( i , j ) in selected or ( j , i ) in selected : continue if counter [ i ] >= 2 or counter [ j ] >= 2 : continue selected . add (( i , j )) counter [ i ] += 1 counter [ j ] += 1 length += dist if len ( selected ) == N : break _fitness = 1.0 / length return [ Solution ( variables = list ( range ( N + 1 )), objectives = ( _fitness ,), fitness = _fitness ) ]","title":"greedy"},{"location":"reference/solvers/tsp/#solvers.tsp.nneighbour","text":"Nearest-Neighbour Heuristic for the Travelling Salesman Problem Parameters: problem ( TSP ) \u2013 Problem to solve Raises: ValueError \u2013 If problem is None Returns: list [ Solution ] \u2013 list[Solution]: Collection of solutions to the problem. Source code in digneapy/solvers/tsp.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def nneighbour ( problem : TSP , * args , ** kwargs ) -> list [ Solution ]: \"\"\"Nearest-Neighbour Heuristic for the Travelling Salesman Problem Args: problem (TSP): Problem to solve Raises: ValueError: If problem is None Returns: list[Solution]: Collection of solutions to the problem. \"\"\" if problem is None : raise ValueError ( \"No problem found in nneighbour heuristic\" ) distances = problem . _distances current_node = 0 visited_nodes : set [ int ] = set ([ current_node ]) tour = np . zeros ( problem . dimension + 1 ) length = np . float32 ( 0 ) idx = 1 while len ( visited_nodes ) != problem . dimension : next_node = 0 min_distance = np . finfo ( np . float32 ) . max for j in range ( problem . dimension ): if j not in visited_nodes and distances [ current_node ][ j ] < min_distance : min_distance = distances [ current_node ][ j ] next_node = j visited_nodes . add ( next_node ) tour [ idx ] = next_node idx += 1 length += min_distance current_node = next_node length += distances [ current_node ][ 0 ] length = 1.0 / length return [ Solution ( variables = tour , objectives = ( length ,), fitness = length )]","title":"nneighbour"},{"location":"reference/solvers/tsp/#solvers.tsp.three_opt","text":"3-Opt Heuristic for the Travelling Salesman Problem Parameters: problem ( TSP ) \u2013 Problem to solve Raises: ValueError \u2013 If problem is None Returns: list [ Solution ] \u2013 list[Solution]: Collection of solutions to the problem Source code in digneapy/solvers/tsp.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def three_opt ( problem : TSP , * args , ** kwargs ) -> list [ Solution ]: \"\"\"3-Opt Heuristic for the Travelling Salesman Problem Args: problem (TSP): Problem to solve Raises: ValueError: If problem is None Returns: list[Solution]: Collection of solutions to the problem \"\"\" if problem is None : raise ValueError ( \"No problem found in three_opt heuristic\" ) distances = problem . _distances N = problem . dimension tour = np . arange ( start = 0 , stop = N + 1 , step = 1 , dtype = int ) tour [ - 1 ] = 0 improve = True while improve : improve = False for i in range ( 1 , N - 2 ): for j in range ( i + 2 , N - 1 ): for k in range ( j + 2 , N ): new_tour = np . concatenate ( ( tour [: i ], tour [ j : k ][:: - 1 ], tour [ i : j ], tour [ k :]) ) current = ( distances [ tour [ i - 1 ]][ tour [ i ]] + distances [ tour [ j - 1 ]][ tour [ j ]] + distances [ tour [ k - 1 ]][ tour [ k ]] ) newer = ( distances [ new_tour [ - 2 ]][ new_tour [ - 1 ]] + distances [ new_tour [ 0 ]][ new_tour [ 1 ]] ) if newer < current : tour = new_tour improve = True fitness = problem . evaluate ( tour )[ 0 ] return [ Solution ( variables = tour , objectives = ( fitness ,), fitness = fitness )]","title":"three_opt"},{"location":"reference/solvers/utils/","text":"@File : special_cases.py @Time : 2025/10/14 14:41:38 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2025, Alejandro Marrero @Desc : None","title":"Utils"},{"location":"reference/transformers/","text":"@File : init .py @Time : 2024/06/07 13:56:21 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2024, Alejandro Marrero @Desc : None SupportsTransform Bases: Protocol Protocol to type check all the transformer types in digneapy. A Transformer is any callable type that receives a sequence and transforms it to other sequence. Source code in digneapy/transformers/_base.py 20 21 22 23 24 25 26 class SupportsTransform ( Protocol ): \"\"\"Protocol to type check all the transformer types in digneapy. A Transformer is any callable type that receives a sequence and transforms it to other sequence. \"\"\" def __call__ ( self , x : npt . NDArray ) -> np . ndarray : ... Transformer Bases: ABC , SupportsTransform Transformer is any callable type that receives a sequence and transforms it to other sequence. Ussually, the transformer is a model that is trained to transform the input data to a new space. The transformer is a subclass of the SupportsTransform protocol. Source code in digneapy/transformers/_base.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 class Transformer ( ABC , SupportsTransform ): \"\"\"Transformer is any callable type that receives a sequence and transforms it to other sequence. Ussually, the transformer is a model that is trained to transform the input data to a new space. The transformer is a subclass of the SupportsTransform protocol. \"\"\" def __init__ ( self , name : str ): self . _name = name def train ( self , x : npt . NDArray ): raise NotImplementedError ( \"train method not implemented in Transformer\" ) def predict ( self , x : npt . NDArray ) -> np . ndarray : raise NotImplementedError ( \"predict method not implemented in Transformer\" ) @abstractmethod def __call__ ( self , x : npt . NDArray ) -> np . ndarray : raise NotImplementedError ( \"__call__ method not implemented in Transformer\" ) def save ( self ): raise NotImplementedError ( \"save method not implemented in Transformer\" )","title":"Index"},{"location":"reference/transformers/#transformers.SupportsTransform","text":"Bases: Protocol Protocol to type check all the transformer types in digneapy. A Transformer is any callable type that receives a sequence and transforms it to other sequence. Source code in digneapy/transformers/_base.py 20 21 22 23 24 25 26 class SupportsTransform ( Protocol ): \"\"\"Protocol to type check all the transformer types in digneapy. A Transformer is any callable type that receives a sequence and transforms it to other sequence. \"\"\" def __call__ ( self , x : npt . NDArray ) -> np . ndarray : ...","title":"SupportsTransform"},{"location":"reference/transformers/#transformers.Transformer","text":"Bases: ABC , SupportsTransform Transformer is any callable type that receives a sequence and transforms it to other sequence. Ussually, the transformer is a model that is trained to transform the input data to a new space. The transformer is a subclass of the SupportsTransform protocol. Source code in digneapy/transformers/_base.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 class Transformer ( ABC , SupportsTransform ): \"\"\"Transformer is any callable type that receives a sequence and transforms it to other sequence. Ussually, the transformer is a model that is trained to transform the input data to a new space. The transformer is a subclass of the SupportsTransform protocol. \"\"\" def __init__ ( self , name : str ): self . _name = name def train ( self , x : npt . NDArray ): raise NotImplementedError ( \"train method not implemented in Transformer\" ) def predict ( self , x : npt . NDArray ) -> np . ndarray : raise NotImplementedError ( \"predict method not implemented in Transformer\" ) @abstractmethod def __call__ ( self , x : npt . NDArray ) -> np . ndarray : raise NotImplementedError ( \"__call__ method not implemented in Transformer\" ) def save ( self ): raise NotImplementedError ( \"save method not implemented in Transformer\" )","title":"Transformer"},{"location":"reference/transformers/_base/","text":"@File : transformers.py @Time : 2023/11/15 08:51:42 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2023, Alejandro Marrero @Desc : None SupportsTransform Bases: Protocol Protocol to type check all the transformer types in digneapy. A Transformer is any callable type that receives a sequence and transforms it to other sequence. Source code in digneapy/transformers/_base.py 20 21 22 23 24 25 26 class SupportsTransform ( Protocol ): \"\"\"Protocol to type check all the transformer types in digneapy. A Transformer is any callable type that receives a sequence and transforms it to other sequence. \"\"\" def __call__ ( self , x : npt . NDArray ) -> np . ndarray : ... Transformer Bases: ABC , SupportsTransform Transformer is any callable type that receives a sequence and transforms it to other sequence. Ussually, the transformer is a model that is trained to transform the input data to a new space. The transformer is a subclass of the SupportsTransform protocol. Source code in digneapy/transformers/_base.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 class Transformer ( ABC , SupportsTransform ): \"\"\"Transformer is any callable type that receives a sequence and transforms it to other sequence. Ussually, the transformer is a model that is trained to transform the input data to a new space. The transformer is a subclass of the SupportsTransform protocol. \"\"\" def __init__ ( self , name : str ): self . _name = name def train ( self , x : npt . NDArray ): raise NotImplementedError ( \"train method not implemented in Transformer\" ) def predict ( self , x : npt . NDArray ) -> np . ndarray : raise NotImplementedError ( \"predict method not implemented in Transformer\" ) @abstractmethod def __call__ ( self , x : npt . NDArray ) -> np . ndarray : raise NotImplementedError ( \"__call__ method not implemented in Transformer\" ) def save ( self ): raise NotImplementedError ( \"save method not implemented in Transformer\" )","title":" base"},{"location":"reference/transformers/_base/#transformers._base.SupportsTransform","text":"Bases: Protocol Protocol to type check all the transformer types in digneapy. A Transformer is any callable type that receives a sequence and transforms it to other sequence. Source code in digneapy/transformers/_base.py 20 21 22 23 24 25 26 class SupportsTransform ( Protocol ): \"\"\"Protocol to type check all the transformer types in digneapy. A Transformer is any callable type that receives a sequence and transforms it to other sequence. \"\"\" def __call__ ( self , x : npt . NDArray ) -> np . ndarray : ...","title":"SupportsTransform"},{"location":"reference/transformers/_base/#transformers._base.Transformer","text":"Bases: ABC , SupportsTransform Transformer is any callable type that receives a sequence and transforms it to other sequence. Ussually, the transformer is a model that is trained to transform the input data to a new space. The transformer is a subclass of the SupportsTransform protocol. Source code in digneapy/transformers/_base.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 class Transformer ( ABC , SupportsTransform ): \"\"\"Transformer is any callable type that receives a sequence and transforms it to other sequence. Ussually, the transformer is a model that is trained to transform the input data to a new space. The transformer is a subclass of the SupportsTransform protocol. \"\"\" def __init__ ( self , name : str ): self . _name = name def train ( self , x : npt . NDArray ): raise NotImplementedError ( \"train method not implemented in Transformer\" ) def predict ( self , x : npt . NDArray ) -> np . ndarray : raise NotImplementedError ( \"predict method not implemented in Transformer\" ) @abstractmethod def __call__ ( self , x : npt . NDArray ) -> np . ndarray : raise NotImplementedError ( \"__call__ method not implemented in Transformer\" ) def save ( self ): raise NotImplementedError ( \"save method not implemented in Transformer\" )","title":"Transformer"},{"location":"reference/transformers/autoencoders/","text":"@File : autoencoders.py @Time : 2024/05/28 10:06:48 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2024, Alejandro Marrero @Desc : None KPDecoder Bases: Transformer Source code in digneapy/transformers/autoencoders.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 class KPDecoder ( Transformer ): def __init__ ( self , name : str = \"KPDecoder\" , scale_method : str = \"learnt\" ): super () . __init__ ( name ) if scale_method not in ( \"learnt\" , \"sample\" ): raise ValueError ( \"KPDecoder expects the scale method to be either learnt or sample\" ) self . _scale_method = scale_method self . __scales_fname = \"scales_knapsack_N_50.h5\" self . _expected_latent_dim = 2 self . _decoder = torch . jit . load ( MODELS_PATH / AUTOENCODER_NAME , map_location = torch . device ( DEVICE ) ) with h5py . File ( MODELS_PATH / self . __scales_fname , \"r\" ) as file : self . _max_weights = file [ \"scales\" ][ \"max_weights\" ][:] . astype ( np . int32 ) self . _max_profits = file [ \"scales\" ][ \"max_profits\" ][:] . astype ( np . int32 ) self . _sum_of_weights = file [ \"scales\" ][ \"sum_of_weights\" ][:] . astype ( np . int32 ) if self . _scale_method == \"sample\" : self . _weights_fitted_dist = lognorm . fit ( self . _max_weights , floc = 0 ) self . _profits_fitted_dist = lognorm . fit ( self . _max_profits , floc = 0 ) self . _capacity_fitted_dist = lognorm . fit ( self . _sum_of_weights , floc = 0 ) @property def output_dimension ( self ) -> int : return 101 def __sample_scaling_factors ( self , size : int ) -> Tuple [ Any , Any , Any ]: return ( lognorm . rvs ( self . _weights_fitted_dist [ 0 ], loc = self . _weights_fitted_dist [ 1 ], scale = self . _weights_fitted_dist [ 2 ], size = size , )[:, None ], lognorm . rvs ( self . _profits_fitted_dist [ 0 ], loc = self . _profits_fitted_dist [ 1 ], scale = self . _profits_fitted_dist [ 2 ], size = size , )[:, None ], lognorm . rvs ( self . _capacity_fitted_dist [ 0 ], loc = self . _capacity_fitted_dist [ 1 ], scale = self . _capacity_fitted_dist [ 2 ], size = size , )[:, None ], ) def __scaling_from_training ( self , size : int ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray ]: indexes = np . random . randint ( low = 0 , high = len ( self . _max_weights ), size = size ) return ( self . _max_weights [ indexes ], self . _max_profits [ indexes ], self . _sum_of_weights [ indexes ], ) def __denormalise_instances ( self , decode_X : np . ndarray ) -> np . ndarray : n_instances = decode_X . shape [ 0 ] if self . _scale_method == \"sample\" : max_w , max_p , scale_Q = self . __sample_scaling_factors ( size = n_instances ) else : max_w , max_p , scale_Q = self . __scaling_from_training ( size = n_instances ) rescaled_instances = np . zeros_like ( decode_X , dtype = np . int32 ) rescaled_instances [:, 0 ] = decode_X [:, 0 ] * scale_Q [:, 0 ] # * 1_000_000 rescaled_instances [:, 1 :: 2 ] = decode_X [:, 1 :: 2 ] * max_w # * 100_000 rescaled_instances [:, 2 :: 2 ] = decode_X [:, 2 :: 2 ] * max_p # * 100_000 return rescaled_instances def __call__ ( self , X : npt . NDArray ) -> np . ndarray : \"\"\"Decodes an np.ndarray of shape (M, 2) into KP instances of N = 50. It does not return Knapsack objects but a np.ndarray of shape (M, 101) where 101 corresponds to the capacity (Q) and 50 pairs of weights and profits(w_i, p_i) Args: X (npt.NDArray): an np.ndarray of shape (M, 2) Raises: ValueError: If X has a difference shape than (M, 2) Returns: np.ndarray: numpy array with |M| KP definitions \"\"\" if not isinstance ( X , np . ndarray ): X = np . asarray ( X ) if X . shape [ 1 ] != self . _expected_latent_dim : raise ValueError ( f \"Expected a np.ndarray with shape (M, { self . _expected_latent_dim } ). Instead got: { X . shape } \" ) y = ( self . _decoder . decode ( torch . tensor ( X , device = DEVICE , dtype = torch . float32 )) . cpu () . detach () . numpy () ) return self . __denormalise_instances ( y ) __call__ ( X ) Decodes an np.ndarray of shape (M, 2) into KP instances of N = 50. It does not return Knapsack objects but a np.ndarray of shape (M, 101) where 101 corresponds to the capacity (Q) and 50 pairs of weights and profits(w_i, p_i) Parameters: X ( NDArray ) \u2013 an np.ndarray of shape (M, 2) Raises: ValueError \u2013 If X has a difference shape than (M, 2) Returns: ndarray \u2013 np.ndarray: numpy array with |M| KP definitions Source code in digneapy/transformers/autoencoders.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def __call__ ( self , X : npt . NDArray ) -> np . ndarray : \"\"\"Decodes an np.ndarray of shape (M, 2) into KP instances of N = 50. It does not return Knapsack objects but a np.ndarray of shape (M, 101) where 101 corresponds to the capacity (Q) and 50 pairs of weights and profits(w_i, p_i) Args: X (npt.NDArray): an np.ndarray of shape (M, 2) Raises: ValueError: If X has a difference shape than (M, 2) Returns: np.ndarray: numpy array with |M| KP definitions \"\"\" if not isinstance ( X , np . ndarray ): X = np . asarray ( X ) if X . shape [ 1 ] != self . _expected_latent_dim : raise ValueError ( f \"Expected a np.ndarray with shape (M, { self . _expected_latent_dim } ). Instead got: { X . shape } \" ) y = ( self . _decoder . decode ( torch . tensor ( X , device = DEVICE , dtype = torch . float32 )) . cpu () . detach () . numpy () ) return self . __denormalise_instances ( y ) KPEncoder Bases: Transformer Source code in digneapy/transformers/autoencoders.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 class KPEncoder ( Transformer ): def __init__ ( self , name : str = \"KPEncoder\" ): super () . __init__ ( name ) self . _expected_input_dim = 101 self . _encoder = torch . jit . load ( MODELS_PATH / AUTOENCODER_NAME , map_location = torch . device ( DEVICE ) ) @property def latent_dimension ( self ) -> int : return 2 @property def expected_input_dim ( self ) -> int : return self . _expected_input_dim def __call__ ( self , X : npt . NDArray ) -> np . ndarray : \"\"\"Encodes a numpy array of 50d-KP instances into 2D encodings. Args: X (npt.NDArray): A numpy array with the definitions of the KP instances. Expected to be of shape (M, 101). Raises: ValueError: If the shape of X does not match (M, 101) Returns: np.ndarray: _description_ \"\"\" if not isinstance ( X , np . ndarray ): X = np . asarray ( X ) if X . shape [ 1 ] != self . _expected_input_dim : raise ValueError ( f \"Expected a np.ndarray with shape (M, { self . _expected_input_dim } ). Instead got: { X . shape } \" ) codings_means , codings_log_var = self . _encoder . encode ( torch . tensor ( X , device = DEVICE , dtype = torch . float32 ) ) codings = self . _encoder . sample_codings ( codings_means , codings_log_var ) # Mean and Logarithm of the variance return codings . cpu () . detach () . numpy () __call__ ( X ) Encodes a numpy array of 50d-KP instances into 2D encodings. Parameters: X ( NDArray ) \u2013 A numpy array with the definitions of the KP instances. Expected to be of shape (M, 101). Raises: ValueError \u2013 If the shape of X does not match (M, 101) Returns: ndarray \u2013 np.ndarray: description Source code in digneapy/transformers/autoencoders.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def __call__ ( self , X : npt . NDArray ) -> np . ndarray : \"\"\"Encodes a numpy array of 50d-KP instances into 2D encodings. Args: X (npt.NDArray): A numpy array with the definitions of the KP instances. Expected to be of shape (M, 101). Raises: ValueError: If the shape of X does not match (M, 101) Returns: np.ndarray: _description_ \"\"\" if not isinstance ( X , np . ndarray ): X = np . asarray ( X ) if X . shape [ 1 ] != self . _expected_input_dim : raise ValueError ( f \"Expected a np.ndarray with shape (M, { self . _expected_input_dim } ). Instead got: { X . shape } \" ) codings_means , codings_log_var = self . _encoder . encode ( torch . tensor ( X , device = DEVICE , dtype = torch . float32 ) ) codings = self . _encoder . sample_codings ( codings_means , codings_log_var ) # Mean and Logarithm of the variance return codings . cpu () . detach () . numpy ()","title":"Autoencoders"},{"location":"reference/transformers/autoencoders/#transformers.autoencoders.KPDecoder","text":"Bases: Transformer Source code in digneapy/transformers/autoencoders.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 class KPDecoder ( Transformer ): def __init__ ( self , name : str = \"KPDecoder\" , scale_method : str = \"learnt\" ): super () . __init__ ( name ) if scale_method not in ( \"learnt\" , \"sample\" ): raise ValueError ( \"KPDecoder expects the scale method to be either learnt or sample\" ) self . _scale_method = scale_method self . __scales_fname = \"scales_knapsack_N_50.h5\" self . _expected_latent_dim = 2 self . _decoder = torch . jit . load ( MODELS_PATH / AUTOENCODER_NAME , map_location = torch . device ( DEVICE ) ) with h5py . File ( MODELS_PATH / self . __scales_fname , \"r\" ) as file : self . _max_weights = file [ \"scales\" ][ \"max_weights\" ][:] . astype ( np . int32 ) self . _max_profits = file [ \"scales\" ][ \"max_profits\" ][:] . astype ( np . int32 ) self . _sum_of_weights = file [ \"scales\" ][ \"sum_of_weights\" ][:] . astype ( np . int32 ) if self . _scale_method == \"sample\" : self . _weights_fitted_dist = lognorm . fit ( self . _max_weights , floc = 0 ) self . _profits_fitted_dist = lognorm . fit ( self . _max_profits , floc = 0 ) self . _capacity_fitted_dist = lognorm . fit ( self . _sum_of_weights , floc = 0 ) @property def output_dimension ( self ) -> int : return 101 def __sample_scaling_factors ( self , size : int ) -> Tuple [ Any , Any , Any ]: return ( lognorm . rvs ( self . _weights_fitted_dist [ 0 ], loc = self . _weights_fitted_dist [ 1 ], scale = self . _weights_fitted_dist [ 2 ], size = size , )[:, None ], lognorm . rvs ( self . _profits_fitted_dist [ 0 ], loc = self . _profits_fitted_dist [ 1 ], scale = self . _profits_fitted_dist [ 2 ], size = size , )[:, None ], lognorm . rvs ( self . _capacity_fitted_dist [ 0 ], loc = self . _capacity_fitted_dist [ 1 ], scale = self . _capacity_fitted_dist [ 2 ], size = size , )[:, None ], ) def __scaling_from_training ( self , size : int ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray ]: indexes = np . random . randint ( low = 0 , high = len ( self . _max_weights ), size = size ) return ( self . _max_weights [ indexes ], self . _max_profits [ indexes ], self . _sum_of_weights [ indexes ], ) def __denormalise_instances ( self , decode_X : np . ndarray ) -> np . ndarray : n_instances = decode_X . shape [ 0 ] if self . _scale_method == \"sample\" : max_w , max_p , scale_Q = self . __sample_scaling_factors ( size = n_instances ) else : max_w , max_p , scale_Q = self . __scaling_from_training ( size = n_instances ) rescaled_instances = np . zeros_like ( decode_X , dtype = np . int32 ) rescaled_instances [:, 0 ] = decode_X [:, 0 ] * scale_Q [:, 0 ] # * 1_000_000 rescaled_instances [:, 1 :: 2 ] = decode_X [:, 1 :: 2 ] * max_w # * 100_000 rescaled_instances [:, 2 :: 2 ] = decode_X [:, 2 :: 2 ] * max_p # * 100_000 return rescaled_instances def __call__ ( self , X : npt . NDArray ) -> np . ndarray : \"\"\"Decodes an np.ndarray of shape (M, 2) into KP instances of N = 50. It does not return Knapsack objects but a np.ndarray of shape (M, 101) where 101 corresponds to the capacity (Q) and 50 pairs of weights and profits(w_i, p_i) Args: X (npt.NDArray): an np.ndarray of shape (M, 2) Raises: ValueError: If X has a difference shape than (M, 2) Returns: np.ndarray: numpy array with |M| KP definitions \"\"\" if not isinstance ( X , np . ndarray ): X = np . asarray ( X ) if X . shape [ 1 ] != self . _expected_latent_dim : raise ValueError ( f \"Expected a np.ndarray with shape (M, { self . _expected_latent_dim } ). Instead got: { X . shape } \" ) y = ( self . _decoder . decode ( torch . tensor ( X , device = DEVICE , dtype = torch . float32 )) . cpu () . detach () . numpy () ) return self . __denormalise_instances ( y )","title":"KPDecoder"},{"location":"reference/transformers/autoencoders/#transformers.autoencoders.KPDecoder.__call__","text":"Decodes an np.ndarray of shape (M, 2) into KP instances of N = 50. It does not return Knapsack objects but a np.ndarray of shape (M, 101) where 101 corresponds to the capacity (Q) and 50 pairs of weights and profits(w_i, p_i) Parameters: X ( NDArray ) \u2013 an np.ndarray of shape (M, 2) Raises: ValueError \u2013 If X has a difference shape than (M, 2) Returns: ndarray \u2013 np.ndarray: numpy array with |M| KP definitions Source code in digneapy/transformers/autoencoders.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def __call__ ( self , X : npt . NDArray ) -> np . ndarray : \"\"\"Decodes an np.ndarray of shape (M, 2) into KP instances of N = 50. It does not return Knapsack objects but a np.ndarray of shape (M, 101) where 101 corresponds to the capacity (Q) and 50 pairs of weights and profits(w_i, p_i) Args: X (npt.NDArray): an np.ndarray of shape (M, 2) Raises: ValueError: If X has a difference shape than (M, 2) Returns: np.ndarray: numpy array with |M| KP definitions \"\"\" if not isinstance ( X , np . ndarray ): X = np . asarray ( X ) if X . shape [ 1 ] != self . _expected_latent_dim : raise ValueError ( f \"Expected a np.ndarray with shape (M, { self . _expected_latent_dim } ). Instead got: { X . shape } \" ) y = ( self . _decoder . decode ( torch . tensor ( X , device = DEVICE , dtype = torch . float32 )) . cpu () . detach () . numpy () ) return self . __denormalise_instances ( y )","title":"__call__"},{"location":"reference/transformers/autoencoders/#transformers.autoencoders.KPEncoder","text":"Bases: Transformer Source code in digneapy/transformers/autoencoders.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 class KPEncoder ( Transformer ): def __init__ ( self , name : str = \"KPEncoder\" ): super () . __init__ ( name ) self . _expected_input_dim = 101 self . _encoder = torch . jit . load ( MODELS_PATH / AUTOENCODER_NAME , map_location = torch . device ( DEVICE ) ) @property def latent_dimension ( self ) -> int : return 2 @property def expected_input_dim ( self ) -> int : return self . _expected_input_dim def __call__ ( self , X : npt . NDArray ) -> np . ndarray : \"\"\"Encodes a numpy array of 50d-KP instances into 2D encodings. Args: X (npt.NDArray): A numpy array with the definitions of the KP instances. Expected to be of shape (M, 101). Raises: ValueError: If the shape of X does not match (M, 101) Returns: np.ndarray: _description_ \"\"\" if not isinstance ( X , np . ndarray ): X = np . asarray ( X ) if X . shape [ 1 ] != self . _expected_input_dim : raise ValueError ( f \"Expected a np.ndarray with shape (M, { self . _expected_input_dim } ). Instead got: { X . shape } \" ) codings_means , codings_log_var = self . _encoder . encode ( torch . tensor ( X , device = DEVICE , dtype = torch . float32 ) ) codings = self . _encoder . sample_codings ( codings_means , codings_log_var ) # Mean and Logarithm of the variance return codings . cpu () . detach () . numpy ()","title":"KPEncoder"},{"location":"reference/transformers/autoencoders/#transformers.autoencoders.KPEncoder.__call__","text":"Encodes a numpy array of 50d-KP instances into 2D encodings. Parameters: X ( NDArray ) \u2013 A numpy array with the definitions of the KP instances. Expected to be of shape (M, 101). Raises: ValueError \u2013 If the shape of X does not match (M, 101) Returns: ndarray \u2013 np.ndarray: description Source code in digneapy/transformers/autoencoders.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def __call__ ( self , X : npt . NDArray ) -> np . ndarray : \"\"\"Encodes a numpy array of 50d-KP instances into 2D encodings. Args: X (npt.NDArray): A numpy array with the definitions of the KP instances. Expected to be of shape (M, 101). Raises: ValueError: If the shape of X does not match (M, 101) Returns: np.ndarray: _description_ \"\"\" if not isinstance ( X , np . ndarray ): X = np . asarray ( X ) if X . shape [ 1 ] != self . _expected_input_dim : raise ValueError ( f \"Expected a np.ndarray with shape (M, { self . _expected_input_dim } ). Instead got: { X . shape } \" ) codings_means , codings_log_var = self . _encoder . encode ( torch . tensor ( X , device = DEVICE , dtype = torch . float32 ) ) codings = self . _encoder . sample_codings ( codings_means , codings_log_var ) # Mean and Logarithm of the variance return codings . cpu () . detach () . numpy ()","title":"__call__"},{"location":"reference/transformers/neural/","text":"@File : _neural_networks.py @Time : 2024/09/12 14:21:29 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2024, Alejandro Marrero @Desc : None NNEncoder Bases: Transformer Source code in digneapy/transformers/neural.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 class NNEncoder ( Transformer ): def __init__ ( self , name : str , input_shape : Sequence [ int ], shape : Sequence [ int ], activations : Sequence [ Optional [ str ]], evaluation_metric : Optional [ Callable ] = None , loss_fn : Optional [ Callable ] = None , optimizer : Optional [ keras . Optimizer ] = keras . optimizers . Nadam (), scale : bool = True , ): \"\"\"Neural Network used to transform a space into another. This class uses a Keras backend. Args: name (str): Name of the model to be saved with. Expected a .keras extension. shape (Tuple[int]): Tuple with the number of cells per layer. activations (Tuple[str]): Activation functions for each layer. Raises: ValueError: Raises if any attribute is not valid. \"\"\" if len ( activations ) != len ( shape ): msg = f \"Expected { len ( shape ) } activation functions but only got { len ( activations ) } \" raise ValueError ( msg ) if not name . endswith ( \".keras\" ): name = name + \".keras\" super () . __init__ ( name ) self . scaler = StandardScaler () self . input_shape = input_shape self . _shape = shape self . _activations = activations self . _eval_metric = evaluation_metric self . _loss_fn = loss_fn self . _optimizer = optimizer self . _model = keras . models . Sequential () self . _model . add ( keras . layers . InputLayer ( shape = input_shape )) for d , act in zip ( shape , activations ): self . _model . add ( keras . layers . Dense ( d , activation = act )) self . _model . compile ( loss = self . _loss_fn , optimizer = optimizer , metrics = [ self . _eval_metric ] ) self . _expected_shapes = [ v . shape for v in self . _model . trainable_variables ] self . _expected_sizes = [ np . prod ( s ) for s in self . _expected_shapes ] self . _size = np . sum ( self . _expected_sizes ) def __str__ ( self ) -> str : tokens = [] self . _model . summary ( print_fn = lambda x : tokens . append ( x )) return \" \\n \" . join ( tokens ) def __repr__ ( self ) -> str : return self . __str__ () def save ( self , filename : Optional [ str ] = None ): if filename is not None : self . _model . save ( filename ) else : self . _model . save ( self . _name ) def update_weights ( self , weights : Sequence [ float ]): if len ( weights ) != self . _size : msg = f \"Error in the amount of weights in NN.update_weigths. Expected { self . _size } and got { len ( weights ) } \" raise ValueError ( msg ) new_weights = [ None ] * len ( self . _expected_shapes ) idx = 0 i = 0 for shape , size in zip ( self . _expected_shapes , self . _expected_sizes ): new_weights [ i ] = np . reshape ( weights [ idx : idx + size ], shape ) idx += size i += 1 self . _model . set_weights ( new_weights ) return True def predict ( self , x : npt . NDArray , batch_size : int = 1024 ) -> np . ndarray : if x is None or len ( x ) == 0 : msg = \"x cannot be None in KerasNN predict\" raise RuntimeError ( msg ) if isinstance ( x , list ): x = np . vstack ( x ) elif x . ndim == 1 : x . reshape ( 1 , - 1 ) x_scaled = self . scaler . fit_transform ( x ) return self . _model . predict ( x_scaled , batch_size = batch_size , verbose = 0 ) def __call__ ( self , x : npt . NDArray ) -> np . ndarray : return self . predict ( x ) __init__ ( name , input_shape , shape , activations , evaluation_metric = None , loss_fn = None , optimizer = keras . optimizers . Nadam (), scale = True ) Neural Network used to transform a space into another. This class uses a Keras backend. Parameters: name ( str ) \u2013 Name of the model to be saved with. Expected a .keras extension. shape ( Tuple [ int ] ) \u2013 Tuple with the number of cells per layer. activations ( Tuple [ str ] ) \u2013 Activation functions for each layer. Raises: ValueError: Raises if any attribute is not valid. Source code in digneapy/transformers/neural.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def __init__ ( self , name : str , input_shape : Sequence [ int ], shape : Sequence [ int ], activations : Sequence [ Optional [ str ]], evaluation_metric : Optional [ Callable ] = None , loss_fn : Optional [ Callable ] = None , optimizer : Optional [ keras . Optimizer ] = keras . optimizers . Nadam (), scale : bool = True , ): \"\"\"Neural Network used to transform a space into another. This class uses a Keras backend. Args: name (str): Name of the model to be saved with. Expected a .keras extension. shape (Tuple[int]): Tuple with the number of cells per layer. activations (Tuple[str]): Activation functions for each layer. Raises: ValueError: Raises if any attribute is not valid. \"\"\" if len ( activations ) != len ( shape ): msg = f \"Expected { len ( shape ) } activation functions but only got { len ( activations ) } \" raise ValueError ( msg ) if not name . endswith ( \".keras\" ): name = name + \".keras\" super () . __init__ ( name ) self . scaler = StandardScaler () self . input_shape = input_shape self . _shape = shape self . _activations = activations self . _eval_metric = evaluation_metric self . _loss_fn = loss_fn self . _optimizer = optimizer self . _model = keras . models . Sequential () self . _model . add ( keras . layers . InputLayer ( shape = input_shape )) for d , act in zip ( shape , activations ): self . _model . add ( keras . layers . Dense ( d , activation = act )) self . _model . compile ( loss = self . _loss_fn , optimizer = optimizer , metrics = [ self . _eval_metric ] ) self . _expected_shapes = [ v . shape for v in self . _model . trainable_variables ] self . _expected_sizes = [ np . prod ( s ) for s in self . _expected_shapes ] self . _size = np . sum ( self . _expected_sizes )","title":"Neural"},{"location":"reference/transformers/neural/#transformers.neural.NNEncoder","text":"Bases: Transformer Source code in digneapy/transformers/neural.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 class NNEncoder ( Transformer ): def __init__ ( self , name : str , input_shape : Sequence [ int ], shape : Sequence [ int ], activations : Sequence [ Optional [ str ]], evaluation_metric : Optional [ Callable ] = None , loss_fn : Optional [ Callable ] = None , optimizer : Optional [ keras . Optimizer ] = keras . optimizers . Nadam (), scale : bool = True , ): \"\"\"Neural Network used to transform a space into another. This class uses a Keras backend. Args: name (str): Name of the model to be saved with. Expected a .keras extension. shape (Tuple[int]): Tuple with the number of cells per layer. activations (Tuple[str]): Activation functions for each layer. Raises: ValueError: Raises if any attribute is not valid. \"\"\" if len ( activations ) != len ( shape ): msg = f \"Expected { len ( shape ) } activation functions but only got { len ( activations ) } \" raise ValueError ( msg ) if not name . endswith ( \".keras\" ): name = name + \".keras\" super () . __init__ ( name ) self . scaler = StandardScaler () self . input_shape = input_shape self . _shape = shape self . _activations = activations self . _eval_metric = evaluation_metric self . _loss_fn = loss_fn self . _optimizer = optimizer self . _model = keras . models . Sequential () self . _model . add ( keras . layers . InputLayer ( shape = input_shape )) for d , act in zip ( shape , activations ): self . _model . add ( keras . layers . Dense ( d , activation = act )) self . _model . compile ( loss = self . _loss_fn , optimizer = optimizer , metrics = [ self . _eval_metric ] ) self . _expected_shapes = [ v . shape for v in self . _model . trainable_variables ] self . _expected_sizes = [ np . prod ( s ) for s in self . _expected_shapes ] self . _size = np . sum ( self . _expected_sizes ) def __str__ ( self ) -> str : tokens = [] self . _model . summary ( print_fn = lambda x : tokens . append ( x )) return \" \\n \" . join ( tokens ) def __repr__ ( self ) -> str : return self . __str__ () def save ( self , filename : Optional [ str ] = None ): if filename is not None : self . _model . save ( filename ) else : self . _model . save ( self . _name ) def update_weights ( self , weights : Sequence [ float ]): if len ( weights ) != self . _size : msg = f \"Error in the amount of weights in NN.update_weigths. Expected { self . _size } and got { len ( weights ) } \" raise ValueError ( msg ) new_weights = [ None ] * len ( self . _expected_shapes ) idx = 0 i = 0 for shape , size in zip ( self . _expected_shapes , self . _expected_sizes ): new_weights [ i ] = np . reshape ( weights [ idx : idx + size ], shape ) idx += size i += 1 self . _model . set_weights ( new_weights ) return True def predict ( self , x : npt . NDArray , batch_size : int = 1024 ) -> np . ndarray : if x is None or len ( x ) == 0 : msg = \"x cannot be None in KerasNN predict\" raise RuntimeError ( msg ) if isinstance ( x , list ): x = np . vstack ( x ) elif x . ndim == 1 : x . reshape ( 1 , - 1 ) x_scaled = self . scaler . fit_transform ( x ) return self . _model . predict ( x_scaled , batch_size = batch_size , verbose = 0 ) def __call__ ( self , x : npt . NDArray ) -> np . ndarray : return self . predict ( x )","title":"NNEncoder"},{"location":"reference/transformers/neural/#transformers.neural.NNEncoder.__init__","text":"Neural Network used to transform a space into another. This class uses a Keras backend. Parameters: name ( str ) \u2013 Name of the model to be saved with. Expected a .keras extension. shape ( Tuple [ int ] ) \u2013 Tuple with the number of cells per layer. activations ( Tuple [ str ] ) \u2013 Activation functions for each layer. Raises: ValueError: Raises if any attribute is not valid. Source code in digneapy/transformers/neural.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def __init__ ( self , name : str , input_shape : Sequence [ int ], shape : Sequence [ int ], activations : Sequence [ Optional [ str ]], evaluation_metric : Optional [ Callable ] = None , loss_fn : Optional [ Callable ] = None , optimizer : Optional [ keras . Optimizer ] = keras . optimizers . Nadam (), scale : bool = True , ): \"\"\"Neural Network used to transform a space into another. This class uses a Keras backend. Args: name (str): Name of the model to be saved with. Expected a .keras extension. shape (Tuple[int]): Tuple with the number of cells per layer. activations (Tuple[str]): Activation functions for each layer. Raises: ValueError: Raises if any attribute is not valid. \"\"\" if len ( activations ) != len ( shape ): msg = f \"Expected { len ( shape ) } activation functions but only got { len ( activations ) } \" raise ValueError ( msg ) if not name . endswith ( \".keras\" ): name = name + \".keras\" super () . __init__ ( name ) self . scaler = StandardScaler () self . input_shape = input_shape self . _shape = shape self . _activations = activations self . _eval_metric = evaluation_metric self . _loss_fn = loss_fn self . _optimizer = optimizer self . _model = keras . models . Sequential () self . _model . add ( keras . layers . InputLayer ( shape = input_shape )) for d , act in zip ( shape , activations ): self . _model . add ( keras . layers . Dense ( d , activation = act )) self . _model . compile ( loss = self . _loss_fn , optimizer = optimizer , metrics = [ self . _eval_metric ] ) self . _expected_shapes = [ v . shape for v in self . _model . trainable_variables ] self . _expected_sizes = [ np . prod ( s ) for s in self . _expected_shapes ] self . _size = np . sum ( self . _expected_sizes )","title":"__init__"},{"location":"reference/transformers/pca/","text":"@File : pca.py @Time : 2025/04/07 09:07:49 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2025, Alejandro Marrero @Desc : None","title":"Pca"},{"location":"reference/transformers/tuner/","text":"@File : meta_ea.py @Time : 2024/04/25 09:54:42 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2024, Alejandro Marrero @Desc : None DeapTuner Neural Network Evolutionary Algorithm Tuner This class implements a CMA-ES based tuner for neural networks. It allows to optimize the weights of a neural network to generate transformed spaces in optimization domains. It uses the DEAP library for the evolutionary algorithm Source code in digneapy/transformers/tuner.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 class DeapTuner : \"\"\"Neural Network Evolutionary Algorithm Tuner This class implements a CMA-ES based tuner for neural networks. It allows to optimize the weights of a neural network to generate transformed spaces in optimization domains. It uses the DEAP library for the evolutionary algorithm\"\"\" def __init__ ( self , eval_fn : Callable , dimension : int , transformer : NNEncoder , centroid : Optional [ Sequence [ float ]] = None , sigma : float = 1.0 , lambda_ : int = 50 , generations : int = 250 , direction : Direction = Direction . MAXIMISE , n_jobs : int = 1 , ): \"\"\"Creates a new NNEATuner instance Args: eval_fn (Callable): Funtion to evaluate the fitness of a neural network weights. It must return a single float value representing the fitness. This function will be called with a list of weights as input. It must be defined before creating the tuner instance. dimension (int): Number of weights in the neural network. centroid (Optional[Sequence[float]], optional): Starting point for the CMA-ES algorithm. sigma (float, optional): Defaults to 1.0. lambda_ (int, optional): Population size. Defaults to 50. generations (int, optional): Number of generatios to perform. Defaults to 250. direction (Direction, optional): Optimisation direction. Defaults to Direction.MAXIMISE. n_jobs (int, optional): Number of workers. Defaults to 1. Raises: ValueError: If eval_fn is None or if direction is not a valid Direction. \"\"\" if eval_fn is None : raise ValueError ( \"eval_fn cannot be None in NNTuner. Please give a valid evaluation function.\" ) if transformer is None or not isinstance ( transformer , ( NNEncoder )): raise ValueError ( \"transformer cannot be None in NNTuner. Please give a valid transformer (KerasNN or TorchNN).\" ) self . eval_fn = eval_fn self . dimension = dimension self . transformer = transformer self . centroid = centroid if centroid is not None else [ 0.0 ] * self . dimension self . sigma = sigma self . _lambda = lambda_ if lambda_ != 0 else 50 self . generations = generations self . __performed_gens = 0 # These vars are used to save the data in CSV files self . __evaluated_inds = 0 if not isinstance ( direction , Direction ): msg = f \"Direction: { direction } not available. Please choose between { Direction . values () } \" raise ValueError ( msg ) self . direction = direction self . toolbox = base . Toolbox () self . toolbox . register ( \"evaluate\" , self . evaluation ) self . strategy = cma . Strategy ( centroid = self . centroid , sigma = self . sigma , lambda_ = self . _lambda ) if self . direction == Direction . MAXIMISE : self . toolbox . register ( \"generate\" , self . strategy . generate , creator . IndMax ) else : self . toolbox . register ( \"generate\" , self . strategy . generate , creator . IndMin ) self . toolbox . register ( \"update\" , self . strategy . update ) if n_jobs < 1 : msg = \"The number of jobs must be at least 1.\" raise ValueError ( msg ) elif n_jobs > 1 : self . n_processors = n_jobs self . pool = Pool ( processes = self . n_processors ) self . toolbox . register ( \"map\" , self . pool . map ) self . hof = tools . HallOfFame ( 1 ) self . stats = tools . Statistics ( lambda ind : ind . fitness . values ) self . stats . register ( \"avg\" , np . mean ) self . stats . register ( \"std\" , np . std ) self . stats . register ( \"min\" , np . min ) self . stats . register ( \"max\" , np . max ) def evaluation ( self , individual : Sequence [ float ]) -> tuple [ float ]: \"\"\"Evaluates a chromosome of weights for a NN to generate spaces in optimisation domains Args: individual (Sequence[float]): Sequence of weights for a NN transformer Returns: tuple[float]: Space coverage of the space create from the NN transformer \"\"\" self . transformer . update_weights ( individual ) # filename = f\"dataset_generation_{self.__performed_gens}_individual_{self.__evaluated_inds}.csv\" self . __evaluated_inds += 1 if self . __evaluated_inds == self . _lambda : self . __performed_gens += 1 self . __evaluated_inds = 0 fitness = self . eval_fn ( self . transformer ) return ( fitness ,) def __call__ ( self ): population , logbook = algorithms . eaGenerateUpdate ( self . toolbox , ngen = self . generations , stats = self . stats , halloffame = self . hof , verbose = True , ) return ( self . hof [ 0 ], population , logbook ) __init__ ( eval_fn , dimension , transformer , centroid = None , sigma = 1.0 , lambda_ = 50 , generations = 250 , direction = Direction . MAXIMISE , n_jobs = 1 ) Creates a new NNEATuner instance Parameters: eval_fn ( Callable ) \u2013 Funtion to evaluate the fitness of a neural network dimension ( int ) \u2013 Number of weights in the neural network. centroid ( Optional [ Sequence [ float ]] , default: None ) \u2013 Starting point for the CMA-ES algorithm. sigma ( float , default: 1.0 ) \u2013 Defaults to 1.0. lambda_ ( int , default: 50 ) \u2013 Population size. Defaults to 50. generations ( int , default: 250 ) \u2013 Number of generatios to perform. Defaults to 250. direction ( Direction , default: MAXIMISE ) \u2013 Optimisation direction. Defaults to Direction.MAXIMISE. n_jobs ( int , default: 1 ) \u2013 Number of workers. Defaults to 1. Raises: ValueError \u2013 If eval_fn is None or if direction is not a valid Direction. Source code in digneapy/transformers/tuner.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def __init__ ( self , eval_fn : Callable , dimension : int , transformer : NNEncoder , centroid : Optional [ Sequence [ float ]] = None , sigma : float = 1.0 , lambda_ : int = 50 , generations : int = 250 , direction : Direction = Direction . MAXIMISE , n_jobs : int = 1 , ): \"\"\"Creates a new NNEATuner instance Args: eval_fn (Callable): Funtion to evaluate the fitness of a neural network weights. It must return a single float value representing the fitness. This function will be called with a list of weights as input. It must be defined before creating the tuner instance. dimension (int): Number of weights in the neural network. centroid (Optional[Sequence[float]], optional): Starting point for the CMA-ES algorithm. sigma (float, optional): Defaults to 1.0. lambda_ (int, optional): Population size. Defaults to 50. generations (int, optional): Number of generatios to perform. Defaults to 250. direction (Direction, optional): Optimisation direction. Defaults to Direction.MAXIMISE. n_jobs (int, optional): Number of workers. Defaults to 1. Raises: ValueError: If eval_fn is None or if direction is not a valid Direction. \"\"\" if eval_fn is None : raise ValueError ( \"eval_fn cannot be None in NNTuner. Please give a valid evaluation function.\" ) if transformer is None or not isinstance ( transformer , ( NNEncoder )): raise ValueError ( \"transformer cannot be None in NNTuner. Please give a valid transformer (KerasNN or TorchNN).\" ) self . eval_fn = eval_fn self . dimension = dimension self . transformer = transformer self . centroid = centroid if centroid is not None else [ 0.0 ] * self . dimension self . sigma = sigma self . _lambda = lambda_ if lambda_ != 0 else 50 self . generations = generations self . __performed_gens = 0 # These vars are used to save the data in CSV files self . __evaluated_inds = 0 if not isinstance ( direction , Direction ): msg = f \"Direction: { direction } not available. Please choose between { Direction . values () } \" raise ValueError ( msg ) self . direction = direction self . toolbox = base . Toolbox () self . toolbox . register ( \"evaluate\" , self . evaluation ) self . strategy = cma . Strategy ( centroid = self . centroid , sigma = self . sigma , lambda_ = self . _lambda ) if self . direction == Direction . MAXIMISE : self . toolbox . register ( \"generate\" , self . strategy . generate , creator . IndMax ) else : self . toolbox . register ( \"generate\" , self . strategy . generate , creator . IndMin ) self . toolbox . register ( \"update\" , self . strategy . update ) if n_jobs < 1 : msg = \"The number of jobs must be at least 1.\" raise ValueError ( msg ) elif n_jobs > 1 : self . n_processors = n_jobs self . pool = Pool ( processes = self . n_processors ) self . toolbox . register ( \"map\" , self . pool . map ) self . hof = tools . HallOfFame ( 1 ) self . stats = tools . Statistics ( lambda ind : ind . fitness . values ) self . stats . register ( \"avg\" , np . mean ) self . stats . register ( \"std\" , np . std ) self . stats . register ( \"min\" , np . min ) self . stats . register ( \"max\" , np . max ) evaluation ( individual ) Evaluates a chromosome of weights for a NN to generate spaces in optimisation domains Parameters: individual ( Sequence [ float ] ) \u2013 Sequence of weights for a NN transformer Returns: tuple [ float ] \u2013 tuple[float]: Space coverage of the space create from the NN transformer Source code in digneapy/transformers/tuner.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def evaluation ( self , individual : Sequence [ float ]) -> tuple [ float ]: \"\"\"Evaluates a chromosome of weights for a NN to generate spaces in optimisation domains Args: individual (Sequence[float]): Sequence of weights for a NN transformer Returns: tuple[float]: Space coverage of the space create from the NN transformer \"\"\" self . transformer . update_weights ( individual ) # filename = f\"dataset_generation_{self.__performed_gens}_individual_{self.__evaluated_inds}.csv\" self . __evaluated_inds += 1 if self . __evaluated_inds == self . _lambda : self . __performed_gens += 1 self . __evaluated_inds = 0 fitness = self . eval_fn ( self . transformer ) return ( fitness ,)","title":"Tuner"},{"location":"reference/transformers/tuner/#transformers.tuner.DeapTuner","text":"Neural Network Evolutionary Algorithm Tuner This class implements a CMA-ES based tuner for neural networks. It allows to optimize the weights of a neural network to generate transformed spaces in optimization domains. It uses the DEAP library for the evolutionary algorithm Source code in digneapy/transformers/tuner.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 class DeapTuner : \"\"\"Neural Network Evolutionary Algorithm Tuner This class implements a CMA-ES based tuner for neural networks. It allows to optimize the weights of a neural network to generate transformed spaces in optimization domains. It uses the DEAP library for the evolutionary algorithm\"\"\" def __init__ ( self , eval_fn : Callable , dimension : int , transformer : NNEncoder , centroid : Optional [ Sequence [ float ]] = None , sigma : float = 1.0 , lambda_ : int = 50 , generations : int = 250 , direction : Direction = Direction . MAXIMISE , n_jobs : int = 1 , ): \"\"\"Creates a new NNEATuner instance Args: eval_fn (Callable): Funtion to evaluate the fitness of a neural network weights. It must return a single float value representing the fitness. This function will be called with a list of weights as input. It must be defined before creating the tuner instance. dimension (int): Number of weights in the neural network. centroid (Optional[Sequence[float]], optional): Starting point for the CMA-ES algorithm. sigma (float, optional): Defaults to 1.0. lambda_ (int, optional): Population size. Defaults to 50. generations (int, optional): Number of generatios to perform. Defaults to 250. direction (Direction, optional): Optimisation direction. Defaults to Direction.MAXIMISE. n_jobs (int, optional): Number of workers. Defaults to 1. Raises: ValueError: If eval_fn is None or if direction is not a valid Direction. \"\"\" if eval_fn is None : raise ValueError ( \"eval_fn cannot be None in NNTuner. Please give a valid evaluation function.\" ) if transformer is None or not isinstance ( transformer , ( NNEncoder )): raise ValueError ( \"transformer cannot be None in NNTuner. Please give a valid transformer (KerasNN or TorchNN).\" ) self . eval_fn = eval_fn self . dimension = dimension self . transformer = transformer self . centroid = centroid if centroid is not None else [ 0.0 ] * self . dimension self . sigma = sigma self . _lambda = lambda_ if lambda_ != 0 else 50 self . generations = generations self . __performed_gens = 0 # These vars are used to save the data in CSV files self . __evaluated_inds = 0 if not isinstance ( direction , Direction ): msg = f \"Direction: { direction } not available. Please choose between { Direction . values () } \" raise ValueError ( msg ) self . direction = direction self . toolbox = base . Toolbox () self . toolbox . register ( \"evaluate\" , self . evaluation ) self . strategy = cma . Strategy ( centroid = self . centroid , sigma = self . sigma , lambda_ = self . _lambda ) if self . direction == Direction . MAXIMISE : self . toolbox . register ( \"generate\" , self . strategy . generate , creator . IndMax ) else : self . toolbox . register ( \"generate\" , self . strategy . generate , creator . IndMin ) self . toolbox . register ( \"update\" , self . strategy . update ) if n_jobs < 1 : msg = \"The number of jobs must be at least 1.\" raise ValueError ( msg ) elif n_jobs > 1 : self . n_processors = n_jobs self . pool = Pool ( processes = self . n_processors ) self . toolbox . register ( \"map\" , self . pool . map ) self . hof = tools . HallOfFame ( 1 ) self . stats = tools . Statistics ( lambda ind : ind . fitness . values ) self . stats . register ( \"avg\" , np . mean ) self . stats . register ( \"std\" , np . std ) self . stats . register ( \"min\" , np . min ) self . stats . register ( \"max\" , np . max ) def evaluation ( self , individual : Sequence [ float ]) -> tuple [ float ]: \"\"\"Evaluates a chromosome of weights for a NN to generate spaces in optimisation domains Args: individual (Sequence[float]): Sequence of weights for a NN transformer Returns: tuple[float]: Space coverage of the space create from the NN transformer \"\"\" self . transformer . update_weights ( individual ) # filename = f\"dataset_generation_{self.__performed_gens}_individual_{self.__evaluated_inds}.csv\" self . __evaluated_inds += 1 if self . __evaluated_inds == self . _lambda : self . __performed_gens += 1 self . __evaluated_inds = 0 fitness = self . eval_fn ( self . transformer ) return ( fitness ,) def __call__ ( self ): population , logbook = algorithms . eaGenerateUpdate ( self . toolbox , ngen = self . generations , stats = self . stats , halloffame = self . hof , verbose = True , ) return ( self . hof [ 0 ], population , logbook )","title":"DeapTuner"},{"location":"reference/transformers/tuner/#transformers.tuner.DeapTuner.__init__","text":"Creates a new NNEATuner instance Parameters: eval_fn ( Callable ) \u2013 Funtion to evaluate the fitness of a neural network dimension ( int ) \u2013 Number of weights in the neural network. centroid ( Optional [ Sequence [ float ]] , default: None ) \u2013 Starting point for the CMA-ES algorithm. sigma ( float , default: 1.0 ) \u2013 Defaults to 1.0. lambda_ ( int , default: 50 ) \u2013 Population size. Defaults to 50. generations ( int , default: 250 ) \u2013 Number of generatios to perform. Defaults to 250. direction ( Direction , default: MAXIMISE ) \u2013 Optimisation direction. Defaults to Direction.MAXIMISE. n_jobs ( int , default: 1 ) \u2013 Number of workers. Defaults to 1. Raises: ValueError \u2013 If eval_fn is None or if direction is not a valid Direction. Source code in digneapy/transformers/tuner.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def __init__ ( self , eval_fn : Callable , dimension : int , transformer : NNEncoder , centroid : Optional [ Sequence [ float ]] = None , sigma : float = 1.0 , lambda_ : int = 50 , generations : int = 250 , direction : Direction = Direction . MAXIMISE , n_jobs : int = 1 , ): \"\"\"Creates a new NNEATuner instance Args: eval_fn (Callable): Funtion to evaluate the fitness of a neural network weights. It must return a single float value representing the fitness. This function will be called with a list of weights as input. It must be defined before creating the tuner instance. dimension (int): Number of weights in the neural network. centroid (Optional[Sequence[float]], optional): Starting point for the CMA-ES algorithm. sigma (float, optional): Defaults to 1.0. lambda_ (int, optional): Population size. Defaults to 50. generations (int, optional): Number of generatios to perform. Defaults to 250. direction (Direction, optional): Optimisation direction. Defaults to Direction.MAXIMISE. n_jobs (int, optional): Number of workers. Defaults to 1. Raises: ValueError: If eval_fn is None or if direction is not a valid Direction. \"\"\" if eval_fn is None : raise ValueError ( \"eval_fn cannot be None in NNTuner. Please give a valid evaluation function.\" ) if transformer is None or not isinstance ( transformer , ( NNEncoder )): raise ValueError ( \"transformer cannot be None in NNTuner. Please give a valid transformer (KerasNN or TorchNN).\" ) self . eval_fn = eval_fn self . dimension = dimension self . transformer = transformer self . centroid = centroid if centroid is not None else [ 0.0 ] * self . dimension self . sigma = sigma self . _lambda = lambda_ if lambda_ != 0 else 50 self . generations = generations self . __performed_gens = 0 # These vars are used to save the data in CSV files self . __evaluated_inds = 0 if not isinstance ( direction , Direction ): msg = f \"Direction: { direction } not available. Please choose between { Direction . values () } \" raise ValueError ( msg ) self . direction = direction self . toolbox = base . Toolbox () self . toolbox . register ( \"evaluate\" , self . evaluation ) self . strategy = cma . Strategy ( centroid = self . centroid , sigma = self . sigma , lambda_ = self . _lambda ) if self . direction == Direction . MAXIMISE : self . toolbox . register ( \"generate\" , self . strategy . generate , creator . IndMax ) else : self . toolbox . register ( \"generate\" , self . strategy . generate , creator . IndMin ) self . toolbox . register ( \"update\" , self . strategy . update ) if n_jobs < 1 : msg = \"The number of jobs must be at least 1.\" raise ValueError ( msg ) elif n_jobs > 1 : self . n_processors = n_jobs self . pool = Pool ( processes = self . n_processors ) self . toolbox . register ( \"map\" , self . pool . map ) self . hof = tools . HallOfFame ( 1 ) self . stats = tools . Statistics ( lambda ind : ind . fitness . values ) self . stats . register ( \"avg\" , np . mean ) self . stats . register ( \"std\" , np . std ) self . stats . register ( \"min\" , np . min ) self . stats . register ( \"max\" , np . max )","title":"__init__"},{"location":"reference/transformers/tuner/#transformers.tuner.DeapTuner.evaluation","text":"Evaluates a chromosome of weights for a NN to generate spaces in optimisation domains Parameters: individual ( Sequence [ float ] ) \u2013 Sequence of weights for a NN transformer Returns: tuple [ float ] \u2013 tuple[float]: Space coverage of the space create from the NN transformer Source code in digneapy/transformers/tuner.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def evaluation ( self , individual : Sequence [ float ]) -> tuple [ float ]: \"\"\"Evaluates a chromosome of weights for a NN to generate spaces in optimisation domains Args: individual (Sequence[float]): Sequence of weights for a NN transformer Returns: tuple[float]: Space coverage of the space create from the NN transformer \"\"\" self . transformer . update_weights ( individual ) # filename = f\"dataset_generation_{self.__performed_gens}_individual_{self.__evaluated_inds}.csv\" self . __evaluated_inds += 1 if self . __evaluated_inds == self . _lambda : self . __performed_gens += 1 self . __evaluated_inds = 0 fitness = self . eval_fn ( self . transformer ) return ( fitness ,)","title":"evaluation"},{"location":"reference/utils/","text":"@File : init .py @Time : 2024/06/14 13:45:31 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2024, Alejandro Marrero @Desc : None save_results_to_files ( filename_pattern , result , only_instances = True , only_genotypes = False , solvers_names = None , features_names = None , vars_names = None , files_format = 'parquet' ) Saves the results of the generation to CSV files. Args: filename_pattern (str): Pattern for the filenames. result (GenResult): Result of the generation. only_instances (bool): Generate only the files with the resulting instances. Default True. If False, it would generate an history and arhice_metrics files. only_genotypes (bool): Extract only the genotype of each instance. Default False (extracts features and portfolio scores). solvers_names (Sequence[str]): Names of the solvers. features_names (Sequence[str]): Names of the features. vars_names (Sequence[str]): Names of the variables. files_format (Literal[str] = \"csv\" or \"parquet\"): Format to store the resulting instances file. Parquet is the most efficient for large datasets. Source code in digneapy/utils/save_data.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def save_results_to_files ( filename_pattern : str , result : GenResult , only_instances : bool = True , only_genotypes : bool = False , solvers_names : Optional [ Sequence [ str ]] = None , features_names : Optional [ Sequence [ str ]] = None , vars_names : Optional [ Sequence [ str ]] = None , files_format : Literal [ \"csv\" , \"parquet\" ] = \"parquet\" , ): \"\"\"Saves the results of the generation to CSV files. Args: filename_pattern (str): Pattern for the filenames. result (GenResult): Result of the generation. only_instances (bool): Generate only the files with the resulting instances. Default True. If False, it would generate an history and arhice_metrics files. only_genotypes (bool): Extract only the genotype of each instance. Default False (extracts features and portfolio scores). solvers_names (Sequence[str]): Names of the solvers. features_names (Sequence[str]): Names of the features. vars_names (Sequence[str]): Names of the variables. files_format (Literal[str] = \"csv\" or \"parquet\"): Format to store the resulting instances file. Parquet is the most efficient for large datasets. \"\"\" if files_format not in ( \"csv\" , \"parquet\" ): print ( f \"Unrecognised file format: { files_format } . Selecting parquet.\" ) files_format = \"parquet\" df = pd . DataFrame ( [ i . to_series ( only_genotype = only_genotypes , variables_names = vars_names , features_names = features_names , score_names = solvers_names , ) for i in result . instances ] ) if not df . empty : df . insert ( 0 , \"target\" , result . target ) if files_format == \"csv\" : df . to_csv ( f \" { filename_pattern } _instances.csv\" , index = False ) elif files_format == \"parquet\" : df . to_parquet ( f \" { filename_pattern } _instances.parquet\" , index = False ) if not only_instances : result . history . to_df () . to_csv ( f \" { filename_pattern } _history.csv\" , index = False ) if result . metrics is not None : result . metrics . to_csv ( f \" { filename_pattern } _archive_metrics.csv\" ) sort_knapsack_instances ( instances ) sort_knapsack_instances ( instances : np . ndarray , ) -> np . ndarray sort_knapsack_instances ( instances : Sequence [ Instance ], ) -> List [ Instance ] Sorts a collection of Knapsack Instances Genotypes based on lexicograph order by (w_i, p_i) Parameters: instances ( ndarray | Sequence [ Instance ] ) \u2013 Instances to sort Raises: ValueError \u2013 If the dimension of the genotypes (minus Q) is not even. Note that KP instances should contain N pairs of values plus the capacity. Returns: ndarray | List [ Instance ] \u2013 np.ndarray | Sequence[Instance]: Sorted instances Source code in digneapy/utils/sorting.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def sort_knapsack_instances ( instances : np . ndarray | Sequence [ Instance ], ) -> np . ndarray | List [ Instance ]: \"\"\"Sorts a collection of Knapsack Instances Genotypes based on lexicograph order by (w_i, p_i) Args: instances (np.ndarray | Sequence[Instance]): Instances to sort Raises: ValueError: If the dimension of the genotypes (minus Q) is not even. Note that KP instances should contain N pairs of values plus the capacity. Returns: np.ndarray | Sequence[Instance]: Sorted instances \"\"\" genotypes = np . empty ( 0 ) if isinstance ( instances , np . ndarray ) and ( instances . shape [ 1 ] - 1 ) % 2 != 0 : raise ValueError ( f \"Something is wrong with these KP instances. Shape 1 should be even and got { instances . shape [ 1 ] } \" ) elif dimension := ( len ( instances [ 0 ]) - 1 ) % 2 != 0 : raise ValueError ( f \"Something is wrong with these KP instances. Shape 1 should be even and got { dimension } \" ) genotypes = np . asarray ( instances , copy = True ) M , N = genotypes . shape pairs = genotypes [:, 1 :] . reshape ( M , - 1 , 2 ) order = np . lexsort (( pairs [:, :, 1 ], pairs [:, :, 0 ]), axis = 1 ) sorted_pairs = np . take_along_axis ( pairs , order [:, :, None ], axis = 1 ) genotypes [:, 1 :] = sorted_pairs . reshape ( M , - 1 ) if isinstance ( instances , np . ndarray ): return genotypes else : return [ instances [ i ] . clone_with ( variables = genotypes [ i ]) for i in range ( len ( instances )) ] to_json ( obj ) Convert an object to a JSON string. Source code in digneapy/utils/serializer.py 75 76 77 78 79 def to_json ( obj ): \"\"\" Convert an object to a JSON string. \"\"\" return json . dumps ( serialize ( obj ), cls = CustomJSONEncoder , indent = 4 )","title":"Index"},{"location":"reference/utils/#utils.save_results_to_files","text":"Saves the results of the generation to CSV files. Args: filename_pattern (str): Pattern for the filenames. result (GenResult): Result of the generation. only_instances (bool): Generate only the files with the resulting instances. Default True. If False, it would generate an history and arhice_metrics files. only_genotypes (bool): Extract only the genotype of each instance. Default False (extracts features and portfolio scores). solvers_names (Sequence[str]): Names of the solvers. features_names (Sequence[str]): Names of the features. vars_names (Sequence[str]): Names of the variables. files_format (Literal[str] = \"csv\" or \"parquet\"): Format to store the resulting instances file. Parquet is the most efficient for large datasets. Source code in digneapy/utils/save_data.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def save_results_to_files ( filename_pattern : str , result : GenResult , only_instances : bool = True , only_genotypes : bool = False , solvers_names : Optional [ Sequence [ str ]] = None , features_names : Optional [ Sequence [ str ]] = None , vars_names : Optional [ Sequence [ str ]] = None , files_format : Literal [ \"csv\" , \"parquet\" ] = \"parquet\" , ): \"\"\"Saves the results of the generation to CSV files. Args: filename_pattern (str): Pattern for the filenames. result (GenResult): Result of the generation. only_instances (bool): Generate only the files with the resulting instances. Default True. If False, it would generate an history and arhice_metrics files. only_genotypes (bool): Extract only the genotype of each instance. Default False (extracts features and portfolio scores). solvers_names (Sequence[str]): Names of the solvers. features_names (Sequence[str]): Names of the features. vars_names (Sequence[str]): Names of the variables. files_format (Literal[str] = \"csv\" or \"parquet\"): Format to store the resulting instances file. Parquet is the most efficient for large datasets. \"\"\" if files_format not in ( \"csv\" , \"parquet\" ): print ( f \"Unrecognised file format: { files_format } . Selecting parquet.\" ) files_format = \"parquet\" df = pd . DataFrame ( [ i . to_series ( only_genotype = only_genotypes , variables_names = vars_names , features_names = features_names , score_names = solvers_names , ) for i in result . instances ] ) if not df . empty : df . insert ( 0 , \"target\" , result . target ) if files_format == \"csv\" : df . to_csv ( f \" { filename_pattern } _instances.csv\" , index = False ) elif files_format == \"parquet\" : df . to_parquet ( f \" { filename_pattern } _instances.parquet\" , index = False ) if not only_instances : result . history . to_df () . to_csv ( f \" { filename_pattern } _history.csv\" , index = False ) if result . metrics is not None : result . metrics . to_csv ( f \" { filename_pattern } _archive_metrics.csv\" )","title":"save_results_to_files"},{"location":"reference/utils/#utils.sort_knapsack_instances","text":"sort_knapsack_instances ( instances : np . ndarray , ) -> np . ndarray sort_knapsack_instances ( instances : Sequence [ Instance ], ) -> List [ Instance ] Sorts a collection of Knapsack Instances Genotypes based on lexicograph order by (w_i, p_i) Parameters: instances ( ndarray | Sequence [ Instance ] ) \u2013 Instances to sort Raises: ValueError \u2013 If the dimension of the genotypes (minus Q) is not even. Note that KP instances should contain N pairs of values plus the capacity. Returns: ndarray | List [ Instance ] \u2013 np.ndarray | Sequence[Instance]: Sorted instances Source code in digneapy/utils/sorting.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def sort_knapsack_instances ( instances : np . ndarray | Sequence [ Instance ], ) -> np . ndarray | List [ Instance ]: \"\"\"Sorts a collection of Knapsack Instances Genotypes based on lexicograph order by (w_i, p_i) Args: instances (np.ndarray | Sequence[Instance]): Instances to sort Raises: ValueError: If the dimension of the genotypes (minus Q) is not even. Note that KP instances should contain N pairs of values plus the capacity. Returns: np.ndarray | Sequence[Instance]: Sorted instances \"\"\" genotypes = np . empty ( 0 ) if isinstance ( instances , np . ndarray ) and ( instances . shape [ 1 ] - 1 ) % 2 != 0 : raise ValueError ( f \"Something is wrong with these KP instances. Shape 1 should be even and got { instances . shape [ 1 ] } \" ) elif dimension := ( len ( instances [ 0 ]) - 1 ) % 2 != 0 : raise ValueError ( f \"Something is wrong with these KP instances. Shape 1 should be even and got { dimension } \" ) genotypes = np . asarray ( instances , copy = True ) M , N = genotypes . shape pairs = genotypes [:, 1 :] . reshape ( M , - 1 , 2 ) order = np . lexsort (( pairs [:, :, 1 ], pairs [:, :, 0 ]), axis = 1 ) sorted_pairs = np . take_along_axis ( pairs , order [:, :, None ], axis = 1 ) genotypes [:, 1 :] = sorted_pairs . reshape ( M , - 1 ) if isinstance ( instances , np . ndarray ): return genotypes else : return [ instances [ i ] . clone_with ( variables = genotypes [ i ]) for i in range ( len ( instances )) ]","title":"sort_knapsack_instances"},{"location":"reference/utils/#utils.to_json","text":"Convert an object to a JSON string. Source code in digneapy/utils/serializer.py 75 76 77 78 79 def to_json ( obj ): \"\"\" Convert an object to a JSON string. \"\"\" return json . dumps ( serialize ( obj ), cls = CustomJSONEncoder , indent = 4 )","title":"to_json"},{"location":"reference/utils/_timers/","text":"@File : _timers.py @Time : 2024/06/14 13:45:40 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2024, Alejandro Marrero @Desc : None","title":" timers"},{"location":"reference/utils/save_data/","text":"@File : save_data.py @Time : 2025/04/03 10:02:16 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2025, Alejandro Marrero @Desc : None save_results_to_files ( filename_pattern , result , only_instances = True , only_genotypes = False , solvers_names = None , features_names = None , vars_names = None , files_format = 'parquet' ) Saves the results of the generation to CSV files. Args: filename_pattern (str): Pattern for the filenames. result (GenResult): Result of the generation. only_instances (bool): Generate only the files with the resulting instances. Default True. If False, it would generate an history and arhice_metrics files. only_genotypes (bool): Extract only the genotype of each instance. Default False (extracts features and portfolio scores). solvers_names (Sequence[str]): Names of the solvers. features_names (Sequence[str]): Names of the features. vars_names (Sequence[str]): Names of the variables. files_format (Literal[str] = \"csv\" or \"parquet\"): Format to store the resulting instances file. Parquet is the most efficient for large datasets. Source code in digneapy/utils/save_data.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def save_results_to_files ( filename_pattern : str , result : GenResult , only_instances : bool = True , only_genotypes : bool = False , solvers_names : Optional [ Sequence [ str ]] = None , features_names : Optional [ Sequence [ str ]] = None , vars_names : Optional [ Sequence [ str ]] = None , files_format : Literal [ \"csv\" , \"parquet\" ] = \"parquet\" , ): \"\"\"Saves the results of the generation to CSV files. Args: filename_pattern (str): Pattern for the filenames. result (GenResult): Result of the generation. only_instances (bool): Generate only the files with the resulting instances. Default True. If False, it would generate an history and arhice_metrics files. only_genotypes (bool): Extract only the genotype of each instance. Default False (extracts features and portfolio scores). solvers_names (Sequence[str]): Names of the solvers. features_names (Sequence[str]): Names of the features. vars_names (Sequence[str]): Names of the variables. files_format (Literal[str] = \"csv\" or \"parquet\"): Format to store the resulting instances file. Parquet is the most efficient for large datasets. \"\"\" if files_format not in ( \"csv\" , \"parquet\" ): print ( f \"Unrecognised file format: { files_format } . Selecting parquet.\" ) files_format = \"parquet\" df = pd . DataFrame ( [ i . to_series ( only_genotype = only_genotypes , variables_names = vars_names , features_names = features_names , score_names = solvers_names , ) for i in result . instances ] ) if not df . empty : df . insert ( 0 , \"target\" , result . target ) if files_format == \"csv\" : df . to_csv ( f \" { filename_pattern } _instances.csv\" , index = False ) elif files_format == \"parquet\" : df . to_parquet ( f \" { filename_pattern } _instances.parquet\" , index = False ) if not only_instances : result . history . to_df () . to_csv ( f \" { filename_pattern } _history.csv\" , index = False ) if result . metrics is not None : result . metrics . to_csv ( f \" { filename_pattern } _archive_metrics.csv\" )","title":"Save data"},{"location":"reference/utils/save_data/#utils.save_data.save_results_to_files","text":"Saves the results of the generation to CSV files. Args: filename_pattern (str): Pattern for the filenames. result (GenResult): Result of the generation. only_instances (bool): Generate only the files with the resulting instances. Default True. If False, it would generate an history and arhice_metrics files. only_genotypes (bool): Extract only the genotype of each instance. Default False (extracts features and portfolio scores). solvers_names (Sequence[str]): Names of the solvers. features_names (Sequence[str]): Names of the features. vars_names (Sequence[str]): Names of the variables. files_format (Literal[str] = \"csv\" or \"parquet\"): Format to store the resulting instances file. Parquet is the most efficient for large datasets. Source code in digneapy/utils/save_data.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def save_results_to_files ( filename_pattern : str , result : GenResult , only_instances : bool = True , only_genotypes : bool = False , solvers_names : Optional [ Sequence [ str ]] = None , features_names : Optional [ Sequence [ str ]] = None , vars_names : Optional [ Sequence [ str ]] = None , files_format : Literal [ \"csv\" , \"parquet\" ] = \"parquet\" , ): \"\"\"Saves the results of the generation to CSV files. Args: filename_pattern (str): Pattern for the filenames. result (GenResult): Result of the generation. only_instances (bool): Generate only the files with the resulting instances. Default True. If False, it would generate an history and arhice_metrics files. only_genotypes (bool): Extract only the genotype of each instance. Default False (extracts features and portfolio scores). solvers_names (Sequence[str]): Names of the solvers. features_names (Sequence[str]): Names of the features. vars_names (Sequence[str]): Names of the variables. files_format (Literal[str] = \"csv\" or \"parquet\"): Format to store the resulting instances file. Parquet is the most efficient for large datasets. \"\"\" if files_format not in ( \"csv\" , \"parquet\" ): print ( f \"Unrecognised file format: { files_format } . Selecting parquet.\" ) files_format = \"parquet\" df = pd . DataFrame ( [ i . to_series ( only_genotype = only_genotypes , variables_names = vars_names , features_names = features_names , score_names = solvers_names , ) for i in result . instances ] ) if not df . empty : df . insert ( 0 , \"target\" , result . target ) if files_format == \"csv\" : df . to_csv ( f \" { filename_pattern } _instances.csv\" , index = False ) elif files_format == \"parquet\" : df . to_parquet ( f \" { filename_pattern } _instances.parquet\" , index = False ) if not only_instances : result . history . to_df () . to_csv ( f \" { filename_pattern } _history.csv\" , index = False ) if result . metrics is not None : result . metrics . to_csv ( f \" { filename_pattern } _archive_metrics.csv\" )","title":"save_results_to_files"},{"location":"reference/utils/serializer/","text":"@File : serializer.py @Time : 2025/04/02 16:00:58 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2025, Alejandro Marrero @Desc : None CustomJSONEncoder Bases: JSONEncoder Custom JSON encoder to handle complex types like NumPy arrays and custom objects. Source code in digneapy/utils/serializer.py 63 64 65 66 67 68 69 70 71 72 class CustomJSONEncoder ( json . JSONEncoder ): \"\"\" Custom JSON encoder to handle complex types like NumPy arrays and custom objects. \"\"\" def default ( self , obj ): try : return serialize ( obj ) except TypeError : return super () . default ( obj ) serialize ( obj ) Recursively serialize an object to a dictionary. Handles nested objects, lists, and dictionaries. Source code in digneapy/utils/serializer.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def serialize ( obj ): \"\"\" Recursively serialize an object to a dictionary. Handles nested objects, lists, and dictionaries. \"\"\" if isinstance ( obj , ( int , float , str , bool , type ( None ))): return obj # Primitive types are directly serializable if ( isinstance ( obj , ( FunctionType )) or type ( obj ) . __name__ == \"cython_function_or_method\" ): return obj . __name__ if isinstance ( obj , ( list , tuple , dict )): return [ serialize ( item ) for item in obj ] # if isinstance(obj, (list, tuple)) or isinstance(obj, dict): # return [ # serialize(item) for item in obj # ] # Serialize each element in the list/tuple if isinstance ( obj , dict ): return { key : serialize ( value ) for key , value in obj . items () } # Serialize each key-value pair if isinstance ( obj , np . ndarray ): return obj . tolist () # Convert NumPy arrays to lists if hasattr ( obj , \"__dict__\" ): # Handle custom objects return { key : serialize ( value ) for key , value in vars ( obj ) . items () if not key . startswith ( \"_\" ) } if hasattr ( obj , \"__slots__\" ): return { slot : serialize ( getattr ( obj , slot )) for slot in obj . __slots__ } return str ( obj ) # Fallback: Convert to string for unsupported types to_json ( obj ) Convert an object to a JSON string. Source code in digneapy/utils/serializer.py 75 76 77 78 79 def to_json ( obj ): \"\"\" Convert an object to a JSON string. \"\"\" return json . dumps ( serialize ( obj ), cls = CustomJSONEncoder , indent = 4 )","title":"Serializer"},{"location":"reference/utils/serializer/#utils.serializer.CustomJSONEncoder","text":"Bases: JSONEncoder Custom JSON encoder to handle complex types like NumPy arrays and custom objects. Source code in digneapy/utils/serializer.py 63 64 65 66 67 68 69 70 71 72 class CustomJSONEncoder ( json . JSONEncoder ): \"\"\" Custom JSON encoder to handle complex types like NumPy arrays and custom objects. \"\"\" def default ( self , obj ): try : return serialize ( obj ) except TypeError : return super () . default ( obj )","title":"CustomJSONEncoder"},{"location":"reference/utils/serializer/#utils.serializer.serialize","text":"Recursively serialize an object to a dictionary. Handles nested objects, lists, and dictionaries. Source code in digneapy/utils/serializer.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def serialize ( obj ): \"\"\" Recursively serialize an object to a dictionary. Handles nested objects, lists, and dictionaries. \"\"\" if isinstance ( obj , ( int , float , str , bool , type ( None ))): return obj # Primitive types are directly serializable if ( isinstance ( obj , ( FunctionType )) or type ( obj ) . __name__ == \"cython_function_or_method\" ): return obj . __name__ if isinstance ( obj , ( list , tuple , dict )): return [ serialize ( item ) for item in obj ] # if isinstance(obj, (list, tuple)) or isinstance(obj, dict): # return [ # serialize(item) for item in obj # ] # Serialize each element in the list/tuple if isinstance ( obj , dict ): return { key : serialize ( value ) for key , value in obj . items () } # Serialize each key-value pair if isinstance ( obj , np . ndarray ): return obj . tolist () # Convert NumPy arrays to lists if hasattr ( obj , \"__dict__\" ): # Handle custom objects return { key : serialize ( value ) for key , value in vars ( obj ) . items () if not key . startswith ( \"_\" ) } if hasattr ( obj , \"__slots__\" ): return { slot : serialize ( getattr ( obj , slot )) for slot in obj . __slots__ } return str ( obj ) # Fallback: Convert to string for unsupported types","title":"serialize"},{"location":"reference/utils/serializer/#utils.serializer.to_json","text":"Convert an object to a JSON string. Source code in digneapy/utils/serializer.py 75 76 77 78 79 def to_json ( obj ): \"\"\" Convert an object to a JSON string. \"\"\" return json . dumps ( serialize ( obj ), cls = CustomJSONEncoder , indent = 4 )","title":"to_json"},{"location":"reference/utils/sorting/","text":"@File : sorting.py @Time : 2025/10/17 16:34:46 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2025, Alejandro Marrero @Desc : None sort_knapsack_instances ( instances ) sort_knapsack_instances ( instances : np . ndarray , ) -> np . ndarray sort_knapsack_instances ( instances : Sequence [ Instance ], ) -> List [ Instance ] Sorts a collection of Knapsack Instances Genotypes based on lexicograph order by (w_i, p_i) Parameters: instances ( ndarray | Sequence [ Instance ] ) \u2013 Instances to sort Raises: ValueError \u2013 If the dimension of the genotypes (minus Q) is not even. Note that KP instances should contain N pairs of values plus the capacity. Returns: ndarray | List [ Instance ] \u2013 np.ndarray | Sequence[Instance]: Sorted instances Source code in digneapy/utils/sorting.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def sort_knapsack_instances ( instances : np . ndarray | Sequence [ Instance ], ) -> np . ndarray | List [ Instance ]: \"\"\"Sorts a collection of Knapsack Instances Genotypes based on lexicograph order by (w_i, p_i) Args: instances (np.ndarray | Sequence[Instance]): Instances to sort Raises: ValueError: If the dimension of the genotypes (minus Q) is not even. Note that KP instances should contain N pairs of values plus the capacity. Returns: np.ndarray | Sequence[Instance]: Sorted instances \"\"\" genotypes = np . empty ( 0 ) if isinstance ( instances , np . ndarray ) and ( instances . shape [ 1 ] - 1 ) % 2 != 0 : raise ValueError ( f \"Something is wrong with these KP instances. Shape 1 should be even and got { instances . shape [ 1 ] } \" ) elif dimension := ( len ( instances [ 0 ]) - 1 ) % 2 != 0 : raise ValueError ( f \"Something is wrong with these KP instances. Shape 1 should be even and got { dimension } \" ) genotypes = np . asarray ( instances , copy = True ) M , N = genotypes . shape pairs = genotypes [:, 1 :] . reshape ( M , - 1 , 2 ) order = np . lexsort (( pairs [:, :, 1 ], pairs [:, :, 0 ]), axis = 1 ) sorted_pairs = np . take_along_axis ( pairs , order [:, :, None ], axis = 1 ) genotypes [:, 1 :] = sorted_pairs . reshape ( M , - 1 ) if isinstance ( instances , np . ndarray ): return genotypes else : return [ instances [ i ] . clone_with ( variables = genotypes [ i ]) for i in range ( len ( instances )) ]","title":"Sorting"},{"location":"reference/utils/sorting/#utils.sorting.sort_knapsack_instances","text":"sort_knapsack_instances ( instances : np . ndarray , ) -> np . ndarray sort_knapsack_instances ( instances : Sequence [ Instance ], ) -> List [ Instance ] Sorts a collection of Knapsack Instances Genotypes based on lexicograph order by (w_i, p_i) Parameters: instances ( ndarray | Sequence [ Instance ] ) \u2013 Instances to sort Raises: ValueError \u2013 If the dimension of the genotypes (minus Q) is not even. Note that KP instances should contain N pairs of values plus the capacity. Returns: ndarray | List [ Instance ] \u2013 np.ndarray | Sequence[Instance]: Sorted instances Source code in digneapy/utils/sorting.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def sort_knapsack_instances ( instances : np . ndarray | Sequence [ Instance ], ) -> np . ndarray | List [ Instance ]: \"\"\"Sorts a collection of Knapsack Instances Genotypes based on lexicograph order by (w_i, p_i) Args: instances (np.ndarray | Sequence[Instance]): Instances to sort Raises: ValueError: If the dimension of the genotypes (minus Q) is not even. Note that KP instances should contain N pairs of values plus the capacity. Returns: np.ndarray | Sequence[Instance]: Sorted instances \"\"\" genotypes = np . empty ( 0 ) if isinstance ( instances , np . ndarray ) and ( instances . shape [ 1 ] - 1 ) % 2 != 0 : raise ValueError ( f \"Something is wrong with these KP instances. Shape 1 should be even and got { instances . shape [ 1 ] } \" ) elif dimension := ( len ( instances [ 0 ]) - 1 ) % 2 != 0 : raise ValueError ( f \"Something is wrong with these KP instances. Shape 1 should be even and got { dimension } \" ) genotypes = np . asarray ( instances , copy = True ) M , N = genotypes . shape pairs = genotypes [:, 1 :] . reshape ( M , - 1 , 2 ) order = np . lexsort (( pairs [:, :, 1 ], pairs [:, :, 0 ]), axis = 1 ) sorted_pairs = np . take_along_axis ( pairs , order [:, :, None ], axis = 1 ) genotypes [:, 1 :] = sorted_pairs . reshape ( M , - 1 ) if isinstance ( instances , np . ndarray ): return genotypes else : return [ instances [ i ] . clone_with ( variables = genotypes [ i ]) for i in range ( len ( instances )) ]","title":"sort_knapsack_instances"},{"location":"reference/visualize/","text":"@File : init .py @Time : 2024/09/18 11:03:14 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2024, Alejandro Marrero @Desc : None","title":"Index"},{"location":"reference/visualize/_evo_generator_evolution_plot/","text":"@File : _evo_generator_evolution_plot.py @Time : 2024/09/18 11:03:49 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2024, Alejandro Marrero @Desc : None","title":" evo generator evolution plot"},{"location":"reference/visualize/_map_elites_evolution_plot/","text":"@File : _map_elites_evolution_plot.py @Time : 2024/09/18 11:04:14 @Author : Alejandro Marrero @Version : 1.0 @Contact : amarrerd@ull.edu.es @License : (C)Copyright 2024, Alejandro Marrero @Desc : None","title":" map elites evolution plot"},{"location":"tutorial/01_eig_example/","text":"Instance Generation Example Instance Generation Example Generating Knapsack Problem Instances with DIGNEApy How to Generating Knapsack Problem Instances with DIGNEApy Here's an example of how to generated diverse and discriminatory instances for the Knapsack Problem . We will be creating a set of KP instances which will be adapted to the performance of a specific heuristic. For this experiment we will: Define EAGenerator's parameter configuration. Set the dimension of the KP instances to generate. Specify the portfolio of algorithms to solve the instances. In this case, four deterministic heuristics. Run the experiment. Finally, collect the results in a Pandas DataFrame. domain = KnapsackDomain(50, capacity_approach=\"percentage\") eig = EAGenerator( pop_size=128, generations=100, domain=domain, portfolio=[default_kp, map_kp, miw_kp, mpw_kp], novelty_approach=NS(Archive(threshold=3.0), k=15), solution_set=Archive(threshold=3.0), repetitions=1, descriptor_strategy='features', replacement=generational_replacement, ) result = eig() df = pd.DataFrame(list(i.to_series() for i in result.instances)) df.insert(0, \"target\", result.target) How to Run an experiment Create a domain Create a solver","title":"Run an experiment"},{"location":"tutorial/01_eig_example/#instance-generation-example","text":"Instance Generation Example Generating Knapsack Problem Instances with DIGNEApy How to","title":"Instance Generation Example"},{"location":"tutorial/01_eig_example/#generating-knapsack-problem-instances-with-digneapy","text":"Here's an example of how to generated diverse and discriminatory instances for the Knapsack Problem . We will be creating a set of KP instances which will be adapted to the performance of a specific heuristic. For this experiment we will: Define EAGenerator's parameter configuration. Set the dimension of the KP instances to generate. Specify the portfolio of algorithms to solve the instances. In this case, four deterministic heuristics. Run the experiment. Finally, collect the results in a Pandas DataFrame. domain = KnapsackDomain(50, capacity_approach=\"percentage\") eig = EAGenerator( pop_size=128, generations=100, domain=domain, portfolio=[default_kp, map_kp, miw_kp, mpw_kp], novelty_approach=NS(Archive(threshold=3.0), k=15), solution_set=Archive(threshold=3.0), repetitions=1, descriptor_strategy='features', replacement=generational_replacement, ) result = eig() df = pd.DataFrame(list(i.to_series() for i in result.instances)) df.insert(0, \"target\", result.target)","title":"Generating Knapsack Problem Instances with DIGNEApy"},{"location":"tutorial/01_eig_example/#how-to","text":"Run an experiment Create a domain Create a solver","title":"How to"},{"location":"tutorial/02_create_domain/","text":"Create a new domain Create a new domain Creating the Travelling Salesman Problem (TSP) Creating the Travelling Salesman Problem Domain (TSPDomain) How to This tutorial will show how to include a new domain in DIGNEApy. For this purpose, we will create the Travelling Salesman Problem domain. It is important to remark that the addition of a new domain requires the definition of the Problem and Domain subclasses for such specific domain. Creating the Travelling Salesman Problem (TSP) Let's start by creating a Travelling Salesman Problem class which will allow us to solve TSP instances. The TSP class must be a subclass of the Problem class and define (at least) the __call__ , evaluate , create_solution and to_instance methods. For this example, we should considering some domain-dependent attributes such as the number of nodes and the coordinates of each node in the instance. class TSP(Problem): \"\"\"Symmetric Travelling Salesman Problem\"\"\" def __init__( self, nodes: int, coords: Tuple[Tuple[int, int], ...], seed: int = 42, *args, **kwargs, ): \"\"\"Creates a new Symmetric Travelling Salesman Problem Args: nodes (int): Number of nodes/cities in the instance to solve coords (Tuple[Tuple[int, int], ...]): Coordinates of each node/city. \"\"\" self._nodes = nodes self._coords = np.array(coords) x_min, y_min = np.min(self._coords, axis=0) x_max, y_max = np.max(self._coords, axis=0) bounds = list(((x_min, y_min), (x_max, y_max)) for _ in range(self._nodes)) super().__init__(dimension=nodes, bounds=bounds, name=\"TSP\", seed=seed) self._distances = np.zeros((self._nodes, self._nodes)) for i in range(self._nodes): for j in range(i + 1, self._nodes): self._distances[i][j] = np.linalg.norm( self._coords[i] - self._coords[j] ) self._distances[j][i] = self._distances[i][j] Then, we could implement the evaluation method. This can be written directly in the __call__ method or alternatively you could split the evaluation in two methods like: def __evaluate_constraints(self, individual: Sequence | Solution) -> bool: counter = Counter(individual) if any(counter[c] != 1 for c in counter if c != 0) or ( individual[0] != 0 or individual[-1] != 0 ): return False return True and def evaluate(self, individual: Sequence | Solution) -> tuple[float]: \"\"\"Evaluates the candidate individual with the information of the Travelling Salesmas Problem. The fitness of the solution is the fraction of the sum of the distances of the tour Args: individual (Sequence | Solution): Individual to evaluate Returns: Tuple[float]: Fitness \"\"\" if len(individual) != self._nodes + 1: msg = f\"Mismatch between individual variables ({len(individual)}) and instance variables ({self._nodes}) in {self.__class__.__name__}. A solution for the TSP must be a sequence of len {self._nodes + 1}\" raise ValueError(msg) penalty: np.float64 = np.float64(0) if self.__evaluate_constraints(individual): distance: float = 0.0 for i in range(len(individual) - 2): distance += self._distances[individual[i]][individual[i + 1]] fitness = 1.0 / distance else: fitness = 2.938736e-39 # --> 1.0 / np.float.max penalty = np.finfo(np.float64).max if isinstance(individual, Solution): individual.fitness = fitness individual.objectives = (fitness,) individual.constraints = (penalty,) return (fitness,) def __call__(self, individual: Sequence | Solution) -> tuple[float]: return self.evaluate(individual) We must implement the method create_solution so we can initialise the algorithms solutions. def create_solution(self) -> Solution: items = [0] + list(range(1, self._nodes)) + [0] return Solution(chromosome=items) To cast the TSP problem to an actual evolvable instance we must define the to_instance method. In this example, the variables/chromosome of the Instance is the coordinates of the nodes. Note that for each domain, the chromosome of the instances must be adapted to the relevant information. def to_instance(self) -> Instance: return Instance(variables=self._coords.flatten()) Finally, the whole TSP class with several added methods looks like this: class TSP(Problem): \"\"\"Symmetric Travelling Salesman Problem\"\"\" def __init__( self, nodes: int, coords: Tuple[Tuple[int, int], ...], seed: int = 42, *args, **kwargs, ): \"\"\"Creates a new Symmetric Travelling Salesman Problem Args: nodes (int): Number of nodes/cities in the instance to solve coords (Tuple[Tuple[int, int], ...]): Coordinates of each node/city. \"\"\" self._nodes = nodes self._coords = np.array(coords) x_min, y_min = np.min(self._coords, axis=0) x_max, y_max = np.max(self._coords, axis=0) bounds = list(((x_min, y_min), (x_max, y_max)) for _ in range(self._nodes)) super().__init__(dimension=nodes, bounds=bounds, name=\"TSP\", seed=seed) self._distances = np.zeros((self._nodes, self._nodes)) for i in range(self._nodes): for j in range(i + 1, self._nodes): self._distances[i][j] = np.linalg.norm( self._coords[i] - self._coords[j] ) self._distances[j][i] = self._distances[i][j] def __evaluate_constraints(self, individual: Sequence | Solution) -> bool: counter = Counter(individual) if any(counter[c] != 1 for c in counter if c != 0) or ( individual[0] != 0 or individual[-1] != 0 ): return False return True def evaluate(self, individual: Sequence | Solution) -> tuple[float]: \"\"\"Evaluates the candidate individual with the information of the Travelling Salesmas Problem. The fitness of the solution is the fraction of the sum of the distances of the tour Args: individual (Sequence | Solution): Individual to evaluate Returns: Tuple[float]: Fitness \"\"\" if len(individual) != self._nodes + 1: msg = f\"Mismatch between individual variables ({len(individual)}) and instance variables ({self._nodes}) in {self.__class__.__name__}. A solution for the TSP must be a sequence of len {self._nodes + 1}\" raise ValueError(msg) penalty: np.float64 = np.float64(0) if self.__evaluate_constraints(individual): distance: float = 0.0 for i in range(len(individual) - 2): distance += self._distances[individual[i]][individual[i + 1]] fitness = 1.0 / distance else: fitness = 2.938736e-39 # --> 1.0 / np.float.max penalty = np.finfo(np.float64).max if isinstance(individual, Solution): individual.fitness = fitness individual.objectives = (fitness,) individual.constraints = (penalty,) return (fitness,) def __call__(self, individual: Sequence | Solution) -> tuple[float]: return self.evaluate(individual) def __repr__(self): return f\"TSP<n={self._nodes}>\" def __len__(self): return self._nodes def create_solution(self) -> Solution: items = [0] + list(range(1, self._nodes)) + [0] return Solution(chromosome=items) def to_file(self, filename: str = \"instance.tsp\"): with open(filename, \"w\") as file: file.write(f\"{len(self)}\\n\\n\") content = \"\\n\".join(f\"{x}\\t{y}\" for (x, y) in self._coords) file.write(content) @classmethod def from_file(cls, filename: str) -> Self: with open(filename) as f: lines = f.readlines() lines = [line.rstrip() for line in lines] nodes = int(lines[0]) coords = [] for line in lines[2:]: x, y = line.split() coords.append((int(x), int(y))) return cls(nodes=nodes, coords=tuple(coords)) def to_instance(self) -> Instance: return Instance(variables=self._coords.flatten()) Creating the Travelling Salesman Problem Domain (TSPDomain) Once we have created the optimisation problem, we can define the domain of such problem. In this example, we call it TSPDomain. Every domain must define (at least) the generate_instances , extract_features , extract_features_as_dict and from_instance methods. The features methods are only relevant if you are planning to use the generators with the features-based descriptors. Otherwise you can return NotImplemented. A domain should include all the attributes and relevant information to generate the instances with particular characteristics. In this example, we need the dimension of the instances (number of nodes) and the ranges for the coordinates (xmin, xmax) and (ymin, ymax) for each node in the instance. class TSPDomain(Domain): \"\"\"Domain to generate instances for the Symmetric Travelling Salesman Problem.\"\"\" def __init__( self, dimension: int = 100, x_range: Tuple[int, int] = (0, 1000), y_range: Tuple[int, int] = (0, 1000), seed: int = 42, ): \"\"\"Creates a new TSPDomain to generate instances for the Symmetric Travelling Salesman Problem Args: dimension (int, optional): Dimension of the instances to generate. Defaults to 100. x_range (Tuple[int, int], optional): Ranges for the Xs coordinates of each node/city. Defaults to (0, 1000). y_range (Tuple[int, int], optional): Ranges for the ys coordinates of each node/city. Defaults to (0, 1000). Raises: ValueError: If dimension is < 0 ValueError: If x_range OR y_range does not have 2 dimensions each ValueError: If minimum ranges are greater than maximum ranges \"\"\" if dimension < 0: raise ValueError(f\"Expected dimension > 0 got {dimension}\") if len(x_range) != 2 or len(y_range) != 2: raise ValueError( f\"Expected x_range and y_range to be a tuple with only to integers. Got: x_range = {x_range} and y_range = {y_range}\" ) x_min, x_max = x_range y_min, y_max = y_range if x_min < 0 or x_max <= x_min: raise ValueError( f\"Expected x_range to be (x_min, x_max) where x_min >= 0 and x_max > x_min. Got: x_range {x_range}\" ) if y_min < 0 or y_max <= y_min: raise ValueError( f\"Expected y_range to be (y_min, y_max) where y_min >= 0 and y_max > y_min. Got: y_range {y_range}\" ) self._x_range = x_range self._y_range = y_range __bounds = [ (x_min, x_max) if i % 2 == 0 else (y_min, y_max) for i in range(dimension * 2) ] super().__init__(dimension=dimension, bounds=__bounds, name=\"TSP\", seed=seed) The generate_instances method is quite similar to the create_solution in the Problem class. Basically, it creates a random instance with the characteristics defined in the domain. def generate_instances(self) -> Instance: \"\"\"Generates a new instances for the TSP domain Returns: Instance: New randomly generated instance \"\"\" coords = self._rng.integers( low=(self._x_range[0], self._y_range[0]), high=(self._x_range[1], self._y_range[1]), size=(self.dimension, 2), dtype=int, ) coords = coords.flatten() return Instance(variables=coords) Likewise, the from_instance methods allows the domain to create an optimisation problem that can be solve using the definition of a particular instance. def from_instance(self, instance: Instance) -> TSP: n_nodes = len(instance) // 2 coords = tuple([*zip(instance[::2], instance[1::2])]) return TSP(nodes=n_nodes, coords=coords) Finally, the extract_features methods are domain and user depedent since the features to extract may vary from everyone needs. For this example the methods looks like: def extract_features(self, instance: Instance) -> tuple: \"\"\"Extract the features of the instance based on the TSP domain. For the TSP the features are: - Size - Standard deviation of the distances - Centroid coordinates - Radius of the instance - Fraction of distinct instances - Rectangular area - Variance of the normalised nearest neighbours distances - Coefficient of variantion of the nearest neighbours distances - Cluster ratio - Mean cluster radius Args: instance (Instance): Instance to extract the features from Returns: Tuple[float]: Values of each feature \"\"\" tsp = self.from_instance(instance) xs = instance[0::2] ys = instance[1::2] area = (max(xs) - min(xs)) * (max(ys) - min(ys)) std_distances = np.std(tsp._distances) centroid = (np.mean(xs), np.mean(ys)) # (0.01 * np.sum(xs), 0.01 * np.sum(ys)) centroid_distance = [np.linalg.norm(city - centroid) for city in tsp._coords] radius = np.mean(centroid_distance) fraction = len(np.unique(tsp._distances)) / (len(tsp._distances) / 2) # Top five only norm_distances = np.sort(tsp._distances)[::-1][:5] / np.max(tsp._distances) variance_nnds = np.var(norm_distances) variation_nnds = variance_nnds / np.mean(norm_distances) dbscan = DBSCAN() dbscan.fit(tsp._coords) cluster_ratio = len(set(dbscan.labels_)) / self.dimension # Cluster radius mean_cluster_radius = 0.0 for label_id in dbscan.labels_: points_in_cluster = tsp._coords[dbscan.labels_ == label_id] cluster_centroid = ( np.mean(points_in_cluster[:, 0]), np.mean(points_in_cluster[:, 1]), ) mean_cluster_radius = np.mean( [np.linalg.norm(city - cluster_centroid) for city in tsp._coords] ) mean_cluster_radius /= len(set(dbscan.labels_)) return ( self.dimension, std_distances, centroid[0], centroid[1], radius, fraction, area, variance_nnds, variation_nnds, cluster_ratio, mean_cluster_radius, ) def extract_features_as_dict(self, instance: Instance) -> Mapping[str, float]: \"\"\"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Args: instance (Instance): Instance to extract the features from Returns: Mapping[str, float]: Dictionary with the names/values of each feature \"\"\" names = \"size,std_distances,centroid_x,centroid_y,radius,fraction_distances,area,variance_nnNds,variation_nnNds,cluster_ratio,mean_cluster_radius\" features = self.extract_features(instance) return {k: v for k, v in zip(names.split(\",\"), features)} The complete TSPDomain class looks like: class TSPDomain(Domain): \"\"\"Domain to generate instances for the Symmetric Travelling Salesman Problem.\"\"\" def __init__( self, dimension: int = 100, x_range: Tuple[int, int] = (0, 1000), y_range: Tuple[int, int] = (0, 1000), seed: int = 42, ): \"\"\"Creates a new TSPDomain to generate instances for the Symmetric Travelling Salesman Problem Args: dimension (int, optional): Dimension of the instances to generate. Defaults to 100. x_range (Tuple[int, int], optional): Ranges for the Xs coordinates of each node/city. Defaults to (0, 1000). y_range (Tuple[int, int], optional): Ranges for the ys coordinates of each node/city. Defaults to (0, 1000). Raises: ValueError: If dimension is < 0 ValueError: If x_range OR y_range does not have 2 dimensions each ValueError: If minimum ranges are greater than maximum ranges \"\"\" if dimension < 0: raise ValueError(f\"Expected dimension > 0 got {dimension}\") if len(x_range) != 2 or len(y_range) != 2: raise ValueError( f\"Expected x_range and y_range to be a tuple with only to integers. Got: x_range = {x_range} and y_range = {y_range}\" ) x_min, x_max = x_range y_min, y_max = y_range if x_min < 0 or x_max <= x_min: raise ValueError( f\"Expected x_range to be (x_min, x_max) where x_min >= 0 and x_max > x_min. Got: x_range {x_range}\" ) if y_min < 0 or y_max <= y_min: raise ValueError( f\"Expected y_range to be (y_min, y_max) where y_min >= 0 and y_max > y_min. Got: y_range {y_range}\" ) self._x_range = x_range self._y_range = y_range __bounds = [ (x_min, x_max) if i % 2 == 0 else (y_min, y_max) for i in range(dimension * 2) ] super().__init__(dimension=dimension, bounds=__bounds, name=\"TSP\", seed=seed) def generate_instances(self) -> Instance: \"\"\"Generates a new instances for the TSP domain Returns: Instance: New randomly generated instance \"\"\" coords = self._rng.integers( low=(self._x_range[0], self._y_range[0]), high=(self._x_range[1], self._y_range[1]), size=(self.dimension, 2), dtype=int, ) coords = coords.flatten() return Instance(coords) def extract_features(self, instance: Instance) -> tuple: \"\"\"Extract the features of the instance based on the TSP domain. For the TSP the features are: - Size - Standard deviation of the distances - Centroid coordinates - Radius of the instance - Fraction of distinct instances - Rectangular area - Variance of the normalised nearest neighbours distances - Coefficient of variantion of the nearest neighbours distances - Cluster ratio - Mean cluster radius Args: instance (Instance): Instance to extract the features from Returns: Tuple[float]: Values of each feature \"\"\" tsp = self.from_instance(instance) xs = instance[0::2] ys = instance[1::2] area = (max(xs) - min(xs)) * (max(ys) - min(ys)) std_distances = np.std(tsp._distances) centroid = (np.mean(xs), np.mean(ys)) # (0.01 * np.sum(xs), 0.01 * np.sum(ys)) centroid_distance = [np.linalg.norm(city - centroid) for city in tsp._coords] radius = np.mean(centroid_distance) fraction = len(np.unique(tsp._distances)) / (len(tsp._distances) / 2) # Top five only norm_distances = np.sort(tsp._distances)[::-1][:5] / np.max(tsp._distances) variance_nnds = np.var(norm_distances) variation_nnds = variance_nnds / np.mean(norm_distances) dbscan = DBSCAN() dbscan.fit(tsp._coords) cluster_ratio = len(set(dbscan.labels_)) / self.dimension # Cluster radius mean_cluster_radius = 0.0 for label_id in dbscan.labels_: points_in_cluster = tsp._coords[dbscan.labels_ == label_id] cluster_centroid = ( np.mean(points_in_cluster[:, 0]), np.mean(points_in_cluster[:, 1]), ) mean_cluster_radius = np.mean( [np.linalg.norm(city - cluster_centroid) for city in tsp._coords] ) mean_cluster_radius /= len(set(dbscan.labels_)) return ( self.dimension, std_distances, centroid[0], centroid[1], radius, fraction, area, variance_nnds, variation_nnds, cluster_ratio, mean_cluster_radius, ) def extract_features_as_dict(self, instance: Instance) -> Mapping[str, float]: \"\"\"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Args: instance (Instance): Instance to extract the features from Returns: Mapping[str, float]: Dictionary with the names/values of each feature \"\"\" names = \"size,std_distances,centroid_x,centroid_y,radius,fraction_distances,area,variance_nnNds,variation_nnNds,cluster_ratio,mean_cluster_radius\" features = self.extract_features(instance) return {k: v for k, v in zip(names.split(\",\"), features)} def from_instance(self, instance: Instance) -> TSP: n_nodes = len(instance) // 2 coords = tuple([*zip(instance[::2], instance[1::2])]) return TSP(nodes=n_nodes, coords=coords) How to Run an experiment Create a domain Create a solver","title":"Create a domain"},{"location":"tutorial/02_create_domain/#create-a-new-domain","text":"Create a new domain Creating the Travelling Salesman Problem (TSP) Creating the Travelling Salesman Problem Domain (TSPDomain) How to This tutorial will show how to include a new domain in DIGNEApy. For this purpose, we will create the Travelling Salesman Problem domain. It is important to remark that the addition of a new domain requires the definition of the Problem and Domain subclasses for such specific domain.","title":"Create a new domain"},{"location":"tutorial/02_create_domain/#creating-the-travelling-salesman-problem-tsp","text":"Let's start by creating a Travelling Salesman Problem class which will allow us to solve TSP instances. The TSP class must be a subclass of the Problem class and define (at least) the __call__ , evaluate , create_solution and to_instance methods. For this example, we should considering some domain-dependent attributes such as the number of nodes and the coordinates of each node in the instance. class TSP(Problem): \"\"\"Symmetric Travelling Salesman Problem\"\"\" def __init__( self, nodes: int, coords: Tuple[Tuple[int, int], ...], seed: int = 42, *args, **kwargs, ): \"\"\"Creates a new Symmetric Travelling Salesman Problem Args: nodes (int): Number of nodes/cities in the instance to solve coords (Tuple[Tuple[int, int], ...]): Coordinates of each node/city. \"\"\" self._nodes = nodes self._coords = np.array(coords) x_min, y_min = np.min(self._coords, axis=0) x_max, y_max = np.max(self._coords, axis=0) bounds = list(((x_min, y_min), (x_max, y_max)) for _ in range(self._nodes)) super().__init__(dimension=nodes, bounds=bounds, name=\"TSP\", seed=seed) self._distances = np.zeros((self._nodes, self._nodes)) for i in range(self._nodes): for j in range(i + 1, self._nodes): self._distances[i][j] = np.linalg.norm( self._coords[i] - self._coords[j] ) self._distances[j][i] = self._distances[i][j] Then, we could implement the evaluation method. This can be written directly in the __call__ method or alternatively you could split the evaluation in two methods like: def __evaluate_constraints(self, individual: Sequence | Solution) -> bool: counter = Counter(individual) if any(counter[c] != 1 for c in counter if c != 0) or ( individual[0] != 0 or individual[-1] != 0 ): return False return True and def evaluate(self, individual: Sequence | Solution) -> tuple[float]: \"\"\"Evaluates the candidate individual with the information of the Travelling Salesmas Problem. The fitness of the solution is the fraction of the sum of the distances of the tour Args: individual (Sequence | Solution): Individual to evaluate Returns: Tuple[float]: Fitness \"\"\" if len(individual) != self._nodes + 1: msg = f\"Mismatch between individual variables ({len(individual)}) and instance variables ({self._nodes}) in {self.__class__.__name__}. A solution for the TSP must be a sequence of len {self._nodes + 1}\" raise ValueError(msg) penalty: np.float64 = np.float64(0) if self.__evaluate_constraints(individual): distance: float = 0.0 for i in range(len(individual) - 2): distance += self._distances[individual[i]][individual[i + 1]] fitness = 1.0 / distance else: fitness = 2.938736e-39 # --> 1.0 / np.float.max penalty = np.finfo(np.float64).max if isinstance(individual, Solution): individual.fitness = fitness individual.objectives = (fitness,) individual.constraints = (penalty,) return (fitness,) def __call__(self, individual: Sequence | Solution) -> tuple[float]: return self.evaluate(individual) We must implement the method create_solution so we can initialise the algorithms solutions. def create_solution(self) -> Solution: items = [0] + list(range(1, self._nodes)) + [0] return Solution(chromosome=items) To cast the TSP problem to an actual evolvable instance we must define the to_instance method. In this example, the variables/chromosome of the Instance is the coordinates of the nodes. Note that for each domain, the chromosome of the instances must be adapted to the relevant information. def to_instance(self) -> Instance: return Instance(variables=self._coords.flatten()) Finally, the whole TSP class with several added methods looks like this: class TSP(Problem): \"\"\"Symmetric Travelling Salesman Problem\"\"\" def __init__( self, nodes: int, coords: Tuple[Tuple[int, int], ...], seed: int = 42, *args, **kwargs, ): \"\"\"Creates a new Symmetric Travelling Salesman Problem Args: nodes (int): Number of nodes/cities in the instance to solve coords (Tuple[Tuple[int, int], ...]): Coordinates of each node/city. \"\"\" self._nodes = nodes self._coords = np.array(coords) x_min, y_min = np.min(self._coords, axis=0) x_max, y_max = np.max(self._coords, axis=0) bounds = list(((x_min, y_min), (x_max, y_max)) for _ in range(self._nodes)) super().__init__(dimension=nodes, bounds=bounds, name=\"TSP\", seed=seed) self._distances = np.zeros((self._nodes, self._nodes)) for i in range(self._nodes): for j in range(i + 1, self._nodes): self._distances[i][j] = np.linalg.norm( self._coords[i] - self._coords[j] ) self._distances[j][i] = self._distances[i][j] def __evaluate_constraints(self, individual: Sequence | Solution) -> bool: counter = Counter(individual) if any(counter[c] != 1 for c in counter if c != 0) or ( individual[0] != 0 or individual[-1] != 0 ): return False return True def evaluate(self, individual: Sequence | Solution) -> tuple[float]: \"\"\"Evaluates the candidate individual with the information of the Travelling Salesmas Problem. The fitness of the solution is the fraction of the sum of the distances of the tour Args: individual (Sequence | Solution): Individual to evaluate Returns: Tuple[float]: Fitness \"\"\" if len(individual) != self._nodes + 1: msg = f\"Mismatch between individual variables ({len(individual)}) and instance variables ({self._nodes}) in {self.__class__.__name__}. A solution for the TSP must be a sequence of len {self._nodes + 1}\" raise ValueError(msg) penalty: np.float64 = np.float64(0) if self.__evaluate_constraints(individual): distance: float = 0.0 for i in range(len(individual) - 2): distance += self._distances[individual[i]][individual[i + 1]] fitness = 1.0 / distance else: fitness = 2.938736e-39 # --> 1.0 / np.float.max penalty = np.finfo(np.float64).max if isinstance(individual, Solution): individual.fitness = fitness individual.objectives = (fitness,) individual.constraints = (penalty,) return (fitness,) def __call__(self, individual: Sequence | Solution) -> tuple[float]: return self.evaluate(individual) def __repr__(self): return f\"TSP<n={self._nodes}>\" def __len__(self): return self._nodes def create_solution(self) -> Solution: items = [0] + list(range(1, self._nodes)) + [0] return Solution(chromosome=items) def to_file(self, filename: str = \"instance.tsp\"): with open(filename, \"w\") as file: file.write(f\"{len(self)}\\n\\n\") content = \"\\n\".join(f\"{x}\\t{y}\" for (x, y) in self._coords) file.write(content) @classmethod def from_file(cls, filename: str) -> Self: with open(filename) as f: lines = f.readlines() lines = [line.rstrip() for line in lines] nodes = int(lines[0]) coords = [] for line in lines[2:]: x, y = line.split() coords.append((int(x), int(y))) return cls(nodes=nodes, coords=tuple(coords)) def to_instance(self) -> Instance: return Instance(variables=self._coords.flatten())","title":"Creating the Travelling Salesman Problem (TSP)"},{"location":"tutorial/02_create_domain/#creating-the-travelling-salesman-problem-domain-tspdomain","text":"Once we have created the optimisation problem, we can define the domain of such problem. In this example, we call it TSPDomain. Every domain must define (at least) the generate_instances , extract_features , extract_features_as_dict and from_instance methods. The features methods are only relevant if you are planning to use the generators with the features-based descriptors. Otherwise you can return NotImplemented. A domain should include all the attributes and relevant information to generate the instances with particular characteristics. In this example, we need the dimension of the instances (number of nodes) and the ranges for the coordinates (xmin, xmax) and (ymin, ymax) for each node in the instance. class TSPDomain(Domain): \"\"\"Domain to generate instances for the Symmetric Travelling Salesman Problem.\"\"\" def __init__( self, dimension: int = 100, x_range: Tuple[int, int] = (0, 1000), y_range: Tuple[int, int] = (0, 1000), seed: int = 42, ): \"\"\"Creates a new TSPDomain to generate instances for the Symmetric Travelling Salesman Problem Args: dimension (int, optional): Dimension of the instances to generate. Defaults to 100. x_range (Tuple[int, int], optional): Ranges for the Xs coordinates of each node/city. Defaults to (0, 1000). y_range (Tuple[int, int], optional): Ranges for the ys coordinates of each node/city. Defaults to (0, 1000). Raises: ValueError: If dimension is < 0 ValueError: If x_range OR y_range does not have 2 dimensions each ValueError: If minimum ranges are greater than maximum ranges \"\"\" if dimension < 0: raise ValueError(f\"Expected dimension > 0 got {dimension}\") if len(x_range) != 2 or len(y_range) != 2: raise ValueError( f\"Expected x_range and y_range to be a tuple with only to integers. Got: x_range = {x_range} and y_range = {y_range}\" ) x_min, x_max = x_range y_min, y_max = y_range if x_min < 0 or x_max <= x_min: raise ValueError( f\"Expected x_range to be (x_min, x_max) where x_min >= 0 and x_max > x_min. Got: x_range {x_range}\" ) if y_min < 0 or y_max <= y_min: raise ValueError( f\"Expected y_range to be (y_min, y_max) where y_min >= 0 and y_max > y_min. Got: y_range {y_range}\" ) self._x_range = x_range self._y_range = y_range __bounds = [ (x_min, x_max) if i % 2 == 0 else (y_min, y_max) for i in range(dimension * 2) ] super().__init__(dimension=dimension, bounds=__bounds, name=\"TSP\", seed=seed) The generate_instances method is quite similar to the create_solution in the Problem class. Basically, it creates a random instance with the characteristics defined in the domain. def generate_instances(self) -> Instance: \"\"\"Generates a new instances for the TSP domain Returns: Instance: New randomly generated instance \"\"\" coords = self._rng.integers( low=(self._x_range[0], self._y_range[0]), high=(self._x_range[1], self._y_range[1]), size=(self.dimension, 2), dtype=int, ) coords = coords.flatten() return Instance(variables=coords) Likewise, the from_instance methods allows the domain to create an optimisation problem that can be solve using the definition of a particular instance. def from_instance(self, instance: Instance) -> TSP: n_nodes = len(instance) // 2 coords = tuple([*zip(instance[::2], instance[1::2])]) return TSP(nodes=n_nodes, coords=coords) Finally, the extract_features methods are domain and user depedent since the features to extract may vary from everyone needs. For this example the methods looks like: def extract_features(self, instance: Instance) -> tuple: \"\"\"Extract the features of the instance based on the TSP domain. For the TSP the features are: - Size - Standard deviation of the distances - Centroid coordinates - Radius of the instance - Fraction of distinct instances - Rectangular area - Variance of the normalised nearest neighbours distances - Coefficient of variantion of the nearest neighbours distances - Cluster ratio - Mean cluster radius Args: instance (Instance): Instance to extract the features from Returns: Tuple[float]: Values of each feature \"\"\" tsp = self.from_instance(instance) xs = instance[0::2] ys = instance[1::2] area = (max(xs) - min(xs)) * (max(ys) - min(ys)) std_distances = np.std(tsp._distances) centroid = (np.mean(xs), np.mean(ys)) # (0.01 * np.sum(xs), 0.01 * np.sum(ys)) centroid_distance = [np.linalg.norm(city - centroid) for city in tsp._coords] radius = np.mean(centroid_distance) fraction = len(np.unique(tsp._distances)) / (len(tsp._distances) / 2) # Top five only norm_distances = np.sort(tsp._distances)[::-1][:5] / np.max(tsp._distances) variance_nnds = np.var(norm_distances) variation_nnds = variance_nnds / np.mean(norm_distances) dbscan = DBSCAN() dbscan.fit(tsp._coords) cluster_ratio = len(set(dbscan.labels_)) / self.dimension # Cluster radius mean_cluster_radius = 0.0 for label_id in dbscan.labels_: points_in_cluster = tsp._coords[dbscan.labels_ == label_id] cluster_centroid = ( np.mean(points_in_cluster[:, 0]), np.mean(points_in_cluster[:, 1]), ) mean_cluster_radius = np.mean( [np.linalg.norm(city - cluster_centroid) for city in tsp._coords] ) mean_cluster_radius /= len(set(dbscan.labels_)) return ( self.dimension, std_distances, centroid[0], centroid[1], radius, fraction, area, variance_nnds, variation_nnds, cluster_ratio, mean_cluster_radius, ) def extract_features_as_dict(self, instance: Instance) -> Mapping[str, float]: \"\"\"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Args: instance (Instance): Instance to extract the features from Returns: Mapping[str, float]: Dictionary with the names/values of each feature \"\"\" names = \"size,std_distances,centroid_x,centroid_y,radius,fraction_distances,area,variance_nnNds,variation_nnNds,cluster_ratio,mean_cluster_radius\" features = self.extract_features(instance) return {k: v for k, v in zip(names.split(\",\"), features)} The complete TSPDomain class looks like: class TSPDomain(Domain): \"\"\"Domain to generate instances for the Symmetric Travelling Salesman Problem.\"\"\" def __init__( self, dimension: int = 100, x_range: Tuple[int, int] = (0, 1000), y_range: Tuple[int, int] = (0, 1000), seed: int = 42, ): \"\"\"Creates a new TSPDomain to generate instances for the Symmetric Travelling Salesman Problem Args: dimension (int, optional): Dimension of the instances to generate. Defaults to 100. x_range (Tuple[int, int], optional): Ranges for the Xs coordinates of each node/city. Defaults to (0, 1000). y_range (Tuple[int, int], optional): Ranges for the ys coordinates of each node/city. Defaults to (0, 1000). Raises: ValueError: If dimension is < 0 ValueError: If x_range OR y_range does not have 2 dimensions each ValueError: If minimum ranges are greater than maximum ranges \"\"\" if dimension < 0: raise ValueError(f\"Expected dimension > 0 got {dimension}\") if len(x_range) != 2 or len(y_range) != 2: raise ValueError( f\"Expected x_range and y_range to be a tuple with only to integers. Got: x_range = {x_range} and y_range = {y_range}\" ) x_min, x_max = x_range y_min, y_max = y_range if x_min < 0 or x_max <= x_min: raise ValueError( f\"Expected x_range to be (x_min, x_max) where x_min >= 0 and x_max > x_min. Got: x_range {x_range}\" ) if y_min < 0 or y_max <= y_min: raise ValueError( f\"Expected y_range to be (y_min, y_max) where y_min >= 0 and y_max > y_min. Got: y_range {y_range}\" ) self._x_range = x_range self._y_range = y_range __bounds = [ (x_min, x_max) if i % 2 == 0 else (y_min, y_max) for i in range(dimension * 2) ] super().__init__(dimension=dimension, bounds=__bounds, name=\"TSP\", seed=seed) def generate_instances(self) -> Instance: \"\"\"Generates a new instances for the TSP domain Returns: Instance: New randomly generated instance \"\"\" coords = self._rng.integers( low=(self._x_range[0], self._y_range[0]), high=(self._x_range[1], self._y_range[1]), size=(self.dimension, 2), dtype=int, ) coords = coords.flatten() return Instance(coords) def extract_features(self, instance: Instance) -> tuple: \"\"\"Extract the features of the instance based on the TSP domain. For the TSP the features are: - Size - Standard deviation of the distances - Centroid coordinates - Radius of the instance - Fraction of distinct instances - Rectangular area - Variance of the normalised nearest neighbours distances - Coefficient of variantion of the nearest neighbours distances - Cluster ratio - Mean cluster radius Args: instance (Instance): Instance to extract the features from Returns: Tuple[float]: Values of each feature \"\"\" tsp = self.from_instance(instance) xs = instance[0::2] ys = instance[1::2] area = (max(xs) - min(xs)) * (max(ys) - min(ys)) std_distances = np.std(tsp._distances) centroid = (np.mean(xs), np.mean(ys)) # (0.01 * np.sum(xs), 0.01 * np.sum(ys)) centroid_distance = [np.linalg.norm(city - centroid) for city in tsp._coords] radius = np.mean(centroid_distance) fraction = len(np.unique(tsp._distances)) / (len(tsp._distances) / 2) # Top five only norm_distances = np.sort(tsp._distances)[::-1][:5] / np.max(tsp._distances) variance_nnds = np.var(norm_distances) variation_nnds = variance_nnds / np.mean(norm_distances) dbscan = DBSCAN() dbscan.fit(tsp._coords) cluster_ratio = len(set(dbscan.labels_)) / self.dimension # Cluster radius mean_cluster_radius = 0.0 for label_id in dbscan.labels_: points_in_cluster = tsp._coords[dbscan.labels_ == label_id] cluster_centroid = ( np.mean(points_in_cluster[:, 0]), np.mean(points_in_cluster[:, 1]), ) mean_cluster_radius = np.mean( [np.linalg.norm(city - cluster_centroid) for city in tsp._coords] ) mean_cluster_radius /= len(set(dbscan.labels_)) return ( self.dimension, std_distances, centroid[0], centroid[1], radius, fraction, area, variance_nnds, variation_nnds, cluster_ratio, mean_cluster_radius, ) def extract_features_as_dict(self, instance: Instance) -> Mapping[str, float]: \"\"\"Creates a dictionary with the features of the instance. The key are the names of each feature and the values are the values extracted from instance. Args: instance (Instance): Instance to extract the features from Returns: Mapping[str, float]: Dictionary with the names/values of each feature \"\"\" names = \"size,std_distances,centroid_x,centroid_y,radius,fraction_distances,area,variance_nnNds,variation_nnNds,cluster_ratio,mean_cluster_radius\" features = self.extract_features(instance) return {k: v for k, v in zip(names.split(\",\"), features)} def from_instance(self, instance: Instance) -> TSP: n_nodes = len(instance) // 2 coords = tuple([*zip(instance[::2], instance[1::2])]) return TSP(nodes=n_nodes, coords=coords)","title":"Creating the Travelling Salesman Problem Domain (TSPDomain)"},{"location":"tutorial/02_create_domain/#how-to","text":"Run an experiment Create a domain Create a solver","title":"How to"},{"location":"tutorial/03_create_algorithm/","text":"Create an algorithm Create an algorithm NN as a function. NN as as class How to To exemplify how to create a new algorithm in DIGNEA, we will see how to include the classical Nearest Neighbour algorithm for TSP. This can be simply defined as a function: NN as a function. The function receives an object of the TSP with the required information to evaluate the solutions it generates: def nneighbour(problem: TSP) -> list[Solution]: \"\"\"Nearest-Neighbour Heuristic for the Travelling Salesman Problem Args: problem (TSP): Problem to solve Raises: ValueError: If problem is None Returns: list[Solution]: Collection of solutions to the problem. \"\"\" if problem is None: raise ValueError(\"No problem found in nneighbour heuristic\") distances = problem._distances current_node = 0 visited_nodes: set[int] = set([current_node]) tour = np.zeros(problem.dimension + 1) length = np.float32(0) idx = 1 while len(visited_nodes) != problem.dimension: next_node = 0 min_distance = np.finfo(np.float32).max for j in range(problem.dimension): if j not in visited_nodes and distances[current_node][j] < min_distance: min_distance = distances[current_node][j] next_node = j visited_nodes.add(next_node) tour[idx] = next_node idx += 1 length += min_distance current_node = next_node length += distances[current_node][0] length = 1.0 / length return [Solution(chromosome=tour, objectives=(length,), fitness=length)] NN as as class Alternatively, you could defined your solver as a Python class like: class NearestNeighbour(Solver, SupportsSolve[P]): def __init__(self): ... def __call__(self, problem: TSP) -> list[Solution]: if problem is None: raise ValueError(\"No problem found in nneighbour heuristic\") distances = problem._distances current_node = 0 visited_nodes: set[int] = set([current_node]) tour = np.zeros(problem.dimension + 1) length = np.float32(0) idx = 1 while len(visited_nodes) != problem.dimension: next_node = 0 min_distance = np.finfo(np.float32).max for j in range(problem.dimension): if j not in visited_nodes and distances[current_node][j] < min_distance: min_distance = distances[current_node][j] next_node = j visited_nodes.add(next_node) tour[idx] = next_node idx += 1 length += min_distance current_node = next_node length += distances[current_node][0] length = 1.0 / length return [Solution(chromosome=tour, objectives=(length,), fitness=length)] How to Run an experiment Create a domain Create a solver","title":"Create an algorithm"},{"location":"tutorial/03_create_algorithm/#create-an-algorithm","text":"Create an algorithm NN as a function. NN as as class How to To exemplify how to create a new algorithm in DIGNEA, we will see how to include the classical Nearest Neighbour algorithm for TSP. This can be simply defined as a function:","title":"Create an algorithm"},{"location":"tutorial/03_create_algorithm/#nn-as-a-function","text":"The function receives an object of the TSP with the required information to evaluate the solutions it generates: def nneighbour(problem: TSP) -> list[Solution]: \"\"\"Nearest-Neighbour Heuristic for the Travelling Salesman Problem Args: problem (TSP): Problem to solve Raises: ValueError: If problem is None Returns: list[Solution]: Collection of solutions to the problem. \"\"\" if problem is None: raise ValueError(\"No problem found in nneighbour heuristic\") distances = problem._distances current_node = 0 visited_nodes: set[int] = set([current_node]) tour = np.zeros(problem.dimension + 1) length = np.float32(0) idx = 1 while len(visited_nodes) != problem.dimension: next_node = 0 min_distance = np.finfo(np.float32).max for j in range(problem.dimension): if j not in visited_nodes and distances[current_node][j] < min_distance: min_distance = distances[current_node][j] next_node = j visited_nodes.add(next_node) tour[idx] = next_node idx += 1 length += min_distance current_node = next_node length += distances[current_node][0] length = 1.0 / length return [Solution(chromosome=tour, objectives=(length,), fitness=length)]","title":"NN as a function."},{"location":"tutorial/03_create_algorithm/#nn-as-as-class","text":"Alternatively, you could defined your solver as a Python class like: class NearestNeighbour(Solver, SupportsSolve[P]): def __init__(self): ... def __call__(self, problem: TSP) -> list[Solution]: if problem is None: raise ValueError(\"No problem found in nneighbour heuristic\") distances = problem._distances current_node = 0 visited_nodes: set[int] = set([current_node]) tour = np.zeros(problem.dimension + 1) length = np.float32(0) idx = 1 while len(visited_nodes) != problem.dimension: next_node = 0 min_distance = np.finfo(np.float32).max for j in range(problem.dimension): if j not in visited_nodes and distances[current_node][j] < min_distance: min_distance = distances[current_node][j] next_node = j visited_nodes.add(next_node) tour[idx] = next_node idx += 1 length += min_distance current_node = next_node length += distances[current_node][0] length = 1.0 / length return [Solution(chromosome=tour, objectives=(length,), fitness=length)]","title":"NN as as class"},{"location":"tutorial/03_create_algorithm/#how-to","text":"Run an experiment Create a domain Create a solver","title":"How to"},{"location":"tutorial/0_getting_started/","text":"Getting Started Getting Started How DIGNEApy works. How to Main components. Results More information How DIGNEApy works. DIGNEA (Diverse Instance Generator with Novelty Search and Evolutionary Algorithms) is a framework which allow researchers to generate diverse and discriminatory instances for any combinatorial optimization problem. The method uses a portfolio of solvers to evaluate its performance over a population of instances for a particular optimization problem. Moreover, it also uses a Novelty Search algorithm to search for instances that are diverse but also elicit discriminatory performance from a set of target solvers. A single run of DIGNEApy per target solver provides discriminatory instances and broad coverage of the feature-space. How to Run an experiment Create a domain Create a solver Main components. The main building blocks of DIGNEApy are the following classes and protocols: SupportsSolve: Any callable type that receives a Problem (P) and returns a list of Solution. Problem: The Problem class defines a classical optimization to be solved (KP, TSP, Sphere, etc). The __call__ method must be defined. The method must received a sequence or Solution type and return a tuple with the evaluation of the solution. Domain: Abstract class that represents a domain (optimisation domain) for which you want to generate instances. New domains must subclass this class. Generator: Any callable that when invoked returns a GenResult object. GenResult: A dataclass which contains the following attributes: target: String with the name of the target solver of the instances instances: A sequence of instances generated history: An object of class Logbook with the history of the different metrics computed during the evolution metrics: A pd.Series with the metrics of the resulting instances EAGenerator: Meta-Evolutionary Algorithm. This class represents the instance generator algorithm. It receives a portfolio of solvers (one defined as the target algorithm) and it returns a set of diverse instances where the target solver outperforms the remaining algorithms in the portfolio; i.e., the instances are biased to the performance of such target. Results An example of the results obtained by DIGNEApy is shown down below. Four different sets of KP instances were generated for different configurations of Genetic Algorithms using EAGenerator. More information To get more information on how the method works check the following papers : Marrero, A., Segredo, E., Le\u00f3n, C., Hart, E. (2022). A Novelty-Search Approach to Filling an Instance-Space with Diverse and Discriminatory Instances for the Knapsack Problem. In: Rudolph, G., Kononova, A.V., Aguirre, H., Kerschke, P., Ochoa, G., Tu\u0161ar, T. (eds) Parallel Problem Solving from Nature \u2013 PPSN XVII. PPSN 2022. Lecture Notes in Computer Science, vol 13398. Springer, Cham. https://doi.org/10.1007/978-3-031-14714-2_16 Alejandro Marrero, Eduardo Segredo, Coromoto Le\u00f3n, Emma Hart; Synthesising Diverse and Discriminatory Sets of Instances using Novelty Search in Combinatorial Domains. Evolutionary Computation 2024; doi: https://doi.org/10.1162/evco_a_00350","title":"Overview"},{"location":"tutorial/0_getting_started/#getting-started","text":"Getting Started How DIGNEApy works. How to Main components. Results More information","title":"Getting Started"},{"location":"tutorial/0_getting_started/#how-digneapy-works","text":"DIGNEA (Diverse Instance Generator with Novelty Search and Evolutionary Algorithms) is a framework which allow researchers to generate diverse and discriminatory instances for any combinatorial optimization problem. The method uses a portfolio of solvers to evaluate its performance over a population of instances for a particular optimization problem. Moreover, it also uses a Novelty Search algorithm to search for instances that are diverse but also elicit discriminatory performance from a set of target solvers. A single run of DIGNEApy per target solver provides discriminatory instances and broad coverage of the feature-space.","title":"How DIGNEApy works."},{"location":"tutorial/0_getting_started/#how-to","text":"Run an experiment Create a domain Create a solver","title":"How to"},{"location":"tutorial/0_getting_started/#main-components","text":"The main building blocks of DIGNEApy are the following classes and protocols: SupportsSolve: Any callable type that receives a Problem (P) and returns a list of Solution. Problem: The Problem class defines a classical optimization to be solved (KP, TSP, Sphere, etc). The __call__ method must be defined. The method must received a sequence or Solution type and return a tuple with the evaluation of the solution. Domain: Abstract class that represents a domain (optimisation domain) for which you want to generate instances. New domains must subclass this class. Generator: Any callable that when invoked returns a GenResult object. GenResult: A dataclass which contains the following attributes: target: String with the name of the target solver of the instances instances: A sequence of instances generated history: An object of class Logbook with the history of the different metrics computed during the evolution metrics: A pd.Series with the metrics of the resulting instances EAGenerator: Meta-Evolutionary Algorithm. This class represents the instance generator algorithm. It receives a portfolio of solvers (one defined as the target algorithm) and it returns a set of diverse instances where the target solver outperforms the remaining algorithms in the portfolio; i.e., the instances are biased to the performance of such target.","title":"Main components."},{"location":"tutorial/0_getting_started/#results","text":"An example of the results obtained by DIGNEApy is shown down below. Four different sets of KP instances were generated for different configurations of Genetic Algorithms using EAGenerator.","title":"Results"},{"location":"tutorial/0_getting_started/#more-information","text":"To get more information on how the method works check the following papers : Marrero, A., Segredo, E., Le\u00f3n, C., Hart, E. (2022). A Novelty-Search Approach to Filling an Instance-Space with Diverse and Discriminatory Instances for the Knapsack Problem. In: Rudolph, G., Kononova, A.V., Aguirre, H., Kerschke, P., Ochoa, G., Tu\u0161ar, T. (eds) Parallel Problem Solving from Nature \u2013 PPSN XVII. PPSN 2022. Lecture Notes in Computer Science, vol 13398. Springer, Cham. https://doi.org/10.1007/978-3-031-14714-2_16 Alejandro Marrero, Eduardo Segredo, Coromoto Le\u00f3n, Emma Hart; Synthesising Diverse and Discriminatory Sets of Instances using Novelty Search in Combinatorial Domains. Evolutionary Computation 2024; doi: https://doi.org/10.1162/evco_a_00350","title":"More information"}]}